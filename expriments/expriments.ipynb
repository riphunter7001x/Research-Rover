{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import arxiv\n",
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_qdrant import Qdrant\n",
    "from langchain.embeddings import CohereEmbeddings\n",
    "from typing import List, Optional\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"COHERE_API_KEY\"] = os.getenv(\"COHERE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_papers(query: str) -> str:\n",
    "    # \"\"\" \n",
    "    # Downloads and processes papers from arXiv based on the query.\n",
    "\n",
    "    # Args:\n",
    "    #     query (str): The search query to fetch papers.\n",
    "\n",
    "    # Returns:\n",
    "    #     str: The concatenated content of all papers.\n",
    "  \n",
    "    # Replace spaces with underscores in the query to create a valid directory name\n",
    "        # Set up the directory path relative to the current working directory\n",
    "    base_dir = os.getcwd()\n",
    "    dirpath = os.path.join(base_dir, f\"arxiv_papers_for_{query.replace(' ', '_')}\")\n",
    "\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "\n",
    "\n",
    "    # Initialize arxiv client and search for papers\n",
    "    client = arxiv.Client()\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=2,\n",
    "        sort_order=arxiv.SortOrder.Descending\n",
    "    )\n",
    "\n",
    "   # Download and save the papers\n",
    "    for result in client.results(search):\n",
    "        while True:\n",
    "            try:\n",
    "                paper_id = result.get_short_id()\n",
    "                # Truncate and sanitize title to avoid overly long filenames\n",
    "                title = result.title.replace(' ', '_').replace('/', '_').replace(':', '').replace('?', '')[:30]\n",
    "                filepath = os.path.join(dirpath, f\"{paper_id}_{title}.pdf\")\n",
    "                result.download_pdf(dirpath=dirpath, filename=f\"{paper_id}_{title}.pdf\")\n",
    "                logging.info(f\"-> Paper id {paper_id} with title '{result.title}' is downloaded.\")\n",
    "                break\n",
    "            except (FileNotFoundError, ConnectionResetError) as e:\n",
    "                logging.error(f\"Error occurred: {e}\")\n",
    "                time.sleep(5)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"An unexpected error occurred: {e}\")\n",
    "                break\n",
    "    return dirpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-06 21:27:14,797 - INFO - Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=AI+In+defence&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=100\n",
      "2024-07-06 21:27:21,659 - INFO - Got first page: 100 of 2346457 total results\n",
      "2024-07-06 21:27:26,526 - INFO - -> Paper id 2112.01252v2 with title 'Australia's Approach to AI Governance in Security and Defence' is downloaded.\n",
      "2024-07-06 21:27:29,897 - INFO - -> Paper id 1809.11089v1 with title 'A Systems Approach to Achieving the Benefits of Artificial Intelligence in UK Defence' is downloaded.\n"
     ]
    }
   ],
   "source": [
    " paers = get_papers(\"AI In defence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\github\\\\openai\\\\ResearchRover\\\\expriments\\\\arxiv_papers_for_AI_In_defence'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_papers(dirpath):\n",
    "    papers = []\n",
    "    loader = DirectoryLoader(dirpath, glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
    "    try:\n",
    "        papers = loader.load()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading files: {e}\")\n",
    "\n",
    "    logging.info(f\"Total number of pages loaded: {len(papers)}\")\n",
    "\n",
    "    # Concatenate all pages' content into a single string\n",
    "    full_text = ''.join(paper.page_content for paper in papers)\n",
    "\n",
    "    # Remove empty lines and join lines into a single string\n",
    "    full_text = \" \".join(line for line in full_text.splitlines() if line)\n",
    "\n",
    "    return full_text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-06 21:28:47,834 - INFO - Total number of pages loaded: 66\n"
     ]
    }
   ],
   "source": [
    "text = load_papers(paers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abstract1  The current resurgent interest in Artificial Intelligence  (AI) has been driven by the availability of data (particularly  labelled data), the democratisation of computing  infrastructure and tooling, and the ability to combine these  elements to cre ate AI algorithms.  Benefit is achieved once  an algorithm is deployed into an operational system to  achieve an operational advantage.     The ability to exploit the opportunities offered by AI  within UK Defence calls for an understanding of systemic  issues r equired to achieve an effective operational  capability.  This paper provides the authors’ views of issues  which currently block UK Defence from fully benefitting  from AI technology. These are situated within a reference  model for the AI Value Train, so ena bling the community to  address the exploitation of such data and software intensive  systems in a systematic, end to end manner.   The paper sets out the conditions for success including:   • Researching future solutions to known problems and  clearly defined u se cases;   • Addressing achievable use case s to show benefit;   • Enhancing the availability of Defence -relevant data;   • Enhancing Defence ‘know how’ in AI;   • Operating Software Inte nsive supply chain eco -systems  at required breadth and pace;  • Governance a nd, the integration of software and  platform supply chains and operating models .  1 Introduction   The UK Ministry of Defence (MOD), like the US  Department of Defense (DoD), has an ongoing need to  adapt and flexibly employ military forces to meet changing  operational needs [1, 2, 3, 4]. The US pursuit of the ‘third  offset’ under initiatives such as Project Maven places                                                    1 Dstl/CP111074.   Content includes material subject to ©  Crown copyright (2018), Dstl. This information is licensed under  the terms of the Open Government Licence except where otherwise  stated. To view this licence, visit  http://www.nationalarchives.gov.uk/ doc/open -government - licence/version/3 or write to the Information Policy Team, The  National Archives, Kew, London TW9 4DU, or email:  psi@nationalarchives.gsi.gov.uk  significant emphasis on the exploitation of relevant data sets  – a point reinforced by the UK’s Vice Chief of the Defence  Staff – using modern d ata analysis techniques including  Artificial Intelligence (AI). Generating the notion of  ‘algorithmic warfare’, the US intent is to generate and  maintain an unassailable data exploitation position for its  competitive operational advantage.  AI becomes a fo rce  multiplier, with data t he de facto ‘Centre of Gravity’ .  1.1 Systems Approach   Taking a ‘systems approach’ requires looking at an issue  within the context of the system in its entirety, considering  the systems behaviour as a whole, as distinct from focus ing  on parts of the system in isolation [5]. In accordance with  principles of Boundary Critique, a critical first stage is to  define the problem and the ‘system under study’ [6] – what  is included and excluded fundamentally shapes the ability of  the UK Def ence enterprise to fully realise the benefits of AI.  It is not just the system in which AI might be embedded, but  the wider system (or even system of systems) that is  required to deplo y and exploit the AI technology .  The recent resurgence in AI has been dr iven by a near  exponential growth in the availability of data (particularly  labelled data), computing power, open/affordable tooling,  and the ability to put these elements together to create,  deploy and employ superior algorithms to deliver benefit to  an enterprise.  Therefore these are all parts of the system   (see Figure 1) , yet in pulling the elements together, there is  also the realisation that Defence is itself a system of systems  (which by its nature is complex and adaptive). Therefore  this process wil l have inherent frictions which must be  overcome with the appropriate degree of planning rigour.   Further there is the fact which we have all become very  well aware of over the last 30 years and that is the rapid rate  of technology development within Inform ation Systems.    Additionally, in the military, as in so many domains, there is  uncertainty over the future situations within which military  forces will be required to operate and thus the performance  required from their socio -technical Information Systems   (including AI). The complexities of the Contemporary  Operating Environment, and the potential vagaries of that in  the future, present planning conundrums – is a capability  deigned to be versatile for use across much of the Spectrum    A Systems Approach to Achieving the Benefits of Artificial Intelligence in UK  Defence\\uf020 Gavin Pearsona, Phil Jolleyb , Geraint Evansc  aDefence Science and Technology Laboratory, Porton Down, Salisbury, UK    bIBM Ltd, PO Box 41, North Harbour, Portsmouth, Hampshire, UK   cCranfield Defence and Security Doctoral Training Centre, Defence Academy, UK  Figure 1: Strawman AI Value Train   of Conflict? Or is it n arrowly defined for use in a specific  type of operation and adversary? This uncertainty coupled  with the ‘democratisation of technology’ means that  Defence needs to act at sufficient pace to maintain  Information Advantage as and when it is required along  temporal , geographic and mission -specific lines. It would be  unrealistic to expect it to be sustained indefinitely. UK  Defence must be able to quickly generate it when needed .  Additionally the system needs to be affordable , a by no  means insignificant factor in a time of strong fiscal head - winds for UK Defence.  Therefore, we need the ability to  evolve our AI system in an effective, timely and cost - effective manner.  These four elements are illustrated in  Figure 2.   Figure 2 : Four elements of the broad context of AI   2 AI immaturity: Defence Challenge to AI  Technical Capabilities   One challenge with the adoption of AI within Defence is  that many Defence tasks require AI capabilities which are  currently immature.  Examples inclu de [7, 8] : \\uf0b7 Military decision -making within combat operations can  be characte rised as having a “high regret” if the “wrong”  decision is made; so requiring a high degree of trust in  any dec ision-making conducted by AI and the associated  need for interpretabi lity. The co mplexity of this challenge  is compounded by the need to consider mili tary-decision- making in different combat scenarios, operating env i- ronments and levels of command. This is an open r e- search question .  \\uf0b7 Military operations may be undertaken agai nst an a c- tive opponent, who is deliberately attemp ting to deceive  the AI systems; and the adversary may be using their own  AI systems to control the deception operation. This is  an- other open r esearch question .  \\uf0b7 Military operations will almost certainly be Multi - Domain in nature, with force elements active across  Land, Maritime, Air, Space and Cyber domains in para l- lel, creating the need for AI capable of linking across  highly -dispersed warfighters and agents (robotic and  software) operating in complex envir onments (another  open r esearch question).   \\uf0b7 Military operations are frequently conducted in new  settings, so there is a need for AI which can operate e f- fectively with sparse data sets and very little training data  (another open research question ).  \\uf0b7 Military o perations are increasingly Coalition oper a- tions in which many parties come together to achieve  common aims, but operate under different sets of policies  and with differing levels of trust.  Thus there is a need for  AI systems which can learn and reason eff ectively when  distributed across a set of partners, whilst also retaining  the ability for the UK to operate alone for sovereign pu r- poses ( another open research question ).  \\uf0b7 Tactical military operations are also characterised by  their operation on edge comput ing infrastructures with  limited resources, and an ele ctro-magnetic spectrum that  is both congested and contested.  This is a very different  setting for the typical server farm or ‘cloud’ env ironment  and again gives rise to open research questions.   Defence  funded AI research is active in these areas, yet if it  is to avoid the sins of its past, there is the need to ma nage  stakeholder expectations very carefully, so that their d e- mand signal for AI is pragmatic and achievable. The MOD  Chief Scientific Adviser has been particularly direct in his  description of AI.  His description of it being ‘no more than  architecture, data and algorithms’ was d eliberate, noting that  there is a danger of AI being ‘over -sold’ and thus uninte n- tionally entering a ‘trough of despon dency’ within D efence.   Finally it is worth touching on the need to match the c a- pabilities and behaviour of AI ensembles to the type of u n- derstanding and decision -making activity which is being  unde rtaken.  Here it is worth reflecting on the very different  responses needed by systems (including human organis a- tions) when addressing open versus closed que stions [9]; or  when being data or hypothesis led in their attempt to unde r- stand the world [10].  Ong oing Army -sponsored research  suggests that the Understandi ng function as doctrinally cod i- fied is itself unachievable – staffs are almost certain to ne ver  be able to fully appreciate or comprehend a situation (esp e- cially if it is in the chaotic region of the Cynefin Fram ework  or is a ‘ wicked prob lem’), but instead  must be able to inte r- pret ce rtain events and factors at a given point in time, or  seek to predict any future change which may o ccur. The  research suggests that understanding is more closely linked  to risk mitigation and tole rance for any decision -making  which follows. This is where AI could play a particularly  important role in helping staffs to focus on the response they  seek or, even if they do not know it initially, they need  it.  Figure 3: Analyst Sense -Making Process [10]   3 Nearer term exploitation of AI    There are, however, many ways in which AI can be used  within the Defence business, as it is being used within many  other businesses of a similar size and organisational  complexity (e.g. within logistics and utilities maintenance).   Therefore, it is worth  focusing on the issues associated with nearer term exploitation of AI within Defence. This must,  going forward, be based on clear and unambiguous use  cases with varying degrees of complexity and ambition. The  temptation to keep everything simple and in a neatly framed  problem is always here. At some point, Defence  stakeholders need to be bold and embrace more complex  problems to investigate resolving with AI techniques.   Use of AI relies on availability of relevant data,  computing infrastructure, tooling a nd the ‘know how’ to put  the elements together to develop algorithms and deploy  them into the hands of users.   We suggest that tooling and  computing infrastructures are not the elements limiting the  ability to deploy AI in many areas (though the diversity  of  MOD computing infrastructures is a factor as discussed in  section 7).  Instead the critical issues are availability of and  access to data, particularly labelled data (as discussed in  section 4), ‘know how’ (as discussed in section 5) and  operating a so ftware intensive supply chain (as discussed in  section 6).   Further the successful integration of AI means changing  what people do and how the business operates; so it is part  of a larger business change process. The paradox here is that  UK Defence is a la rgely conservative organisation with a  strong ethos of ‘making do’ and succeeding with limited  resources.  But its professiona lism and strong moral  component  means it recognises the need to adapt and is  fomenting a drive for far -reaching modernisation.  As  such  it requires business use cases where the benefits of applying  AI can be clearly articulated, and requires the development  and testing of the changed business processes as well as the  technology.  Therefore it benefits from a senior champion  who must not shy away from the difficult decisions [4].   4  Data   “Data is the raw material of the 21st century” [11], and is  the foundational component which must be available in  order to enable exploitation of AI (as without data then there  is no data for an algori thm to process).  Further the recent  advance in AI has been driven by the availability of human  labelled data which is used when training the Machine  Learning component of an AI system.  Therefore, it is vital  that early exploitation of AI is driven by the  availability of  sufficient relevant data of sufficient quality, and supported  by an suitable approach to creating labelled data where that  is needed.  Note here the point on Data and the centre of  gravity – UK Defence will not fully embrace the benefits  unless its data access is addressed, allowing the data to be  appropriately accessed for exploitation by AI technologies.  The scale of that challenge must not be underestimated.   4.1 Defence Data about Defence Systems   Many Defence Systems are legacy systems o r are operated  in a manner which means their support and maintenance is  contracted out.  Hence it is by no means always the case that  the commercial agreement under which they were procured  makes prov ision for their instrumentation  and for the data ,  from a ny instrumentation, to be available to the wider  Defence enterprise. In essence, legacy contractual decisions  place what is effectively a commercial blocker to AI  integration and exploitation in the Defence Equipment  Program’s near -term activity. Moves tow ards more open  standards downstream provide greater opportunity and flex,  yet how are operational demand signals for AI able to be  met in the interim?   Further instrumenting Defence assets, so creating an  effective Internet of Military Things, has implicati ons where  (i) emission control is needed in order to deny an opponent  information and (ii) an opponent is actively engaging in  cyber operations. This is itself acting as a key driver for AI  – the Future Force hypothesis suggests much smaller  physical and e lectronic footprints in the battlespace, which  is where AI technology which may facilitate this has been  identified as having particular utility in Land C ommand and  Control (C 2) studies. In addition there are other implications  associated with legality, pr ivacy and ethics.   However, adopting a Defence equivalent of the UK  Government Open Data Rating scale and applying it across  Defence projects would help create the conditions for  success by providing Defence with access to data from its  own systems. At the  same time it must be asked, does  Defence know where all its data is and can it be made  readily available for AI testing and experimentation?   4.2 Data about wider world   As noted earlier, many Defence operations require data  about the exterior world.  Thus many Defence systems  include sensors which collect data about the world.  This is  not simply data on the physical domain, but also includes  information about the cognitive and information domains.    As such data collection and analysis (inc. AI) needs to b e  conducted in a manner which confirms to local and UK  ethical and legal constraints – including preservation of  privacy and proportionality. Yet consider the potential  benefit – could AI be used, for example, to unpick the  digital ‘fog of war’ and unambig uously identify Indicators  and Warnings from across data about the wider world which  could, if appropriately exploited, generate Courses or Action  which themselves could be accelerated and risks reduced by  using other AI technologies which feed off the con ditions  and actions or other elements in the system (or even system  of systems?).   5  “Know How” within Defence   There is a recognised shortage of Suitably Qualified and  Experienced Personnel (SQEP) within the UK to maximise  the exploitation of AI.  This sho rtage is naturally reflected  within Defence across civilian and military staff.  The career  management of the small pool of Defence SQEP aside, the  MOD is constrained in its ability to pay more for scarce  skills.  However, Defence is able to compete for SQ EP on  other grounds.   In response to the opportunity offered by AI and need to  manage ‘know how’, the Defence Science and Technology  Laboratory  (Dstl) has created an ‘AI lab’  which aims to be  “A single pan -Dstl flagship for AI, Machine Learning and  Data Sc ience that works with suppliers and partners to establish a world -class capability in the application of AI  related technologies to Defence a nd Security challenges”  [12].  ‘ AI Lab’ will work with similar centres of excellence,  such as Project Nelson [13], be ing set up within the Defence  Commands.   6  Software Intensive Supply Chain Eco - System   Experience dealing with successful Open Systems shows  that it is equally important to focus on the commercial,  supply chain and operating model, as it is the technical  architecture [ 14, 15 ].  Additionally, any ‘AI system’ is  clearly a software intensive system and experience teaches  that the production of software is best considered a design  activity rather than a production activity.  Thus there is a  need to adopt an oper ating model which employs Lean,  Agile and DevOps principles and practices, noting that some  capabilities will be Joint Forces Command -sponsored,   whilst  others will be championed and funded by the Single  Services.   This naturally has implications for the s upply  chain and commercial strategy adopted to enable the  acquisition of AI within government systems.  Critically it  does not lend itself to the standard CADMID model of  Defence acquisition.   If one accepts that a significant portion of AI software  innovat ion occurs within Small & Medium Enterprises  (SMEs) then it is necessary to design a supply chain and  commercial construct that works for such SMEs.   Traditionally, the large number and size of Defence standard  terms and conditions which apply to procureme nt make it  hard for an SME to engage, as understanding and complying  with such a large set of terms and conditions is costly.   Finding an approach which enables the effective integration  of a wide and rapidly fluctuating pool of SMEs into the  supply chain of such software intensive systems is a key  issue which needs further investigation.  It is a long - standing question in UK Defence which must be tackled  head on. As a case in point, the Defence and Security  Accelerator initiative has led to the emergence o f some  startling and potential ly ground -breaking proposals and  proof of concept demonstrations . Yet the pull -through to the  Equipment Program is so convoluted thereafter, both from a  Defence stakeholder and SME perspective, that some ideas  can ‘waste away. ’  Furthermore, it must address how to deal with the sudden  unavailability of an SME which has delivered an important  micro -service.  There are both technical and Intellectual  Property Right (IPR) challenges.  Firstly, there is the  technical challenge of ma intaining and, when necessary,  adapting or replacing code which has been generated by an  SME.  Secondly, the IPR associated with both algorithms  and data can, at such times, be hugely contentious. Should  algorithms trained on MOD data be intellectually own ed by  MOD?  There is the need, therefore, to differentiate between  COTS algorithms used in AI tooling (by MOD and its  suppliers) and those AI algorithms, and their data  architectures, generated by the MOD and its suppliers.    The move to an open system, s ervices, agile, incremental  and DevOps model would mean that the average size and  complexity of individual projects will fall.  This will impact  both major companies and SMEs, as the cost of the  overheads associated with engaging with Defence will  become a  more significant element of each project – unless  we can develop new models of engagement which are (in  the jargon) ‘frictionless’.  Otherwise the attempt to broaden  the supply pool may have entirely unintended consequences.   There also exists the need to  understand the regulatory  framework around development and testing of such services  – particularly given the need to “harness AI for defence &  security in a manner that is moral & ethical , reinforces  international norms  and counters irresponsible use of A I.”  This should not stymie AI exploration  and the  understanding of AI threats and opportunities – after all,  potential UK adversaries may have a radically different  moral compass – but the strictures it will force have to be  sensibly acknowledged upfront .  7  A Fragmented Defence Information  Infrastructure   Most organisations contemplating the exploitation of AI  have a relatively simple information infrastructure.   Unfortunately this is not the case within Defence.  Defence  has traditionally been in the busi ness of either ‘equipping  the man’ or ‘manning equipment’, which leads to a situation  in which multiple platforms ( e.g. ships, boats, planes,  helicopters & vehicles) are purchased from a wide range of  suppliers as complete systems.  From a capability  persp ective, the Information Defence Lines of Development  (DLOD) and essential data can exist in silos of ‘splendid  isolation.’ Further, over time the information systems  component of each such platform has become a larger and  more significant part of it.  Thus  we have a multitude of  information infrastructures provided by any number of  suppliers; each of which tends to have its own supply chain,  commercial model, operating model and technical  architecture – this makes re -use of (micro -)services and data  across these multiple platforms a challenge. The UK -based  supply chain is bad enough; reconciling this for deployed  forces in a theatre of operations unnecessarily adds to the  Logistics and support burden.   Figure 4: Tactical Networks   This is also why Defence has  been strongly engaged  within the development and testing of Open Systems  approaches.  However, in simple terms it means we are  operating across the full set of computing paradigms (see figure 5).  Importantly, as noted above, the contested nature  of many Defence operations means that the communications  infrastructures, and communication qualities of service,  assumed by COTS solutions are not appropriate .  Worse still once we deploy forces on operations we are  aiming to work with a very wide range of partner s (see  figure 6) – thus interoperability is now a non -discretionary  requirement in UK Defence planning.     Figure 5: Computing Paradigms (image courtesy of IBM)   Figure 6: Operating in the Urban Environment   8  Conclusions and Recommendations   The ability to  exploit the opportunities offered by AI within  UK Defence requires an understanding of systemic issues  required to achieve an effective operational capability.      We have set out a strawman reference model for the AI  Value Train which can potentially pro vide a common  system reference model, so enabling the community (though  provision of a common lens) to address the exploitation of  such data and software intensive systems in a systematic,  end to end manner.   We have discussed the conditions for success in cluding:   • Researching future solutions to known problems and  clearly defined use cases;   • Addressing achievable use case s to show benefit (one s  where the algorithms and computing infrastructure are  mature, and data is available);   • Enhancing the availabil ity of Defence -relevant data  (outside project boundaries);   • Enhancing Defence ‘know how’ in AI through centres  of excellence/competence and growth of expert  personnel;   • Operating Software Intensive supply chain eco -systems  (to access exterior ‘know how’ and operate at required  pace));   • Governance and, the integration of software and  platform supply chains and operating models.   In conclusion, we recommend a strategic approach with  three main lines of action:   a) Exploit near term opportunities with an eye on the future  breadth and  pace required (see section 3);   b) Focus on fundamental enablers of Data, Defence ‘know  how’ and supply chain relationships (see sections 4, 5 & 6):   i. Develop the supply chain relationship between  emerging Defence Centres of AI Excellence and wider  supply chain (see section 5 & 6);   ii. Develop a strategy to break data out from its current  silos, addressing both the near -term need and longer -term  opportunity (c.f. government open data rating scale) (see  section 4).   c) Plan for act ion at required breadth, depth and pace;   i. Maintain research on maturing AI to cope with future  defence challenges and clearly defined future use cases  (see section 1);   ii. Develop and test the approach to such software  intensive supply chains (see sectio n 6);  iii. Develop and test an approach to software intensive  supply chains and their intersection with the fragmented  Defence Information Infrastructure (see section 7).   Acknowledgments  and Copyright   This research was sponsored by the U.S. Army Research  Laboratory and the U.K. Ministry of Defence under  Agreement Number W911NF -16-3-0001. The views and  conclusions contained in this document are those of the  authors and should not be interpreted as representing the  official policies, either expressed or impli ed, of the U.S.  Army Research Laboratory, the U.S. Government, the U.K.  Ministry of Defence or the U.K. Government. The U.S. and  U.K. Governments are authorized to reproduce and  distribute reprints for Government purposes notwithstanding  any copyright nota tion hereon.   Dstl/CP111074.   Content includes material subject to ©  Crown copyright (2018), Dstl. This information is licensed  under the terms of the Open Government Licence except  where otherwise stated. To view this licence, visit  http://www.nationalar chives.gov.uk/doc/open -government - licence/version/3 or write to the Information Policy Team,  The National Archives, Kew, London TW9 4DU, or email:  psi@nationalarchives.gsi.gov.uk   References   [1] “Nation al Security Strategy and Strategic Defence and  Security Review 2015:  A Secure and Prosperous United  Kingdom”, Cm 9161, Nov 2015   [2]  “Future Operating Environment: Out to 2035”, DCDC,  MOD  [3] Speech by Secretary of Defense Chuck Hagel at Ronald  Reagan Pre sidential Library on 15 Nov 2014  (https://dod.defense.gov/News/Speeches/Speech - View/Article/606635/)   [4] Cukor,D.  “Project Maven: operationalizing machine  learning”.  SPIE Defense + Commercial Sensing;  Ground/Air Multisensor Interoperability, Integration,   and Networking for Persistent ISR IX,  Apr 18   [5] Ramo, S. and StClair, R.K. “The Systems Approach:  Fresh Solutions to Complex Problems Through  Combining Science and Practical Common Sense”,  TRW, 1998   [6] International Councils on Systems Engineering  (INC OSE). (http://www.incose.org/)   [7] Pham, T. et al, “Prevailing in a Complex World: ARL’s  Essential Research Area on AI & ML”,  NATO IST -160  [8] Revolutionise the human information relationship for  Defence.  Defence and Security Accelerator  (https://www.gov .uk/government/publications/accelerato r-themed -competition -revolutionise -the-human - information -relationship -for-defence/competition - document -revolutionise -the-human -information - relationship -for-defence)   [9] Beautement, P. “The Defence Enterprise is More th an  just a Supermarket Chain”, RUSI Defence Systems, Vol  14 No 3, 2012   [10] Pirolli, P. & Card, S., “The Sensemaking Process and  Leverage Points for Analyst Technology” Conference on  Intelligence Analysis (Vol. 5).   Amy, 2005   [11] Speech by Federal Chancel lor Angela Merkel at the  World Economic Forum Annual Meeting in Davos on 24  Jan 2018  (https://www.bundesregierung.de/Content/EN/Reden/20 18/2018 -01-24-bk-merkel -davos_en.html)   [12] Announcement by Defence Secretary Gavin  Williamson at joint US -UK Defence In novation Board.  May, 2018.  (https://www.gov.uk/government/news/flagship -ai-lab- announced -as-defence -secretary -hosts -first-meet - between -british -and-american -defence -innovators)   [13] Speech by Admiral Sir Philip Jones, First Sea Lord at  DSEI 2017, Sep 17  (https://www.gov.uk/government/speeches/dsei -2017 - naval -technology -zone)   [14] Pearson, G. et al, “A Systems Approach to Achieving  the Benefits of Open and Modular systems” SPIE  Defense + Commercial Sensing, 9479 -9, Apr 15   [15] Pearson, G. et al, “Reaping the Benefits of an Open  Systems Approach: Getting the Commercial Approach  Right”, SPIE Defence+ Commercial Sensing, 9849 -12,  Apr 16    Australia’s Approach to AI Governance in Security & Defence   S. Kate Devitt1,2 & Dami an Copeland1  1 University of Queensland   2 Trusted Autonomous Systems   k.devitt@uq.edu.au  ; damian.copeland@uq.edu.au    Acknowledgments: Tim McFarland, Eve Massingham, Rachel Horne  & Tara Roberson     Abstract   Australia is a leading AI nation with strong allies and partnerships. Australia has prioritised  the development of robotics, AI, and autonomous systems to develop sovereign capability for  the military. Australia commits to Article 36 reviews of all new means and method s of warfare  to ensure weapons and weapons systems are operated within acceptable systems of control.  Additionally, Australia has undergone si gnificant reviews of the risks of AI to human rights  and within intelligence organisations and has committed to producing ethics guidelines and  frameworks in Security and Defence. Australia is committed to OECD’s values -based  principles for the responsible  stewardship of trustworthy AI as well as adopting a set of  National AI ethics principles. While Australia has not adopted an AI governance framework  specifically for  the Australian Defence Organisation (ADO) ; Defence Science  and Technology  Group  (DSTG)  has published ‘A Method for Ethical AI in Defence’ (MEAID) technical report  which includes a framework and pragmatic tools for managing ethical and legal risks for  military applications of AI.  Australia can play a leadership ro le by integrating legal and ethical  considerations into its ADO  AI capability acquisition process. This requires a policy  framework that defines its legal and ethical requirements, is informed by Defence industry  stakeholders,  and provides a practical methodology to integrate legal and ethical risk  mitigation strategies into the acquisition process.     2   Keywords : Australia, Article 36, systems of control, AI Ethics, military ethics     Introduction   On 27 February 2021 , Australia’s  Loyal Wingman  military aircraft hinted at  the possibility  of  fully autonomous  flight  at Woomera Range Complex in South Australia  (Royal Australian  Air Force 2021; Insinna 2021) . With no human on board, t he plane used a  pre-programmed  route with remote supervision  to undertake and complete its  mission.  The flight ’s success  and  the Royal Australian Air Force ’s announcement  to order  six aircraft , signalled an  intention to  incorporate artificial intelligence ( AI) to increase military autonomous capability  and  freedom of manoeuvre . Air Vice -Marshal  (AVM)  Cath Roberts  (Head of Air Force  Capability ) said “The Loyal Wingman project is a pathfinder for the integration of  autonomous systems and artificial intelligence to create smart human -machine teams”  (de Git  2021) . AVM Roberts also c onfirmed that “[w]e need to ensure that ethical and legal issues  are resolved at the same pace that the technology is developed ” (Department of Defence  2021b) .    Just over six months later,  on 25th September , Australia1 won the silver medal at the  2021   DARPA Subterranean Challenge , also known as the robot Olympics . In the event, Australia  used multiple robotics platforms  equipped with  AI to autonomously explore, map,  and discover  models representing lost or injured people, suspicious backpacks,  or phones, or navigate tough  conditions such as pockets of gas . The outstanding performance  confirm s Australia’s     1 Team included Commonwealth Scientific and Industrial Research Organisation (CSIRO) Data61 and Emesent;  plus International partner Georgia Institute of Technology     3 international reputation at the forefront of robotics, autonomous systems and AI research and  development (Persley 2021) .   In parallel to technology development, Australia is navigating the challenge of developing  and promoting AI governance structures inclusive of Australian values, standards, ethical and  legal frameworks. National initiatives have been led by:   1. Australia’s n ational research organisation, CSIRO Data61 (Hajkowicz et al. 2019) ,   2. Government (Department of Industry Innovation and Science 2019; Department of  Industry Science Energy and Resources 2021, 2020a, 2 020b)  in the civilian domain;  and   3. Defence Science and Technology Group, Royal Austr alian Air Force and Trusted  Autonomous Systems in Defence (Devitt et al. 2021; Department of Defence 2021b) .     Two large surveys of Australian  attitudes to AI  were conducted  in 2020 . Selwyn and Gallo  Cordoba (2021)  found that , based on  over 2000 respondents,  Australian public attitudes  towards AI are informed  by an educated  awareness of the technolog ies affordances . Attitudes  are generally positive and respondents  are proud of their scientific achievements . However,   Australians  are concern ed about the government’s trustworthiness using automated decision - making algorithms due to incidents such as the Robo debt (Braithwaite 2020)  and #CensusFail  (Galloway 2017)  scandals . A second study  of over 2500 respondents  found that Australians  have low trust in AI systems but generally ‘accept’ or ‘tolerate’ AI (Lockey, Gillespie, and  Curtis 2020 ). They found that Australians trust research institutions and Defence organisati ons  the most to use AI and trust ed commercial organisations  the least.  Australians expect AI to be  regulated and carefully managed  (Lockey, Gillespie, and Curtis 2020) .      4 This chapter will begin with Australia’s strategic position , Australia’s definition of AI and  identifying what ADO  wants AI for. It will then move into AI governance initiatives and  specific efforts to develop frameworks for  ethical AI both in both civilian and military contexts.   The chapter  will conclude  with likely future directions for Australia in AI governance  to  reshape military affairs and  Australia’s role in  international governance  mechanisms  and  strategic  partnerships .     Australia’s strategic position   Australia’s current strategic position has been shaped by two major developments, 1) the  announcement of AUKUS 15 September 2021  (Morrison, Johnson, and Biden 2021)  and 2)  public  commitment to NATO and AUKUS  allies  through  sanctions,  supp ly of humanitarian  support and lethal weapons to the Ukraine  to defend against the invasion of Russia  (Prime  Minister and Minister of Defence 2022, 1 March 2022) .    AUKUS confirmed a shift in strategic interests for the United States to Asia Pacific and away  from the Middle East  with the withdrawal  from Afghanistan.   “Recognizing our deep defense ties, built over decades, today we also  embark on further trilateral collaboration under AUKUS to enhance our joint  capabilities and interoperability. These initial efforts will focus on cyber  capabilities,  artificial intelligence, quantum technologies, and additional  undersea capabilities ” (The Wh ite House 2021b)   Commitment to defend  Ukraine signals a strengthening global alliance amongst liberal  democracies, of which Australia is a part.  The Prime Minister calls the conflict  ““a very big  wake -up call” that reunites liberal democracies to meet a polarising and escalating threat ”  (Kelly 2022) .    5 The 2020 Strategic Update identified new objectives for Australian defence —see Box 1.  Box 1 New Objectives for Australian Defence 2020  (Department of Defence (2020a, 24 - 25)    1. to shape  Australia’s strategic environment;   2. to deter  actions against Australia’s interests; and   3. to respond  with credible military force, when required (Department of Defence  2020a 2.12) .     These new objectives will guide all aspects of ADO’s  planning including force structure  planning, force generation, international engagement and operations (2.13).     To implement these objectives, ADO  will:   • prioritise our immediate region (the  north -eastern Indian Ocean, through maritime  and mainland South East Asia to Papua New Guinea and the South West Pacific) for  the ADF’s geographical focus;   • grow the ADF’s self -reliance for delivering deterrent effects; expand Defence’s  capability to resp ond to grey -zone activities, working closely with other arms of  Government;   • enhance the lethality of the ADF for the sorts of high -intensity operations that are the  most likely and highest priority in relation to Australia’s security; maintain the  ADF’s a bility to deploy forces globally where the Government chooses to do so,  including in the context of US -led coalitions; and   • enhance ADO’s  capacity to support civil authorities in response to natural disasters  and crises.     6   With these in mind, Australia’s global position has been elevated with the announcement of a  new Australia, UK and United States science and technology, industry, and defence partnership  (AUKUS) (Morrison, Johnson, and Bi den 2021) . This partnership is likely to increase data,  information and AI sharing and aligned AI governance structures and interoperability policies  (Deloitte Center for Government Insights 2021)  to manage joint and cooperative  military  action, deterrence, cyber -attacks, data theft, disinformation, foreign interference, economic  coercion, attacks on critical infrastructure, supply chain disruption and so forth (Hanson and  Cave 2021) .     In addition to AUKUS, Australia has a number of strategic partnerships including the global  ‘five -eyes’ n etwork of UK, US, Australia , Canada and New Zealand (Office of the Director of  National Intelligence) ; the Quad surrounding China India, Japan, Australia and US (Shih and  Gearan 2021)  and local partnerships including the Association of Southeast Asian Nations  (ASEAN) (Thi Ha 2021)  and Pacific family (Blades 2021) .      The strategic issues identified in both the Defence White Paper (Department of Defence 2016)   and the 2020 Strategic Update (Department of Defence 2020a)  puts AI among priority  information and co mmunications technology capabilities, e.g.   “3.39 Over the next five years, Defence will need to plan for developments  including next generation secure wireless networks, artificial intelligence,  and augmented analytics ” (Department of Defence 2020a) .     Australia’s definition of artificial intelligence   Australia (Department of Industry Science Energy and Resources 2021)  defines AI as:    7 “AI is a collection of interrelated technologies that can be used to solve  problems autonomously and perform tasks to achieve defined objectives. In  some cases, it can do this without explicit guidance from a human being  (Hajkowicz et al. 2019) . AI is more than just the mathematical algorithms  that enable a computer to learn from text, images or sounds. It is the ability  for a computational system to sen se its environment, learn, predict and take  independent action to control virtual or physical infrastructure. ”   Australia defines AI by its functions (sensing, learning, predicting, independent action), focus  and degree of independence in the achievement of defined objectives with or without explicit  guidance from a human being. The Australian definition encompasses the role of AI in digital  and physical environments without discussing any particular methodology or technology that  might be used. In doing s o it aligns itself with the OECD’s Council on Artificial Intelligence’s  definition of an AI system:   “An AI system is a machine -based system that can, for a given set of human - defined objectives, make predictions, recommendations, or decisions  influencing real or virtual environments. AI systems are designed to operate  with varying levels of autonomy ” (OECD Council on Artificial Intelligence  2019)   What does Australian Defence want artificial intelligence for?   While the  Australian  Department of Defence in Australia has not formally adopted  a Defence  AI Roadmap or Strategy , Australia has prioritised  developing sovereign  AI capabilities —see  Box 2—as well as:  robotics, autonomous systems, precision guided munitions, hypersonic  weapons, and integrated air and missile defence systems; space; and information warfare and  cyber capabilities  (Australian Government 2021) .      8   Box 2 Australian Sovereign Industry Capability Priority:   Robotics, Artificial Intelligence and Autonomous Systems (Australian Government  (2021)     Robotics and autonomous systems are an important element of military capability. They act  as a fo rce multiplier and protect military personnel and assets.     The importance of these capabilities will continue to grow over time. Robotics and  autonomous systems will become more prevalent commercially and in the battlespace.     Australian industry must have the ability to design and deliver robotic and autonomous  systems. This will enhance the ADF\\'s combat and training capability through:   • improving efficiency   • reducing the physical and cognitive load to the operator   • increasing mass   • achieving decision mak ing superiority   • decreasing risk to personnel.     These systems will comprise of:   • advanced robots   • sensing and artificial intelligence encompassing al  gorithms   • machine learning and deep learning.       9 These systems will enhance bulk data analysis. This will facil itate decision making processes  and enable autonomous systems.      Australia notes that AI will play a vital role in ADO’s  future operating environment, delivering  on strategic objectives of shape, deter and respond (Department of Defence 2021a) . AI will  contribute to Australia in maintaining a capable, agile and potent ADO .     Potential military AI applications have been  taxonomized  into warfighting functions (force  application, force protection, force sustainment, situational understanding) and enterprise  functions (personnel, enterprise logistics, business process improvement) —see Annex A   (Devitt et al. 2021) . Operational contexts  help discern the range of purposes of AI within  Defence as well as diverse legal, regulatory,  and ethical structures  required in each domain to  govern  AI use. For example the use of AI in  ensuring abidance with  workplace health and  safety  risk management (Morrison 2021; Centre for Work Health and Safety 2021)  is relevant  to Defence People Group2, whereas  the use of AI within new weapons systems and the Article  36 review process is within the  portfolio for Defence Legal  (Commonwealth of Australia  2018) . AI will also b e needed to manage  Australia’s  grey zone threat  (Townshend, Lonergan,  and Warden 2021) .    The ADO  acknowledges that they need to  effectively use their data holdings  to harness the  opportunities of AI technologies  and t he Defence Artificial Intelligence Centre  (DAIC)  has  been established to accelerate Defence’s AI capability  (Department of Defence 2021a, 35;  2020b) . The ADO  has launched  The AI for Decision Making Initiative  (Defence Science  Institute 2020a, 2021; Defe nce Science & Technology Group 2021a)  and a  Defence Artificial    2 See https://www1.defence.gov.au/about/people -group      10 Intelligence Resea rch Network (DAIRNET)  to develop  AI “to process noisy and dynamic data  in order to produce outcomes to provide decision superiority”  (Defence Science & Technology  Group 2021b) .     These efforts include  some human -centred  projects , such as  human factors for explainable AI   and studies in AI bias  in facial recognition  (Defence Science Institute 2020b) . Defence has  committed to develop  guidelines on the ethical use of data  (Department of Defence 2021a)  and  the Australian Government has committed to  governance and ethical frameworks for the use  of artificial capabilities for intelligence purposes (Attorney -General’s Department 2020,  Recommendation 154) . Ethics guidelines will help Australia respond to  public debate on the  ethics of  facial recognition for military purposes even if biases are reduced (van Noorden  2020) .    AI Governance in Australia   AI governance includes social, legal, ethical and technical layer (algorithms and data) that  require norms, regulation, legislation, criteria, principles, data governance, algorithm  accountability and standards (Gasser and  Almeida 2017) .  Australia ’s strategic, economic,  cultural, diplomatic,  and military  use of AI  will be expected to be governed in accordance with   Australian  attitudes  and values  (such as  described  in Box 3) and international frameworks.     Box 3 Australian Values (Department of Home Affairs 2020)   • Respect for the freedom and dignity of the individual   • Freedom of religion (including the freedom not to follow a particular religion),  freedom of speech and freedom of association     11     Australia is positioning itself to be consistent with emerging best practi ce international ly for a  for ethical, trustworthy (Ministère des Armées 2019) , responsible AI (Fisher 2020)  and allied  frameworks for ethical AI in Defence  (Lopez 2020; Stanley -Lockman 2021) .     To this end, Australia is a founding member of The Global Partnership on AI (GPAI)4, an  international and multi -stakeholder initiative to undertake cutting -edge research and pilot    4 Other GPAI countries include:  Canada, France, Germany, India, Italy, Japan, Mexico, New Zealand, Korea,  Singapore, Slovenia, the United Kingdom, the United States, and the European Union. In December 2020,  Brazil, the Netherlands, Poland, and Spain joined GPAI (Gobal Partnership on AI 2021; Department of Indus try  Science Energy and Resources 2020a) . • Commitment to the rule of law, which means that all people are subject to the law  and should obey it   • Parliamentary democracy whereby our laws are determined by parliaments elected  by the people, those laws being paramount and overriding any other inconsistent  religious or secular ’laws’   • Equality of opportunity for all people, re gardless of their gender, sexual orientation,  age, disability, race or national or ethnic origin   • A ‘fair go’ for all that embraces:   o mutual respect;   o tolerance;   o compassion for those in need;   o equality of opportunity for all   • The English language as the nat ional language, and as an important unifying element  of Australian society     12 projects on AI priorities to advance the responsible development and use of AI built around a  shared commitment to the OECD Recommendation on Artificial Intell igence5. The OECD has  demonstrated considerable “ability to influence global AI governance through epistemic  authority, convening power, and norm - and agenda -setting ” (Schmitt 2021) .     Since 2018 , Australia has used a consultative methodology and public communication of  evidence -based ethics frameworks in both civil and military domains. The civil domain work  driven by CSIRO’s Data61  (Dawson et al. 2019)  and the military work driven by Defence  Science and Technology Group (DSTG) (Devitt et al. 2021) .     AI Ethics  Principles   In 2019 , the Australian Government sought public submissions in response to a CSIRO  Data61AI Ethics discussion paper (Dawson et al. 2019) . A voluntary AI Ethics Framework  emerged  from the  Department of Industry Innovation and Science (2019)  (DISER) to guide  businesses and governments developing and implementing AI in Australia. The framework  includes eight  AI ethics principles (A U-EP) to help reduce the risk o f negative impacts from  AI and ensure the use of AI is underpinned  by good governance standards —see Box 4.       5 GPAI working groups are focussed on four key themes: responsible AI; data governance; the future of work;  and innovation and commercialisation     13   Box 4 Australia’s AI Ethics Principles  (AU-EP) (Department of Industry Innovation and  Science (2019)   1. Human, societal, and environmental wellbeing : AI systems should benefit  individuals, society, and the environment.   2. Human -centred values : AI systems should respect human rights, diversity, and the  autonomy of individuals.   3. Fairness : AI systems should be inclusive and accessible and should not involve or  result in unfair discrimination against individuals, communities, or groups.   4. Privacy protection and security : AI systems should respect and uphold privacy  rights and data protection and ensure the security of data.   5. Reliability and safety : AI systems should reliably operate in accordance with their  intended purpose.   6. Transparency and explainability : There should b e transparency and responsible  disclosure so people can understand when they are being significantly impacted by  AI and can find out when an AI system is engaging with them.   7. Contestability : When an AI system significantly impacts a person, community,  group  or environment, there should be a timely process to allow people to challenge  the use or outcomes of the AI system.   8. Accountability : People responsible for the different phases of the AI system  lifecycle should be identifiable and accountable for the outco mes of the AI systems,  and human oversight of AI systems should be enabled.     Case studies have been undertaken with industry to evaluate the usefulness and effectiveness  of the principles. Many of the findings and due diligence frameworks will be useful in the    14 dialogue between Defence industries, Australian Defence Force and the De partment of  Defence.     Key findings from Industry (Department of Industry Science Energy and Resources 2020b)   include:   • AU-EP are relevant to any organisation involved in AI (private, public, large or small)   • Organisations expect the Australian Government to lead by example and implement  AU-EP.  • Implementing AU -EP can ensure  that businesses can exemplify best practice and be  ready to meet community expectations or any changes in standards or laws   • Ethical issues can be complex , and businesses may need more help from professional  or industry bodies, academia or experts and government.   • Businesses need training and education, certification, case study examples, and cost - effective methods to help them implement and utilise AU -EP.    The case studies revealed that responsibilities of AI purchasers and AI developers differ. Each  group needed internal due diligence, communication and information from external  stakeholders , including vendors or customers , to establish accountability and r esponsibility  obligations. Businesses found some principles more challenging to practically implement. The  advice given by the Government is that businesses ought to document the process of managing  ethical risks (despite ambiguity) and to refer serious is sues to relevant leaders.     To support ethical AI businesses are advised  by DISER  to:  • Set appropriate standards and expectations of responsible behaviour when staff deploy  AI. For example, via a responsible AI policy and supporting guidance.     15 • Include AI app lications in risk assessment processes and data governance  arrangements.   • Ask AI vendors questions about the AI they have developed.   • Form multi -disciplinary teams to develop and deploy AI systems. They can consider  and identify impacts from diverse perspect ives.   • Establish processes to ensure there is clear human accountability for AI -enabled  decisions and appropriate senior approvals to manage ethical risks. For example, a  cross -functional body to approve an AI system’s ethical robustness.   • Increase ethical A I awareness raising activities and training for staff.     The Australian Government commits to continuing to work with agencies to encourage greater  uptake and consistency with AU -EP (Department of Industry Science Energy and Resources  2020b) . In the 2021 AI Action Plan, Australia hopes that widespread adoption of AU -EP among  business, government and academia will build trust in AI  systems (Department of Industry  Science Energy and Resources 2021) . AU -EP are included in the Defence Method for Ethical  AI in Defence report (Appendix A Comparison of Ethical AI Frameworks Devitt et al. 2021,  48-50). However, some have questioned the value of  AU-EP without them being  embedded in  policy , practise  and accountability mechanisms . Contenti ous uses of AI in government such as  facial recognition by police  have not gone through ethical review and do not have institutional  ethical oversight  (ASPI 2022) . While federal frameworks may not have seen effective  operationalisation, NSW  has publish ed their own AI Assurance Framework that mandates  AI  projects  to undergo ethical risk assessment (NSW Government 2022) .    16   Standards   The Future of Humanity Institute at the University of Oxford recommends developing  international standards for ethical AI resear ch and development (Cihon 2019; Dafoe 2018) .  This is consistent with Standards Australia ’s Artificial Intelligence Standards Roadmap:  Making Australia’s Voice Heard  (2020) —see Box 5.     Standards Australia seeks to increase cooperation with  the United States National Institute for  Standards & Technology (NIST) and other Standards Development Organisations (SDOs) .  Australia has a stated aim to participate in ISO/IEC/JTC 1/SC 42, and the National Mirror  Committee (IT -043) regarding AI. Standar ds Australia notes the importance of improving  AI  data quality as well as  ensuring Australia’s adherence to  both domestic and international best  practise in privacy  and security by design.       17   Box 5 Artificial Intelligence Standards Roadmap: Making Australia’s Voice Heard    (Standards Australia (2020)      Recommendations :  1. Increase the membership of the Artificial Intelligence Standards Mirror Committee  in Australia to include participation from more sectors of the economy and society.   2. Explore avenues for enhanced cooperation with the United States National Institute  for Standards & Technology (NIST) and other Standards Development Organisations  (SDOs) with the aim of improving Australia’s knowledge and influence in  international AI Stan dards development.   3. The Australian Government nominate government experts to participate in  ISO/IEC/JTC 1/SC 42, and the National Mirror Committee (IT -043). The Australian  Government should also fund and support their participation, particularly at  internat ional decision -making meetings where key decisions are made, within  existing budgetary means.   4. Australian businesses and government agencies develop a proposal for a direct text  adoption of ISO/IEC 27701 (Privacy Information Management), with an annex  mappe d to local Australian Privacy Law requirements. This will provide Australian  businesses and the community with improved privacy risk management frameworks  that align with local requirements and potentially those of the GDPR, CBPR and  other regional privacy  frameworks.   5. Australian Government stakeholders, with industry input, develop a proposal to  improve data quality in government services, to optimise decision -making, minimise  bias and error, and improve citizen interactions.     18 6. Australian stakeholders channel  their concerns about inclusion, through participating  in the Standards Australia AI Committee (IT -043), to actively shape the development  of an international management system Standard for AI as a pathway to certification.   7. The Australian Government consid er supporting the development of a security -by- design initiative, which leverages existing standards used in the market, and which  recognises and supports the work being carried out by Australia’s safety by -design  initiative.   8. Develop a proposal for a Stand ards hub setup to improve collaboration between  standards -setters, industry certification bodies, and industry participants, to trial new  more agile approaches to AI Standards for Australia .    Human Rights   A report by the Australian Human Rights Commissioner  (AHRC)  (Santow 2021)  concerning  human rights and AI in Australia  makes a suite of recommendations  including the  establishment of an AI Safety Commissioner (Sadler 2021) . Of relevance  to this chapter are  the recommendations to:     • require human rights impact assessment s (HRIA)  before any government department  or agency uses an AI -informed decision -making system to make administrative  decisions  [Recommendation 2]   • the government needs to make AI decision making transparent  and explainable  to  affected individuals and give them rec ourse to challenge the decision   [Recommendations 3, 5, 6, 8].  • AU-EP should be used to encourage corporations and other non -government bodies to  undertake a human rights impact assessment before using an AI -informed decision - making system.     19   Human rights impact assessments of AI in Defence will vary depending on the context of  deployment, e.g. whether the AI is deployed within the war fighting or rear echelon functions.  It is notable that Defence has established  precedence working collaborativ ely with the  Australian Human Rights Commission  (AHRC) , e.g. on ‘Collaboration for Cultural Reform in  Defence’ examining human rights issues including gender, race and diversity, sexual  orientation and gender identity and the impact of alcohol and social m edia on the cultural  reform process (Jenkins 2014) . Thus, it is  possible that Australia could work  again  with the  AHRC on AI  decision making  in the Australian Defence Force . As a lgorithms could unfairly  bias recommendations,  honours and awards, promotion, duties, or postings against particular  groups e.g. women  or LGBTIQ+ . While not documented in the military yet, Tech giants  including Amazon have withdrawn AI powered recruitment when they found that it was biased  against women  (Parikh 2021) . There are also new methods to debias the development , use and  iteration of AI tools in human resources  (Parikh 2021) .     Australian AI Action Plan   In the Australian AI Action Plan (Department of Industry Science Energy and Resources 2021) ,  the Australian government commits to:   • Developing and adopting AI to transform Australian businesses   • Creating an environment to grow and attract the world’s best AI talent   • Using cutting edge AI technologies to solve Australia’s national challenges   • Making Australia a global leader in responsible and inclusive AI       20 To achieve the final point, Australia commits to AI Ethics Principles (Department of Industry  Innovation and Science 2019)  and the OECD Principles (2019) on AI to promote AI that is  innovative, trustworthy and that respects human rights and democratic values.     The OECD AI principles (2019)  are:  1. AI should benefit people and the planet by driving inclusive growth, sustainable  development and well -being.   2. AI systems should be designed in a way that respects the rule of law, human rights,  democratic values and diversity, and they should include appropriate safeguards – for  example, enabling human intervention where necessary – to ensure a fair and just  societ y.  3. There should be transparency and responsible disclosure around AI systems to ensure  that people understand AI -based outcomes and can challenge them.   4. AI systems must function in a robust, secure and safe way throughout their life cycles  and potential ris ks should be continually assessed and managed.   5. Organisations and individuals developing, deploying or operating AI systems should  be held accountable for their proper functioning in line with the above principles.     In 2021, the OECD released a report on how Nations are responding to AI Ethics principles . In  the report , Australia is noted as:   ● deploying a myriad of policy initiatives, including: establishing formal education  programmes on STEM and AI -related fields to empower people with the skills f or AI  and prepare for a fair labour market transition.     21 ● offering fellowships, postgraduate loans, and scholarships to increase domestic AI  research capability and expertise and retain AI talent. Australia has dedicated AUD 1.4  million to AI and Machine Lear ning PhD scholarships   ● Australia and Singapore, building on their pre -existing trade agreement, also signed the  Singapore -Australia Digital Economy Agreement (SADEA) where Parties agreed to  advance their co -operation on AI     Recently the US and Europe confir med commitment to OECD  principles in a joint statement   that:   “The United States and European Union will develop and implement AI  systems that are innovative and trustworthy and that respect universal human  rights and shared democratic values, explore cooperation on AI technologies  designed to enhance privacy protections, and undertake an economic study  examining the impact of AI on the future of our workforces ” (The White  House 2021a) .  Australia is likely to remain  aligned  with the  AI frameworks of allies , particularly the UK (AI  Council 2021)  and the USA  (National Security Commission on Artificial Intelligence 2021) .    AI governance in Defence    While Australia has not released an overarching AI governance framework for Defence, this  chapter outlines an argument for such a framework that draws from publicly released concepts ,  strategy, doctrine, guidelines, papers, reports  and methods relating to human, AI, and data  governance relevant to Defence  and Australia’s strategic position .       22 Australia is a founding partner in the US’s AI Partnership for Defense (PfD ) that includes  Canada, Denmark, Estonia, France, Finland, Germany, Israel, Japan, the Republic of Korea,  Norway, the Netherlands, Singapore, Sweden, the United Kingdom, and the United States  (JAIC Public Affairs 2021, 2020) . In doing so, Australia has aligne d its AI partnerships with  AUKUS, five -eyes (minus New Zealand) , the Quad  (minus India)  and ASEAN via Singapore6.  In particular Australia  is seeking to increase AI collaboration with  the US and UK through  AUKUS  (Nicholson 2021) .     AI in Weapons  Systems   Computer software designed to perform computational or control functions  has been used  in  weapons systems  for over 40  years  (Department of Defense 1978) . Such weapons require  thorough test and evaluation to identify and mitigate risks of computer malfunction.  This has  lead to a recent drive for digital engineering (88th Air Base Wing Public Affairs 2019; National  Security Commission on Artificial Intelligence 2021) . AI and autonomous weapons system  (AWS ) do not necessarily coincide, but the application of Australia’s international and  domestic legal obligations to AI weap on systems will almost certainly affect Australia’s ability  to develop , acquire  and operate autonomous military systems. Austra lia has stated that it   considers a sweeping prohibition of AWS  to be premature  (Australian Permanent Mission and  Consulate -General Geneva  2017; Commonwealth of Australia 2018; Senate Foreign Affairs  Defence and Trade Legislation Committee 2019, 65)  and emphasises the importance in  compliance with the legal obl igation  to undertake Article 36 reviews  to manage the  legal risks  associated with these systems.       6 Note there is little representation from remaining ASEAN nations Brunei, Cambodia, Indonesia, Laos,  Malaysia, Myanmar, the Philippines, Thailand and Vietnam or Pacific Nations     23 Article 36 of the Protocol Additional to the Geneva Conventions of 12 August 1949,  and  relating to the Protection of Victims of International Armed Conflicts, 8 June 1977  (Additional  Protocol 1), provides:   “In the study, development, acquisition or adoption of a new weapon, means or method  of warfare, a High Contracting Party is under an obligation to determine whether its  employment would, in some or all circumstances, be prohibited by this Protocol or by  any other rule of international law applicable to the High Con tracting Party.”   The Article 36 process require s Australia to determine whether  it can meet its  international   legal obligations  in operating AWS . Performing a thorough Article 36 review requires   consideration of International Humanitarian Law (‘IHL’) prohibitions and restrictions on  weapons, including Customary International Law, and an analysis of the normal or expected  use of the AWS against the IHL rules governing the lawful use of weapons  (i.e. distinction,  proportionality and precautions in attack). This includes  ensuring weapon operators understand  their functions and limitations as well as the likely consequences of their use . Thus, users  of  AWS are legally required to be reasonably confident about how they will operate before  deploying them  (Liivoja et al. 2020) .    The ADF Concept for Future robotics and autonomous systems (Vine 2020)  states :  “3.10 Existing international law covers the development, acquisition and  deployment of any new and emerging capability, including future  autonomous weapons systems. ”  “3.44 Australia has submitted two working papers to the LA WS GGE in an  attempt to demonstrate how existing international humanitarian law is  sufficient to regulate current and envisaged weapon systems; the first   (Commonwealth of Australia 2018)  explained the article 36 weapon review    24 process and the second (Australian Government 2019)  outlined the ‘System  of Control’ which regulates the use of force by the ADF. Within the domestic  legal system, the RAS (particularly drones) is being considered in the  development and review of legislation on privacy, int elligence services and  community safety. ”     Australia argues that  “if states uphold existing international law obligations…there is no need  to implement a specific ban on AWS, at this time ” (Commonwealth of Australia 2019) .     However, the 2015 Senate Committee Report on unmanned platforms said ‘ the committee is  not convinced that the use of AWS should be solely governed by the law of armed conflict,  international humanitarian law and existing arms control agreements. A distinct arms control  regime for AWS may be required in the future ” (see para 8.30). The report recommended  that:  “8.33 … the Australian Government support international efforts to establish a  regulatory regime for autonomous weapons systems, including those associated with  unmanned platforms. ”  Australia welcomes discussion (e.g. McFarland 2021, 2020)  around international legal  frameworks on autonomous weapons and how technological advances in weapons systems can  comply wit h international humanitarian law  (Sena te Foreign Affairs Defence and Trade  Legislation Committee 2019) .    Ethical AI Statements Across the Services   Different defence institutions in Australia have addressed the importance of ethical and legal  aspects of AI in their operations. The Royal Australian Navy (2020)  stated that “development  of trusted autonomous systems is expected to increase accuracy, maintain compliance with    25 Navy’s legal and policy obligations as well as regulatory standards, and if utilised during  armed conflict, minimise incidental harm to civili ans”. The Army  (2018)  said it would  “remain cognisant of the ethical, moral and legal issues ar ound the use of RAS technologies  as this strategy evolves and is implemented”. Finally, the Royal Air Force  (RAAF)  (2019, 10 - 11) mentioned that it would explore ways to ensure ethical and moral values and legal  accountabilities remain central, including continuously evaluating which de cisions can be  made by machines and which must be made by humans .  The exploration and pursuit of  augmented intelligence must be transparent and accountable to the RAAF’s  legal, ethical, and  moral values and obligations. Greater engagement with risk and op portunity must be matched  by accountability and transparency .    It is assumed that the governance of AI will dovetail with aspects of human governance ,  particularly where AI augments or replaces human decision -makers , and in some parts  similarly to technology governance and in accordance with best practice in data -governance.   Australia n Defence has confirmed  commitments to non -AI governance of humans and  technology  as detailed in the section below.     Human Governance in Defence   Expectations of human  decision -makers are likely to be applied if not extended whenever AI  influences or replaces human decision -making , including moral and legal responsibilities .  Australian definition of Command  ADDP 00.1 Command  and Control AL1  (Department of  Defence 2019, 1 -1):  Command: The authority which a commander in the military Service lawfully exercises  over subordinates by virtue of rank or assignment.   Notes:     26 1. Command includes the authority and responsibility for effectively using  available resources and for plann ing the employment of organising, directing,  coordinating and controlling military forces for the accomplishment of assigned  missions.   2. It also includes responsibility for health, welfare, morale and discipline of  assigned personne l.    Within the Definiti on of Command is authority and responsibility over military decision  making including the use of physical or digital resources  such as how and when  AI is deployed .  When making decisions, t he Australian Defence Force Leadership Doctrine  (ADF -P-0, Ed.  3)(2021a)  states  “Ethical leadership is the single most important factor in ensuring the  legitimacy of our operations and the support of the Australian people ”.    Suggesting that Command  is expected to deploy  digital assets  ethically , the Leadership   Doctrine argues  in no uncertain terms that “your responsibility as a leader is to ensure the  pursuit of your goals is ethical and lawful. There are no exceptions”  (Australian Defence Force  2021a, 7) . The Australian Defence Force – Philosophic al – 0 Military Ethics  Doctrine  (Australia n Defence Force 2021b)  breaks down ethical leadership  into a framework including  intent, values, evaluate , lawful  and reflect  (see Figure 1 ).       27   Figure 1:  Australian Defence Force Ethical Decision -Making Framework (Figure 5.1,  Australian Defence Force 2021b) .    The Lead the Way: Defence Transformation Strategy  articulates that Defence wants human  decision -makers to be agile, adaptive and ethical  with a continuous improvement culture  “embedding strong Defence values and behaviours, clear accountabilities and informed and  evidence -based decision -making” (Department of Defence 2020c, 21) . ADF -P-7 The  Education Doctrine (Australian Defence Doctrine Publication 2021)  emphasises the  importance of “Innova tive and inquiring minds” that are “better equipped to adapt to fast - changing technological, tactical and strategic environments” . Abilities sought include:   • objectively seek and identify credible information,   • accurately recognise cues and relations hips,   • quickly make sense of information and respond appropriately.   Ethical AI  could contribute and augment human capabilities for a faster and more agile force.        28 Australian  Defence personnel  using AI to augment decision making will be expected to use it  ethically  and lawfully ; to increase the informativeness and evidence -base for decisions ; and for  decision -makers to  agile and  accountable both with and without AI.     Ethics in Australian Cybers ecurity  and Intelligence   Key strategic threat s for Australia are cyber crime , ransomware  and information warfare . Cyber  security incidents are increasing in frequency, scale and sophistication , threatening  Australia’s  economic prosperity and national interests  (White 2021) . AI is likely to play a role in both  decision support and in autonomous defence and  offensive campaigns  to thwart those who seek  to undermine Australia’s interests .    Australia has not published an ethics  of AI  policy  for cybersecurity  or intelligence. However,  ethic al behaviours are highlighted  in publicly available value statement s, such as  “we always  act legally and ethically ” (Australian Signals Directorate 2019a, 2019b)  and communications   suggestive that Australia would expect strong governance of AI systems  used in these  operations  including abidance  with domestic and international law  and the values  of  government organisations.  For example, speaking to the Lowy Institute  Director -General of  Australian Signals Directorate (ASD) , Mike Burgess  (2019)  highlighted  that “rules guide us  when people are watching ; values guide us when they\\'re not ” (Lowy Institute 2019 51:46)  and  that ASD  is “an organi sation that is actually incredibly  focused on doing the right thing by the  public and being lawful that\\'s an excellent part of our culture born out of our values we put a  lot of effort focusing on that ” (Lowy Institute 2019 52:27) .      29 A comprehensive review of the legal framework of the national intelligence community  highlights the importance of accountability , transparency  and oversight of how the Australian  government collects and uses data  (Richardson 2020) .     The government response to the Richardson report  (Attorney -General’s Department 2020, 40 - 41) agrees that governance and ethi cal frameworks should be developed for the use of artificial  capabilities for intelligence purposes  (recommendation 154 ), citing values including control,  oversight, transparency, and accountability  (recommendations 155 -156). The Australian  government noted the importance of human -in-the-loop decision -making where a person’s  rights or interests may be affected or where an agency makes an adverse decision in relation to  a person  (recommendation 155 ).    The Australian Government is also committed  to work ing with businesses on potential   legislative changes including t he role of privacy, consumer and data protection laws  (Cyber  Digital and Technology Policy Division 2020) .     Defence Data Strategy   In 2021 , Defence released a Defence Data Strategy. The strategy  promises  a Data Security  Policy to ensure the adoption of a risk -based approach to data security that allows Defence  more latitude to respond to the increase in grey -zone activities, including cyber -attacks , and  foreign interference, and a renewed focus on data security and storage processes.  Defence  identified ethical considerations as a key component of their data strategy —see Box 6. While  they commit to being  informed by  The Australian Code for the Responsible Conduct of  Research and the National Statement on Ethical Conduct in Research  (Australian Research    30 Council 2020) , neither of these codes provide s any guidance on the development of AI for  Defence or security purposes.       Box 6 Ethical data, Defence Data Strategy 2021 -2023  (Department of Defence 2021a,  42)  Guidelines around the ethical use of data will be developed to ensure we have a shared  understanding of our legislative and ethical responsibilitie s  …  The Australian Code for the Responsible Conduct of Research and the National Statement  on Ethical Conduct in Resea rch will inform these guidelines. The ethical use of data  guidelines will form part of the Defence Human and Animal Research Manual and  policies.     Never theless, Defence  is committed  to producing  training so that personnel are “ equipped to  treat data securely and ethically”  by 2023  (Department of Defence 2021a, 13) .    Framework for Ethical AI in Defence   Australia has  not adopted an ethics framework specifically for AI use in Defence. However, a  Defence Science  and Technology  technical report based on  outcomes from  an evidence -based  workshop7 has recommended a method for ethical AI in Defence  (MEA ID) (Department of  Defence 2021b; Devitt et al. 2021)  and an Australia -specific  framework to guide ethical risk  mitigation. MEAID draws from  the workshop for further consideration and does not represent    7 Workshop held in Canberra 30 Jul to 1 Aug 2019 with 104 people from 45 organisations including  representatives from Defence, other Australian government agencies, the Trusted Autonomous Systems Defence  Cooperative Research Centre (TASDCRC), civil society, universities and Defence industry     31 the views of the Australian Government . Rather than stipulating principles, MEAID  identifi es  five facets of ethical AI  and correspon ding questions to support science and technical  considerations for the potential development of Defence policy, doctrine, research and project  management : Responsibility  – who is responsible for AI?; Governance  – how is AI  controlled?; Trust  – how can AI be trusted?; Law – how can AI be used lawfully? And  Traceability  – How are the actions of AI recorded? —see Figure 2.      Figure 2: Facets of Ethical AI in Defence        32 MEAID notes that facets of ethical AI for Defence and the associated questions align with the  unique concerns and regulatory regimes to which Defence is subject . For example, in times of  conflict, Defence is required to comply with international humanitarian law (IHL, lex specialis)  and international human rig hts law (lex generalis) in armed conflict (jus in bello8). Defence is  also required to comply with international legal norms with respect to the use of force when  not engaged in armed conflict (jus ad bellum) when applying military force.  Australia’s  inclusion of ‘Law’ as a n ethical  facet highlights the values Australia promotes through  abidance  with international humanitarian law, particularly the concepts of proportionality, distinction  and military necessity  which have no direct non -military equivalent and as such requires   consideration of  a specific set of requirements and responsibilities.     Responsibility: Who is responsible for AI?   MEAID  notes two  key challenges  of understanding and responsibility  that must be addressed  when operating with AI systems. Firstly, in order to effectively and ethically employ a given  system (AI or not) , the framework argues that a commander must sufficiently understand its  behaviour and the potential consequences of its operation  (Devitt et al. 2021, 11) . Secondly,  there can be  difficulty in identifying any specific individual responsible for a given decision or  action.      Responsibility for critical decisions is spread across multiple decision -makers offering multiple  opportunities to exercise authority but also to make mistakes. The allocation of ethical and  legal responsibility could be distributed  across  the nodes/agents in the human -AI network  causally relevant for a decision  (Floridi  2016) . However , legal responsibility ultimately lie s  with humans . Additionally , AI could help reduce mistakes and augment human  makers who    8 ‘jus in bello’ usually refers to specifically to IHL even though human rights law still operates in conflict     33 bear responsibility (Ekelhof  2018). Decisions made with the assistance of or by AI are captured  by accountability frameworks including domestic and international law9.     The Department of Defence can examine legal cases of responsibility in the civilian domain to  guide some aspects of the relevant frameworks, e.g. the apportioning of responsibility for the  test-driver in an Uber auto mated  vehicle accident (Ormsby  2019). Defence could also consider  arguments that humans within complex systems without proactive frameworks risk being  caught in moral crumple zones  (Elish  2019) where the locus of responsibility falls on human  operators rather than the broader system  of control within which they operate.  Defence must  keep front of mind  that humans, not AI, have legal responsibili ties, and that Individuals —not  only states —can bear criminal responsibility directly under international law  (Cryer, Robinson,  and Vasiliev 2019) .    Governance – how is AI controlled?   MEAID suggest s that AI creators must consider the context in which AI is to be used and how  AI will be controlled. The point of interface through which control is achieved will vary,  depending on the nature of the system and the operational environment. There must be work  conducted to understand how humans can be capable of operating ethically within machine - based systems of control  in accordance with Australia’s commitment to Article 36 reviews of  all new means and methods of warfare (Commonwealth of Australia 2018)10.      9 There is sometimes considerable uncertainty about exactly how to apply legal  frameworks to decisions made  with significant AI involvement.   10 . The Protocol Additional to the Geneva Conventions of 12 August 1949, and relating to the Protection of  Victims of International Armed Conflicts (Protocol I), 8 June 1977 refers alternately to ‘‘methods or means of  warfare’ ’ (Art. 35(1) and (3), Art. 51(5)(a), Art. 55(1)), ‘‘methods and means of warfare’’ (titles of Part III and  of Section I of Part III), ‘‘means and methods of attack’’ (Art. 57(2)(a)(ii)), and ‘‘weapon, means or method of  warfare’’ (Art. 36) (International Committee of the Red Cross 2006)       34 With regards to the control of lethal autonomous weapons , Australia notes the legal, policy,  technical, and professional forms of controls imposed systematically throughout the ‘life’ cycle  of weapons over nine stages :    System of control of weapons  (Commonwe alth of Australia 2019) :  Stage One: Legal and Policy Framework   Stage Two: Design and Development   Stage Three: Testing, Evaluation and Review   Stage Four: Acceptance, Training and Certification   Stage Five: Pre -deployment Selection   Stage Six: Weapon Use Parameters   Stage Seven: Pre -deployment Certification and Training   Stage Eight: Strategic and Military Controls for the Use of Force   Stage Nine: After -Action Evaluation   MEAID supports  the governance framework of  IEEE’s Ethically Aligned Design  by the  IEEE  Global Initiative on Ethics of Au tonomous and Intelligent Systems (2019) .     Human -machine collaboration should be optimised to safeguard against poor decision -making  including automation bias and/or mistrust of the system  (Hoffman et al. 2018; Alexander 2019) .  AI should provid e confidence  and uncertainty  in the information or choices being offered by  an AI  (Christensen and Lyons 2017; McLellan 2016) .     Trust – how can AI be trusted?   Human -AI systems in Defence need to be trusted by users and operators, by commanders and  support staff and by the military, government,  and civilian population of a nation.  MEAID    35 points out the  High -Level Expert Group on Artificial I ntelligence of the European Union   “believe it is essential that trust remains the bedrock of societies, communities, economies and  sustainable development ” (High -Level Expert Group on Artificial Intelligence 2019) . They  argue that trustworthy AI must be lawful, ethical,  and robust.      MEAID  suggests that trust is a relation between human -huma n, human -machine and machine - machine , consist ing of two components:  competency and integrity. Competence comprises of  skills, reliability and experience; Integrity comprises of motives, honesty and character  (Devitt  2018) . This framework is consistent with  the emphasis on character  and professional  competence  in ADF -P-0 ADF Leadership  Doctrine  (Australian Defence Force 2021a, 4) . It is  noted that  the third value of ADF leadership is understanding , which falls within the  responsibility facet discussed above.     Operators will hold multiple levels of trust in the systems they are using depending on what  aspect of trust is under scrutiny. In some cases, users may develop a reliance on low integrity  technology that they can predict easily, such as using the known flight path of an adversary’s  drone to develop cou ntermeasures. Users may also depend on technologies because of  convenience rather than trust. Finally individual differences exist in the propensity to trust,  highlighting that trust is a relational rather than an objective property.     To be trusted , AI systems need to be safe and secure within the Nation’s sovereign supply  chain. Throughout their lifecycle, AI systems should reliably operate in accordance with their  intended purpose  (Department of Industry Innovation and Science 2019 ).    36   Law: How can AI be used lawfully?   AI developers should be cognisant of the legal obligations within their anticipated use of the  technology. Law within a Defence context has specific ethical considerations that must be  understood. International humanitarian law (IHL) (lex specialis) and in ternational human rights  law (lex generalis) were forged from ethical theories in just war theory jus ad bellum governing  the resort to force, jus in bello regulating the conduct of parties engaged in lawful combat  (Coates  2016), jus post bellum regarding  obligations after combat. The legal frameworks that  accompany Defence activities are human -centred, which should mean that AI compliance with  them will produce more ethical outcomes (Liivoja and McCormack  2016) .     Using AI to augment human decisio n-making could lead to better humanitarian outcomes.  There are many policies and directives that may apply, some of which have the force of law.  In military context , there will also typically be an extant set of rules called the rules of  engagement , which among other things specify the conditions that must be met in order to fire  upon a target.     Legal compliance may be able to be ‘built into’ AI algorithms, but this  relies on legal rules   being sufficiently unambiguous and well specified that they can be encoded as rules that a  computer can interpret and meets stakeholder expectations. In practice, laws are not always  that clear, even to humans.  Laws can intentionally  be created with   ambiguity to provide  flexibility.  In addition, they can have many complicated conditions and have many  interconnections to other laws. Further work is needed to clarify how AI can best enable  abidance with applicable laws.     37   Traceability: How are the actions of AI recorded?   MEAID notes that t here are legislative requirements for Defence to record its decision -making.  However, the increasing use of AI within human -AI systems means the manner of records must  be considered. Records can represent the systems involved, the causal chain of events, and the  humans and AIs that were part of decisions.     MEAID suggests that i nformation needs to be accessible and explanatory; the training and  expertise of humans must be open to scrutiny; and the background theories and assumptions,  training, t est and evaluation process of AIs must be retained. Information on AI systems should  be available and understandable by auditors. Just as some aspects of human decision -making  can be inscrutable , some aspects of the decisions of AIs may remain opaque.  Emer ging  transparency standards  may guide best practise for Defence (Winfield et al. 2021) .    When decisions lead to expected outcomes or positive outcomes, the factors that lead to those  decisions may not come under scrutiny. H owever, when low likelihood and/or negative  outcomes occur , organisations should be able to ‘rewind’ the decision process to understand  what occurred and what lessons might be learned. Noting that decisions made under uncertainty  will always have a chance of producing negative outcomes, even if the decision -making process  is defensible and operators are acting appropriately.     No matter how an AI is deployed in Defence, its data, training, theoretical underpinning,  decision -making models and actions should be recorded and auditable by the appropriate levels  of government and, where appropriate, made available to the public.     38   Method for Ethical AI in Defence   MEAID  recommends  assessing ethical compliance from design to deployment, requiring  repeated testing, prototyping, and reviewing for technological and ethical limitations.  Developers already must produce risk documentation f or technical issues. Similar  documentation for ethical risks ensures developers identify, acknowledge and attempt to  mitigate ethical risks early in the design process and throughout test and evaluation  (Devitt  et  al. 2021).      MEAID closely aligns  to the IEEE Standard Model Process for Addressing Ethical Concerns  During System Design (IEEE 2021) —see Box 7.      Box 7 IEEE 7000 -2021 standard (IEEE 2021)  provides:   • a system engineering standard approach integrating human and social values into  traditional systems engineering and design.   • processes for engineers to translate stakeholder values and ethical considerations into  system requirements and design practices.   • a systematic, transparent, and traceable approach to address ethically - oriented  regulatory obligation s in the design of autonomous intelligent systems.     Australia has developed a practical methodology (Devitt  et al. 2021) that can support AI project  managers and teams to manage ethical risks  in military AI projects  including three tools:   1. An AI Checklist for the development of ethical AI systems   2. An Ethical AI Risk Matrix to describe identified risks and proposed treatment     39 3. For larger  programs, a data item descriptor (DID) for contractors to develop a formal  Legal, Ethical and Assurance Program Plan (LEAPP) to be included in project  documentation for AI programs where an ethical risk assessment is above a certain  threshold  (See APPENDIX G. DATA ITEM DESCRIPTION DID -ENG -SW-LEAPP  Devitt et al. 2021) ).    AI Checklist   The main components of the checklist are:   A. Describe the military context in which the AI will be employed   B. Explain the types of decisions supported by the AI   C. Explain how the AI integrates with human operators to ensure effectiveness  and ethical decision making in the anticipated context of use and  countermeasures to protect against potential misuse   D. Explain framework/s to be used   E. Employ subject matter experts to guide AI development   F. Employ appropriate verification and validation techniques to reduce risk.   Ethical AI Risk Matrix   An Ethical AI Risk Matrix  will:  • Define the activity being  undertak en  • Indicate the ethical facet and topic the activity is intended to address.   • Estimate the risk to the project objectives if issue is not addressed?   • Define specific actions you will undertake to support the activity   • Provide a timeline for the activity   • Define action and activity outcomes   • Identify the responsible party(ies)     40 • Provide t he status of the activity.     Analysis   MEAID offers practical advice and tools for defence industries and Defence to communicate,  document and iterate design specifications for emerging technologies and to identify  operational contexts of use considerate of ethical and legal considerations and obligations.  MEAID also offers entry points to explain system function, capability, and limits to both expert  and non -expert stakeholders to military technologies.     MEAID aims to practically ensure accoun tability for a) considering ethical risks, b) assigning  person (s) to each risk and c) making humans accountable for decisions on how ethics are de - risked. It has been noted on the International Committee of the Red Cross  blog (Copeland and  Sanders 2021)  as establishing an iterative process to engage industry during the design and  acquisition phase of new technologies to increase IHL abidance and reduce civilian harms.     Developed by Defence Science and Technology Group, Plan Jericho Air Force (Department of  Defence 2020b)  and Trusted Autonomous Systems11; the MEAID framework has been adopted  by industry12, is the  ethics framework  used in a case study of Allied Impact (Gaetjens, Devitt,  and Shanahan 2021)  and being  trialled by Australia in  the TTCP AI Strategic Challenge   (Stanley -Lockman 2021, 43 -44).     A side -by-side comparison between AU -EP, MEAID and OECD shows significant overlap in  responsibility and trust, but also gaps where military uses of AI encounter ethical    11 See https://tasdcrc.com.au/    12 See Athena AI at  https://athenadefence.ai/software      41 considerations not applicable to the civilian realm, such as the application of just wa r principles  of distinction and proportionality (Law); and military control of weapons systems  (Governance) —see ANNEX B.  The AU -EP share many similarities with the OECD. For  example, contestability is equivalent to OECD requirement that humans can understa nd and  intervene on AI -based outcomes as well as challenge them     While not a formally adopted view of the Australian government, MEIAD  establishes tools to  assess ethical compliance  that, “[e]ven as an opinion, the Method is the clearest articulation of  ethical AI for defense among the Indo -Pacific allies” (Stanley -Lockman 2021, 21) . As Stanley - Lockman  (2021)  states:   “The [MEAID] tools offer a process to validate that contractors have indeed  taken the ethical risks they identified into account in their design and testing  prior to later acquisition phases ….The incorporation of ethics in design   through the acquisition lifecycle also intend s to build trust in the process and,   by extension, the systems by the time they go into service .”    Like many countries, ADO  undertakes a formal capability acquisition process to assist the  Government to identify and meet its military capability needs. This process, known as the  Product Life Cycle, consist of four phases (strategy and concepts; risk mitigation and  requirement s etting; acquisition; and in -service and disposal) which are separated by  Government decisions gates (Commonwealth of Australia 2020) . This process ensures that  Government’s strategic objectives (see Box 1) drive Defence’s acquisition priorities.     Australia’s Sovereign Industry Capability Priorities (see Box 2) reflect s the Government’s  realisation that future Defence AI capabilities will increasingly rely on research and    42 development in the civil sector. This necessitates closer collaboration between Defence and  Defence industry to ensure the timely delivery of cutting -edge technology the reflects  Australia’s values (see Box 3) and ethical AI principles (see Box 4) and ensures legal and  ethical risks associated with military AI technology are identified and mitigated in the earliest  stages of development.     Australia’s D epartment of Defence may inform Defence industry its legal and ethical  requirements to enable AI developers to introduce design measures to mitigate or remove the  risks before entering the Defence procurement process, rather than attempting to address  legal and ethical risks during the acquisition process. This provides efficacies for both  Defence and industry by allowing industry to better focus their development priorities and  assists Defence in streamlining its AI capability acquisition process.     MEAID p rovides Defence with a practical approach that can readily integrate into the  existing Product Life Cycle process to inform and enable the transfer of legal and ethical AI  technology from Defence industry into Defence. MEAID tools such as the LEAPP provide   Defence with visibility of a contractor’s plan to mitigate legal and ethical risk and, together  with the facets of ethical AI (see Figure 2) and the Article 36 weapon review process, can  inform Government decisions to acquire military AI technology that b oth legal and align with  Australia’s ethical principles.     Australia can play a leadership role by integrating legal and ethical considerations into its  Defence AI capability acquisition process. This requires a policy framework that defines its  legal and ethical requirements, is informed by Defence industry stake holders and provides a    43 practical methodology to integrate legal and ethical risk mitigation strategies into the  acquisition process.     Conclusion   This chapter  explored Australia’s public positioning on AI and AI governance 2018 -2021  through published strategies, frameworks, action plans and government reports. While these  provide a top -down view , high-level national AI strategies may align with the lived experience  of public servants  and personnel  encountering AI  in Defence  or Australian Defe nce Force  commanders and operators using AI systems  (Kuziemski and Misuraca 2020) .     Australia is a leading AI nation with strong allies and partnerships. It has prioritised the  development of robotics, AI, and autonomous systems to develop sovereign capability for the  military. Australia commits to Article 36 reviews of all new means and method of warfare to  ensur e weapons and weapons systems are operated within accep table  systems of control.  Additionally,  the country  has undergone significant reviews of the risks of AI to human rights  and within intelligence organi sations and has committed to producing ethics guidelines and  frameworks in Security and Defence (Department of Defence 2021a; Attorney -General’s  Department 2020) . Australia is committed to OECD’s values -based principles for the  responsible stewardship of trustworthy AI as well as adopting a set of National AI ethics  principles. While Australia has not adopted an AI governance framework specifically for  Defence; A Method for Ethical AI in Defence (MEAID) published by Defence Science includes  a framework and pragmatic tools for managing ethical and legal risks for military applications  of AI.    Key findings of the chapt er are that Australia has formed strong international AI governance  partnerships  likely to reinforce and strengthen strategic partnerships and power relations . Like    44 many nations , Australia’s commitme nt to civilian AI Ethics principles do not provide  military  guidance or governance . The ADO has the opportunity to adopt a robust  AI ethical policy for  security and defence that emphasises commitment to  existing international legal frameworks  and can be applied to AI -driven weapons . A risk -based  ethical AI framework  suited for military  purposes  and aligned with best practise, standards and frameworks internationally  can ensure  defence industries consider ethics -by-design and law -by-design ahead  of the acquisitions  process. Australia should  continue to  inves t, research and develo p AI governance frameworks  to meet the technical potential and strategic requirements of military  uses of AI .           45 Bibliography   88th Air Base Wing Public Affairs, 23 December, 2019, \"Digital engineering transformation  coming to Air Force weapons enterprise,\" https://www.af.mil/News/Article - Display/Article/2046599/digital -engineering -transformation -coming -to-air-force - weapons -enterprise/ .  AI Council. 2021. \"AI Roadmap.\" United Kingdom.  https://www.gov.uk/government/publications/ai -roadmap .  Alexander, Donovan. 2019. \"Is our reliance on technology creating a new dark age?\"  Interesting Engineering , 10 May, 2019. https://interestingengineering.com/is -our- reliance -on-technology -creating -a-new-dark-age.  Army. 2018. \"Robotic and Autonomous Systems Strategy.\"  https://researchcentre.army.gov.au/library/other/robotic -autonomous -systems - strategy .  ASPI, \"Russia –Ukraine war, policing and AI, and an Australian DARPA,\" 4 March, 2022, in  Policy, Guns and Money , https://www.aspistrategist.org.au/policy -guns -and-money - russia -ukraine -war-policing -and-ai-and-an-australian -darpa/ .  Attorney -General’s Department. 20 20. \"Government response to the Comprehensive review  of the legal framework of the National Intelligence Community.\" Accessed 25  September 2021. https://www.ag.gov.au/national -security/publications/government - response -comprehensive -review -legal -framework -national -intelligence -community .  Australian Defence Doctrine Publication. 2021. ADF -P-7 Learning.   Australian Defence Force. 2021a. \"ADF -P-0 ADF Leadership, Edition 3.\" The Forge.  https://theforge.defence.gov.au/adf -philosophical -doctrine -adf-leadership    ---. 2021b. \"ADF -P-0 Military Ethics, Edition 1, 2021.\" Accessed 15 October.  https://theforge.defence.gov.au/ethics .  Australian Government. 2019. \"Australia’s System of Control and applications for  Autonomous Weapon Systems.\" Group of Governmental Experts on Emerging  Technologies in the Area of Lethal Autonomous Weapons Systems, Geneva, 25 –29  March 2019 and 20 –21 August 2019. Accessed 10 September. https://docs - library.unoda.org/Convention_on_Certain_Conventional_Weapons_ - _Group_of_Governmental_Experts_(2019)/CCWGGE.12019WP.2Rev.1.pdf .  ---. 2021. \"Four  new Sovereign Industrial Capability Priorities announced.\" 7 September,  2021. https://business.gov.au/cdic/news -for-defenc e-industry/four -new-sovereign - industrial -capability -priorities -announced .  Australian Permanent Mission and Consulate -General Geneva. 2017. \"Australian Statement -  General Exchange of Views, LAWS GGE 13 -17 November 2017.\"  https://geneva.mission.gov.au/gene/Statement783.html .  Australian Research Council. 2020. \"Codes and Guidelines.\"  https://www.arc.gov.au /policies -strategies/policy/codes -and-guidelines .  Australian Signals Directorate. 2019a. \"ASD Corporate Plan 2019 -2020.\"  https://www.asd.gov.au/sites/default /files/2019 - 08/ASD_Corporate_Plan_final_12.pdf .  ---. 2019b. \"Values.\" Accessed 25 September. https://www.asd.gov.au/about/values .  Blades, Johnny. 2021. \"Aukus pact strikes at heart of Pacific regionalism. \" Radio New  Zealand Pacific , 2021. https://www.rnz.co.nz/international/pacific - news/451715/aukus -pact-strikes -at-heart -of-pacific -regionalism .  Braithwaite, Valerie. 2020. \"Beyond the bubble that is Robodebt: How governments that lose  integrity threaten democracy.\" Australian Journal of Social Issues  55 (3): 242 -259.    46 Burgess, M. 2019. \"What ASD cyber operatives really do to protect Austr alian interests.\" 28  March, 2019. https://www.themandarin.com.au/106332 -mike -burgess -director - general -asd-speech -to-the-lowy -institute/ .  Centre for Work Health and Safety. 2021. \"Ethical use of artificial intelligence in the  workplace - AI WHS Scorecard.\" NSW Government.  https://www.centreforwhs.nsw.gov.au/knowledge -hub/ethical -use-of-artificial - intelligence -in-the-workplace -final-report .  Christensen, James C., and Joseph B. Lyons. 2017. \"Trust between Humans and Learning  Machines: Developing the Gray Box.\" Mechanical Engineering  139 (06): S9 -S13.  https://doi.org/10.1115/1.2017 -Jun-5. https://doi.org/10.1115/1.2017 -Jun-5.  Cihon, Peter. 2019. \"Standards for AI governance: international standards to enable global  coordination in AI research & development.\" Future of Humanity Institute. University  of Oxford . https://www.fhi.ox.ac.uk/wp -content/uploads/Standards_ -FHI-Technical - Report.pdf    Commonwealth of Australia. 2018. \"The Australian Article 36 Review Process.\" United  Nations Group of Governmental Experts of the High Contracting Pa rties to the  Convention on Prohibitions or Restrictions on the Use of Certain Conventional  Weapons Which May Be Deemed to Be Excessively Injurious or to Have  Indiscriminate Effects, 30 August 2018. https://docs - library.unoda.org/Convention_on_Certain_Conventional_Weapons_ - _Group_of_Governmental_Experts_(2018)/2018_GGE%2BLAWS_Aug ust_Working %2Bpaper_Australia.pdf .  ---. 2019. \"Australia’s System of Control and applications for Autonomous Weapon  Systems.\" Group of Governmental Experts on Emerging Technologies in the Area of  Lethal Autonomous Weapons Systems, 25 –29 March 2019 and 20 –21 August 2019,  Geneva, 26 March 2019.  https://www.unog.ch/80256EDD006B8954/(httpAssets)/16C9F75124654510C12583 C9003A4EBF/$fil e/CCWGGE.12019WP.2Rev.1.pdf .  ---. 2020. Capability Life Cycle Manual (V.2.1). edited by Investment Portfolio Management  Branch.   Copeland, D., and L. Sanders. 2021. \"Engaging with the industry: integrating IHL into new  technologies in urban warfare.\" Humani tarian Law and Policy  (blog),  ICRC . 8  October. https://blogs.icrc.org/law -and-policy/2021/10/07/industry -ihl-new- technologies/ .  Cryer, Robert, Darryl Robinson, and Sergey Vasiliev. 2019. An introduction to international  criminal law and procedure . Cambridge University Press.   Cyber Digital and Technology Policy Division. 2020. \"2020 Cyber Security Strategy.\"  Department of Home Affairs. https://www.homeaffairs.gov.au/about -us/our - portfolios/cyber -security/strategy .  Dafoe, Allan. 2018. \"AI governance: a research agenda.\" Governance of AI Program, Future  of Humanity Institut e, University of Oxford: Oxford, UK  1442: 1443.   Dawson, D., E. Schleiger, J. Horton, J. McLaughlin, C. Robinson, G. Quezada, J. Scowcroft,  and S. Hajkowicz. 2019. Artificial Intelligence: Australia’s Ethics Framework: A  Discussion Paper. Data61 CSIRO, Aust ralia (Data61 CSIRO, Australia: Australia  Data61 CSIRO). https://consult.industry.gov.au/strategic -policy/artificial -intelligence - ethics -framework/ .  de Git, Melanie 2021. \"Loyal Wingman uncrewed aircraft completes first flight.\" Innovation  Quarterly, Boeing , 12 April, 2021. https://www.boeing.com/features/in novation - quarterly/2021/04/loyal -wingman.page .    47 Defence Science & Technology Group, 18 November, 2021a, \"AI to enable military  commanders to make better decisions,\"  https://www.dst.defence.gov.au/news/2021/11/18/ai -enable -military -commanders - make -better -decisions -faster .  ---. 2021b. \"Defence Artificial Intelligence Research Network (DAIRNET) Research Call.\"  Accessed 1 November. https://www.dst.defence.gov.au/partner -with- us/opportunities/defence -artificial -intelligence -research -network -dairnet -research -call.  Defence Science Institute, 19 May, 2020a, \"The Artificial Intelligence for Decision Making  Initiative,\" https://www.defencescienceinstitute.com/news/the -artificial -intelligence - for-decision -making -initiative .  ---, 2020b, \"‘Artificial Intelligence for Decisio n Making’ Initiative,\"  https://www.defencescienceinstitu te.com/component/sppagebuilder/?view=page&id= 29&highlight=WyJhcnRpZmljaWFsIiwiJ2FydGlmaWNpYWwiLCJpbnRlbGxpZ2Vu Y2UiLCJhcnRpZmljaWFsIGludGVsbGlnZW5jZSJd .  ---, 10 May, 2021, \"Applications open for the artificial intelligence for decision making  initiative rou nd 2,\"  https://www.defencescienceinstitute.com/news/initiatives/applications -open -for-the- artificial -intelligence -for-decision -making -initiative -round -2.  Deloitte Center for Government Insights. 2021. \"The future of warfighting.\" Deloitte.  https://www2.deloitte.com/global/en/pages/public -sector/articles/future -of- warfighting.html .  Department of Defence. 2016. \"Defence White Paper.\"  https://www1.defence.gov.au/about/publications/2016 -defence -white -paper .  ---. 2019. ADDP 00.1 Command and Control AL1. edited by Department of Defence.   ---. 2020a. \"2020 Def ence Strategic Update.\"  https://www1.defence.gov.au/about/publications/2020 -defence -strategic -update .  ---. 2020b. \"Artificial intelligence enhances the impact of a ir and space power for the Joint  Force.\" Department of Defence Annual Report 2019 -2020.  https://www.transparency.gov.au/annual -reports/department -defence/reporting - year/2019 -20-31   ---. 2020c. \"Lead the Way: Defence Transformation Strategy.\"  https://www1.defence.gov.au/about/publications/lead -way-defence -transformation - strategy .  ---. 2021a. \"Defence Data Strategy 2021 -2023.\"  https://www1.defence.gov.au/about/publications/defence -data-strategy -2021 -2023 .  ---, 16 February, 2021b, \"Defence releases report on ethical use of AI,\"  https://news.defence.gov.au/media/media -releases/defence -releases -report -ethical - use-ai.  Department of Defense. 1978. Managing Weapon System Software: Progress and Problems  (Unclassified Digest of a Classified Report).   Department of Home Affairs. 2020. \"Australian Values.\"  https://www.homeaffairs.gov.au/about -us/our -portfolios/social -cohesion/australian - values .  Department of Industry Innovation and Science. 2019. \"Australia’s Artificial Intelligence  Ethics Framework.\" Accessed 25 September. https://www.industry.gov.au/data -and- publications/australias -artificial -intelligence -ethics -framework/australias -ai-ethics - principles .  Department of Industry Science Energy and Resources, 16 June 2020, 2020a, \"The Global  Partnersh ip on Artificial Intelligence launches,\"    48 https://www.industry.gov.au/news/the -global -partnership -on-artificial -intelligence - launches .  ---. 2020b. \"T esting the AI Ethics Principles.\" Accessed 11 December  https://www.industry.gov.au/data -and-publications/ australias -artificial -intelligence - ethics -framework/testing -the-ai-ethics -principles .  ---. 2021. \"Australia’s Artificial Intelligence Action Plan.\" https://www.industry.gov.au/data - and-publications/australias -artificial -intelligence -action -plan.  Devitt, S K 2018. \"Trustworthiness of autonomous systems.\" In Foundations of Trusted  Autonomy , edited by Hussein A. Abbass, Jason Scholz and Darryn J. Re id, 161 -184.  Cham: Springer International Publishing.   Devitt, S K, M Gan, J Scholz, and R S Bolia. 2021. A Method for Ethical AI in Defence.  Defence Science and Technology Group (Defence Science and Technology).  https://www.dst.defence.gov.au/publication/ethical -ai.  Fisher, Erik. 2020. \"Necessary conditions for responsible innovation.\" Journal of Responsible  Innovation  7 (2): 145 -148. https://doi.org/10.1080/23299460.2020.1774105 .  https://doi.org/10.1080/23299460.2020.1774105 .  Gaetjens, D., S.K. Devitt, and C. Shanahan. 2021. Ethical AI in Defence Case Study: Allied  Impact. DST Technical Report : Defence Science & Technology Group.   Galloway, Kate. 2017. \"Big Data: A case study of disruption and government power.\"  Alternative Law Journal  42 (2): 89 -95.  Gasser, Urs, and Virgilio AF Almeida. 2017. \"A layered model for AI g overnance.\" IEEE  Internet Computing  21 (6): 58 -62.  Gobal Partnership on AI. 2021. \"The Global Partnership on AI \". Accessed 25 September  https://gpai.ai/ .  Hajkowicz, S A, S Karimi, T  Wark, C  Chen, M  Evans, N Rens, D  Daw son, A  Charlton, T   Brennan, C Moffatt, S Srikumar, and K J  Tong. 2019. Artificial Intelligence: Solving  problems, growing the economy and improving our quality of life. (CSIRO Data61  and the Department of Industry, Innovation and Science, Australian Gov ernment).  https://data61.csiro.au/en/Our -Research/Our -Work/AI -Roadmap .  Hanson, Fergus, and Danielle Cave. 2021. \"Australia well placed to turbocharge its strategic  tech capability.\" Australian Strategic Policy Institute. Last Modified 20 September.  https://www.aspi.org.au/opinion/australia -well-placed -turbocharge -its-strategic -tech- capability .  High -Level Expert Group on Artificial Intelligence. 2019. Ethics Guidelines for Trustworthy  AI. European Commission. https://ec.europa.eu/digital -single -market/en/news/ethics - guidelines -trustworthy -ai.  Hoffman, Robert R., Nadine Sarter, Matthew Johnson, and John K. Hawley. 2018. \" Myths of  automation and their implications for military procurement.\" Bulletin of the Atomic  Scientists  74 (4): 255 -261. https://doi.org/10.1080/00963402.2018.1486615 .  IEEE. 2021. \"IEEE 7000™ -2021 - IEEE Standard Model Process for Addressing Ethical  Concerns During System Design.\" https://engagestandards.ieee.org/ieee -7000 -2021 - for-systems -design-ethical -concerns.html .  IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. 2019. Ethically  Aligned Design: A Vision for Prioritizing Human Well -being with Autonomous and  Intelligent Systems (EADe1). IEEE. https://standards.ieee.org/content/ieee - standards/en/industry -connections/ec/autonomous -systems.html .  Insinna, Valerie. 2021. \"Australia makes another order for Boein g’s Loyal Wingman drones  after a successful first flight.\" DefenceNews , 3 March, 2021.  https://ww w.defensenews.com/air/2021/03/02/australia -makes -another -order -for- boeing -made -loyal -wingman -drones -after-a-successful -first-flight/ .    49 International Committee of the Red Cross. 2006. \"A Guide to the Legal Review of New  Weapons, Means and Methods of Warfare: Measures to Implement Article 36 of  Additional Protocol I of 1977.\" International Review of the Red Cross  88 (864).  https://www.icrc.org/eng/assets/files/other/irrc_864_icrc_geneva.pdf .  JAIC Public Affairs, 16 September 2020, 2020, \"JAIC facilitates first -ever International AI  Dialogue for Defense,\" https://www.ai.mil/news_09_16_20 -jaic_facilitates_first - ever_international_ai_dialogue_for_defense_.html .  ---, 28 May, 2021, \"DoD Joint AI  Center Facilitates Third International AI Dialogue for  Defense,\" https://www.ai.mil/news_05_28_21 - jaic_facilitates_third_international_ai_di alogue_for_defense.html .  Jenkins, K. 2014. \"Collaboration for cultural reform in Defence.\"  https://defence.humanrights.gov.au/ .  Kelly, Paul. 2022. \"New world disorder: Ukraine redefines global landscape.\" 4 March, 2022.  https://www.theaustralian.com.au/inquirer/a -new-world -disorder -morrison -calls-on- the-west-to-unite -against -russia -and-china/news - story/433b6ff74637c45454393030f827f1e7 .  Kuziemski, Maciej, and Gianluca Misuraca. 2020. \"AI governance in t he public sector: Three  tales from the frontiers of automated decision -making in democratic settings.\"  Telecommunications policy  44 (6): 101976.   Liivoja, Rain, Eve Massingham, Tim McFarland, and Simon McKenzie. 2020. \"Are  Autonomous Weapons Systems Prohibi ted?\". Game Changer. Trusted Autonomous  Systems https://tasdcrc.com.au/are -autonomous -weapons -systems -prohibited/ .  Lockey, S., N. Gillespie, and C.  Curtis. 2020. Trust in ar tificial intelligence: Australian  insights 2020. (The University of Queensland and KPMG Australia).  https://assets.kpmg/content/dam/kpmg/au/pdf/2020/public -trust-in-ai.pdf.  Lopez, C.T. 2020. \"DOD Adopts 5 Principles of Artificial Intelligence Ethics.\" DOD News ,  2020. https://www.defense. gov/Explore/News/Article/Article/2094085/dod -adopts -5- principles -of-artificial -intelligence -ethics/ .  Lowy Institute. 2019. \"Mike Burgess, Director -General on the Australian Signals Directorate  (ASD) – Offensive cyber\" YouTube. https://youtu.be/Th6EKCwhGrs .  McFarland, Tim. 2020. Autonomous Weapon Systems and the Law of Armed Conflict:  Compatibility with International Humanitarian Law . Cambridge Core.   ---. 2021. \"Autonomous Weapons and The Jus Ad Bellum.\" Law Schoo l Policy Review .  https://lawschoolpolicyreview.com/2021/03/20/autonomous -weapons -and-the-jus-ad- bellum -an-overview/ .  McLellan, Charles. 2016.  \"Inside the black box: Understanding AI decision -making.\" ZDNet ,  1 December, 2016. https://www.zdnet.com/article/inside -the-black -box- understanding -ai-decision -making/ .  Ministère des Armées. 2019. Artificial Intelligence in Support of Defence: Report of the AI  Task Force.   Morrison, B. 2021. \"How will artificial intelligence and machine learning impact OHS?\".  Australian Institute for Health and Safety. Last Modified 21 June.  https://www.aihs.org.au/news -and-publications/news/how -will-artificial -intelligence - and-machine -learning -impact -ohs.  Morrison, S, B Johnson, and J Biden. 2021. \"Remarks by President Biden, Prime Minister  Morrison of Australia, and Prime Minister Johnson of the United Kingdom  Announcing the Creation of AUKUS.\" The White House.  https://www.whitehouse.gov/briefing -room/speeches -remarks/ 2021/09/15/remarks - by-president -biden -prime -minister -morrison -of-australia -and-prime -minister -johnson - of-the-united -kingdom -announcing -the-creation -of-aukus/ .    50 National Security Commission on Artificial Intelligence. 2021. \"Final Report.\" United States  of America. https://www.nscai.gov/wp -content/uploads/2021/03/Full -Report -Digital - 1.pdf .  Nicholson, B. 2021. \"Morrison says AUKUS will strengthen cooperation on critical  technologies.\" The Strategist, Australian Strategic Policy Institute , 17 November,  2021. https://www.aspistrategist.org.au/morrison -says-aukus -will-strengthen - cooperation -on-critical -technologies/ .  NSW Government. 2022. \"NSW AI Assurance Framework.\" NSW Government. Accessed 5  March. https://www.digital.nsw.gov.au/policy/artificial -intelligence/nsw -ai-assurance - framework .  OECD. 2019. \"The OECD AI Principles.\" https://www.oecd.org/going -digital/ai/principles/ .  OECD Council on Artificial Intelligence. 2019. Recommendation of the Council on Artificial  Intelligence.   Office of the Director of National Intelligence. \"Five Eyes Intelligence Oversig ht and Review  Council (FIORC).\" Accessed 25 September. https://www.dni.gov/index.php/ncsc - how-we-work/217 -about/organization/icig -pages/2660 -icig-fiorc .  Parikh, Nish. 2021. \"Understanding Bias In AI -Enabled Hiring.\" Forbes Magazine , 2021.  https://www.forbes. com/sites/forbeshumanresourcescouncil/2021/10/14/understandin g-bias-in-ai-enabled -hiring/?sh=33f2d7997b96 .  Persley, Alexandra, 2021, \"Australia claims historic top two spot in the \\'Robot Olympics\\',\"  https://www.csiro.au/en/news/News -releases/2021/Australia -claims -historic -top-two- spot-in-the-Robot -Olympics .  Prime Minister, and Minister of Defence. 2022, 1 March 2022. \"Australian Suppo rt to the  Ukraine.\" Prime Minister of Australia. Accessed 5 March 2022.  https://www.pm.gov.au/media/australian -support -ukraine .  Richardson, D. 2020. \"Report of the comprehensive review of the legal framework of the  national intelligence community.\" Attorney -General’s Department. Accessed 25  September. https://www.ag.gov.au/national -security/consultations/comprehensive - review -legal -framework -governing -national -intelligence -community .  Royal Australian Air Force. 2019. \"At the edge: Exploring and exploiting our fifth -generation  edges.\" https://www.airforce.gov.au/our -mission/plan -jericho .  ---. 2021. \"Loyal Wingman First Flight.\" YouTube , 2 March, 2021.  https://youtu.be/BiSHVl7UMRk .  Royal Australian Navy. 2020. \"RAS -AI Strategy 2040: Warfare Innovation Navy.\"  https://www.navy.gov.au/media -room/publications/ras -ai-strategy -2040 .  Sadler, Denham. 2021. \" HRC calls for an AI Safety Commissioner.\" InnovationAus , 27 May,  2021. https://www.innovationaus.com/hrc -calls-for-an-ai-safety -commissioner/ .  Santow, E. 2021. \"Human Ri ghts and Technology Final Report.\" Australian Human Rights  Commission. https://tech.humanrights.gov.au/downloads .  Schmitt, Lewin. 2021. \"Mapping global AI governance: a nascent regime in a fragmente d  landscape.\" AI and Ethics . https://doi.org/10.1007/s43681 -021-00083 -y.  https://doi.org/10.1007/s43681 -021-00083 -y.  Selwyn, Neil, and Bea triz Gallo Cordoba. 2021. \"Australian public understandings of  artificial intelligence.\" AI & SOCIETY : 1-18.  https://link.springer.com/article/10.1007/s00146 -021-01268 -z.  Senate F oreign Affairs Defence and Trade Legislation Committee. 2019. \"Official Committee  Hansard Senate Foreign Affairs, Defence and Trade Legislation Committee Estimates  Wednesday, 23 October 2019.\"  https://parlinfo.aph.gov.au/parlInfo/search/display/display.w3p;query=Id%3A%22co mmittees%2Festimate%2F53068544 -efe7-4494 -a0f2-2dbca4d2607b%2F0000%22 .    51 Shih, Gerry, and Anne Gearan. 2021. \"As Biden hosts first Quad summit at the White House,  China is the background music.\" The Washington Post , 24 September, 2021.  https://www.washingtonpost.com/world/2021/09/24/quad -us-india -australia -japan - china/ .  Standards Australia. 2020. Artificial Intelligence Standards Roadmap: Making Australia’s  Voice Heard.  https://www.standards.org.au/news/standards -australia -sets-priorities - for-artificial -intelligence .  Stanley -Lockman, Z. 2021. \"Responsible and Ethical Military AI Allies and Allied  Perspectives: CSET Issue B rief.\" Centre for Security and Emerging Technology,  Georgetown University’s Walsh School of Foreign Service.  https://cset.georgetown.edu/wp -content/ uploads/CSET -Responsible -and-Ethical - Military -AI.pdf .  The White House. 2021a. \"FACT SHEET: U.S. -EU Establish Common Principles to Update  the Rules for the 21st Century Economy at Inaugural Trade and Technology Council  Meeting.\" https://www.whitehouse.gov/briefing -room/statement s- releases/2021/09/29/fact -sheet -u-s-eu-establish -common -principles -to-update -the- rules -for-the-21st-century -economy -at-inaugural -trade -and-technology -council - meeting/ .  ---. 2021b. \"Joint Leaders Statement on AUKUS.\" Last Modified 15 September 2021.  https://www.whitehouse.gov/briefing -room/statements -releases/2021/09/15/joint - leaders -statement -on-aukus/ .  Thi Ha, Hoang 2021. \"The Aukus challenge to Asean.\" The Straits Times , 2021.  https://www.straitstimes.com/opinion/the -aukus-challenge -to-asean .  Townshend, Ashley, Thomas Lonergan, and Toby Warden. 2021. \"The U.S. -Australian  Alliance Needs a Strategy to Deter China’s Gray -Zone Coercion.\" War on the Rocks ,  2021. https://warontherocks.com/2021/09/the -u-s-australian -alliance -needs -a- strategy -to-deter -chinas -gray-zone -coercion/ .  van Noorden, Richard. 2020. \"The ethical questions that haunt facial -recognition research.\"  Nature , 18 November, 2020. https://www.nature.com/articles/d41586 -020-03187 -3.  Vine, R. 2020. Concept for robotics and autonomous systems. Australian Defence Force.  https://www.defence.gov.au/vcdf/forceexploration/adf -concept -future -robotics - autonomous -systems.asp .  White, L. 2021. \"Tackling the growing threat s to Australia’s cyber security.\" 2021 Mandarin  Defence Special Report , 2021. https://www.themandarin.com.au/169281 -tackling -the- growing -threat s-to-australias -cyber -security/ .  Winfield, Alan F. T., Serena Booth, Louise A. Dennis, Takashi Egawa, Helen Hastie, Naomi  Jacobs, Roderick I. Muttram, Joanna I. Olszewska, Fahimeh Rajabiyazdi, Andreas  Theodorou, Mark A. Underwood, Robert H. Wortham, and El eanor Watson. 2021.  \"IEEE P7001: A Proposed Standard on Transparency.\" Frontiers in Robotics and AI  8  (225). https://doi.org/10.3389/frobt.2021.665729 .  https://www.frontiersin.org/article/10.3389/frobt.2021.665729 .             52 ANNEX A  Contexts of AI in Defence   Combat/Warfighting     Tag Force Application (FA)   Description  The conduct of military missions to achieve decisive effects through kinetic  and non -kinetic offensive means.   AI examples  Autonomous weapons (AWs) and autonomous/semi -autonomous combat  vehicles and subsystems   AI used to support strategic, operational and tactical planning, including  optimisation an d deployment of major systems   AI used in modelling and simulation used for planning and mission rehearsal   AI used in support of the targeting cycle including for collateral damage  estimation   AI used for Information Warfare such as a Generative Adversarial Network  (GAN -) generated announcement or strategic communication   AI used to identify potential vulnerabilities in an adversary force to attack   AI used for discrimination of combatants and non -combatants        53 Tag Force Protection (FP)   Description  All measures to counter threats and hazards to, and to minimise vulnerabilities  of, the joint force in order to preserve freedom of action and operational  effectiveness   AI examples  Autonomous defensive systems (i.e. Close in Weapons Systems)   AI used for Cyber Network Defence   AI used to develop and employ camouflage and defensive deception systems  and techniques   Autonomous decoys and physical, electro -optic or radio frequency  countermeasures   AI to identify potential vulnerabilities in a friendly force that  requires  protection   AI used to simulate potential threats for modelling and simulation or rehearsal  activities   Autonomous Medical Evacuation/Joint Personnel Recovery systems       Tag Force Sustainment (FS)   Description  Activities conducted to sustain fielded forces, and to establish and maintain  expeditionary bases. Force sustainment includes the provision of personnel,  logistic and any other form of support required to maintain and prolong  operations until accomplishment of the mission.   AI examples  Autonomous combat logistics and resupply vehicles   Automated combat inventory management     54 Predictive algorithms for the expenditure of resources such as fuel, spares and  munitions   Medical AI systems used in combat environments and expeditionary bases   Predicti ve algorithms for casualty rates for personnel and equipment   Algorithms to optimise supply chains and the recovery, repair and  maintenance of equipment   Algorithms to support the provision of information on climate, environment  and topography   AI used for ba ttle damage repair and front -line maintenance          55 Tag Situational Understanding (SU)   Description  The accurate interpretation of a situation and the likely actions of groups and  individuals within it. Situational Understanding enables timely and accurate  decision making.   AI examples  AI that enables or supports Intelligence, Surveillance and Reconnaissance  (ISR) activities including:   object recognition and categorisation of still and full motion video   removal of unwanted sensor data   identification of enemy deception activities   anomaly detection and alerts   monitoring of social media and other open -source media channels   optimisation of collection assets   AI that fuses data and disseminates intelligence to strategic, operational and  tactical decision makers   Decision support tools   Battle Management Systems   AI that supports Command and Control functions   Algorithms used to predict likely actions of groups and individuals   AI used to assess individual and collective behaviour and attitudes     Enterprise -level and Rear Echelon Functions   Tag Personnel (PR)   Description  All activities that support the Raising, Training and Sustaining (RTS) of  personnel.   AI examples  AI used for Human Resource Management  including:     56 record keeping   posting and promotion   disciplinary and performance management   recruitment and retention   modelling of future personnel requirements   prediction of HR supply and demand events and anomalies   AI used in individual and collective training and education  including  modelling a nd simulation   AI used for testing and certification of personnel   AI used to model the capability and preparedness of permanent and reserve  personnel       Tag Enterprise Logistics (EL)   Description  Activities that support rear -echelon enterprise -level logistics functions  including support of permanent military facilities   AI examples  Autonomous rear -echelon supply vehicles and warehouses   AI used for optimisation of rear -echelon supply chains and inventory  management   AI used in depot -level and intermediate maintenance, including:   Digital twinning   Predictive maintenance   Global supply chain analysis, prediction and optimisation   Enterprise -level analysis and prediction for resource demand and supply (i.e.  national/strategic fuel requirements )    57 AI used in the day -to-day operation of permanent military facilities       Tag Business Process Improvement (BP)   Description  Activities that support rear -echelon administrative business processes that are  not related to personnel or logistics.   AI examples  AI used for Information Management and record -keeping   Informational assistants such as policy chatbots   AI that supports management of policy and procedures   AI used to optimise business and administrative processes, including  modelling and simulati on tools   AI used for enterprise business planning at the strategic, operational and  tactical level          58 ANNEX  B Side-by-Side Comparison of AI Ethic s Frameworks   Facets of Ethical AI  in Defence    Australian Government ’s AI  Ethics Principles   OECD     RESPONSIBILITY:   Who is responsible  for AI?    Human, social and  environmental wellbeing:  Throughout their lifecycle, AI  systems should benefit  individuals, society and the  environment   Human -centred values:  Throughout their lifecycle, AI  systems should respe ct human  rights, diversity, and the  autonomy of individuals      1. AI should benefit people and  the planet by driving inclusive  growth, sustainable  development and well -being.   2. AI systems should be  designed in a way that respects  … human rights, democratic  values and diversity,       GOVERNANCE:   How is AI  controlled?    Accountability: Those  responsible for the different  phases of the AI system  lifecycle should be identifiable  and a ccountable for the  outcomes of the AI systems,  and human oversight of AI  systems should be enabled  5. Organisations and  individuals developing,  deploying or operating AI  systems should be held  accountable for their proper  functioning in line with the  above principles     59 Transparency and  explainability: There should be  transparency and responsible  disclosure to ensure people  know when they are being  significantly impacted by an AI  system, and can find out whe n  an AI system is engaging with  them  Contestability: When an AI  system significantly impacts a  person, community, group or  environment, there should be a  timely process to allow people  to challenge the use or o utput  of the AI system    3. There should be  transparency and responsible  disclosure arou nd AI systems  to ensure that people  understand AI -based outcomes  and can challenge them.   TRUST:   How can AI be  trusted?  Reliability and safety:  Throughout their lifecycle, AI  systems should reliably operate  in accordance with their  intended purpose      Fairness: Throughout their  lifecycle, AI systems should be 2. [AI systems]  should include  appropriate safeguards – for  example, enabling human  intervention where necessary –  to ensure a fair and just  society.       60 inclusive and accessible, and  should not involve or result in  unfair discrimination against  individuals, communities or  groups      Privacy protection and  security: Throughout their  lifecycle, AI syst ems should  respect and uphold privacy  rights and data protection, and  ensure the security of data      4. AI systems must function in  a robust, secure and safe way  throughout their life cycles and  potential risks should be  continually assessed and  managed.       LAW:   How can AI be used  lawfully?    No equivalent  2. AI systems should be  designed in a way that respects  the rule of law   TRACEABLILITY:    How are the actions  of AI recorded?    No equivalent (but implied)      No equivalent (but implied)    '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_papers(query: str) -> str:\n",
    "    # \"\"\" \n",
    "    # Downloads and processes papers from arXiv based on the query.\n",
    "\n",
    "    # Args:\n",
    "    #     query (str): The search query to fetch papers.\n",
    "\n",
    "    # Returns:\n",
    "    #     str: The concatenated content of all papers.\n",
    "  \n",
    "    # Replace spaces with underscores in the query to create a valid directory name\n",
    "        # Set up the directory path relative to the current working directory\n",
    "    base_dir = os.getcwd()\n",
    "    dirpath = os.path.join(base_dir, f\"arxiv_papers_for_{query.replace(' ', '_')}\")\n",
    "\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "\n",
    "\n",
    "    # Initialize arxiv client and search for papers\n",
    "    client = arxiv.Client()\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=2,\n",
    "        sort_order=arxiv.SortOrder.Descending\n",
    "    )\n",
    "\n",
    "   # Download and save the papers\n",
    "    for result in client.results(search):\n",
    "        while True:\n",
    "            try:\n",
    "                paper_id = result.get_short_id()\n",
    "                # Truncate and sanitize title to avoid overly long filenames\n",
    "                title = result.title.replace(' ', '_').replace('/', '_').replace(':', '').replace('?', '')[:30]\n",
    "                filepath = os.path.join(dirpath, f\"{paper_id}_{title}.pdf\")\n",
    "                result.download_pdf(dirpath=dirpath, filename=f\"{paper_id}_{title}.pdf\")\n",
    "                logging.info(f\"-> Paper id {paper_id} with title '{result.title}' is downloaded.\")\n",
    "                break\n",
    "            except (FileNotFoundError, ConnectionResetError) as e:\n",
    "                logging.error(f\"Error occurred: {e}\")\n",
    "                time.sleep(5)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"An unexpected error occurred: {e}\")\n",
    "                break\n",
    "    papers = []\n",
    "    loader = DirectoryLoader(dirpath, glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
    "    try:\n",
    "        papers = loader.load()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading files: {e}\")\n",
    "\n",
    "    logging.info(f\"Total number of pages loaded: {len(papers)}\")\n",
    "\n",
    "    # Concatenate all pages' content into a single string\n",
    "    full_text = ''.join(paper.page_content for paper in papers)\n",
    "\n",
    "    # Remove empty lines and join lines into a single string\n",
    "    full_text = \" \".join(line for line in full_text.splitlines() if line)\n",
    "\n",
    "    return full_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-06 21:57:35,852 - INFO - Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=AI+in+Marketing&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=100\n",
      "2024-07-06 21:57:40,898 - INFO - Got first page: 100 of 2347097 total results\n",
      "2024-07-06 21:57:44,155 - INFO - -> Paper id 2303.03174v1 with title 'Both eyes open: Vigilant Incentives help Regulatory Markets improve AI Safety' is downloaded.\n",
      "2024-07-06 21:57:45,662 - INFO - -> Paper id 2308.02033v1 with title 'AI and the EU Digital Markets Act: Addressing the Risks of Bigness in Generative AI' is downloaded.\n",
      "2024-07-06 21:57:48,398 - INFO - Total number of pages loaded: 46\n"
     ]
    }
   ],
   "source": [
    "text = get_papers(\"AI in Marketing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BOTH EYES OPEN : VIGILANT INCENTIVES HELP REGULATORY MARKETS IMPROVE AI SAFETY A P REPRINT Paolo Bova1∗, Alessandro Di StefanoID1†, and The-Anh HanID1‡ 1Teesside University https://research.tees.ac.uk/ March 7, 2023 ABSTRACT In the context of rapid discoveries by leaders in AI, governments must consider how to design regulation that matches the increasing pace of new AI capabilities. Regulatory Markets for AI is a proposal designed with adaptability in mind. It involves governments setting outcome- based targets for AI companies to achieve, which they can show by purchasing services from a market of private regulators. We use an evolutionary game theory model to explore the role governments can play in building a Regulatory Market for AI systems that deters reck- less behaviour. We warn that it is alarmingly easy to stumble on in- centives which would prevent Regulatory Markets from achieving this goal. These “Bounty Incentives” only reward private regulators for catching unsafe behaviour. We argue that AI companies will likely learn to tailor their behaviour to how much effort regulators invest, discouraging regulators from innovating. Instead, we recommend that governments always reward regulators, except when they ﬁnd that those regulators failed to detect unsafe behaviour that they should have. These “Vigilant Incentives” could encourage private regulators to ﬁnd innovative ways to evaluate cutting-edge AI systems. ∗paolobova@protonmail.com †A.DiStefano@tees.ac.uk ‡T.Han@tees.ac.ukarXiv:2303.03174v1  [cs.AI]  6 Mar 2023Vigilant Incentives help Regulatory Markets A P REPRINT Highlights • We show that governments can incentivise a healthy Regulatory Market using what we call “Vigilant Incentives” — which always pay private regulators unless they fail to detect unsafe behaviour. On the other hand, “Bounty Incentives” — which pay only when they catch unsafe behaviour — destabilise Regulatory Markets. • “Vigilant Incentives” are effective because AI companies are sensitive to how likely private regulators are to detect unsafe behaviour. This allows a Regulatory Market to act as a deterrent to neglecting AI Safety. • We quantify how good regulators have to be at detecting unsafe AI systems to effectively deter reckless behaviour and highlight it as a crucial measure of the health of the Regulatory Market. • We also assess the importance of the size of the incentives. To balance risk reduction and overregulation concerns, incentives should not be too generous, except in situations where large externalities suggest that we prioritise risk reduction. • We visualise how Regulatory Markets are much better at balancing these tradeoffs under uncertainty than direct government regula- tion would be. However, Regulatory Markets also require a vig- ilant government regulator to assess the effects of the Regulatory Market. Regulatory Markets could hold promise in magnifying the impact of the government while minimising concerns of overregu- lation. 2Vigilant Incentives help Regulatory Markets A P REPRINT 1 Introduction A challenge facing us today is to ﬁnd ways to govern the long-term development of new AI capabilities safely. AI researchers recognise that it will be more difﬁcult to align the intentions and values of future goal-directed AI systems with those of the groups they serve (Amodei et al., 2016; Hernández-Orallo et al., 2019; Krakovna et al., 2020; Leike et al., 2017). Even if researchers can address these technical safety challenges, the deployment of powerful AI capabilities brings with it con- cerns of misuse, especially when we consider the dual use of many AI capabilities (Brundage et al., 2018; Shevlane & Dafoe, 2019; Zwetsloot & Dafoe, 2019). Governments around the world have begun to respond to these challenges. The NIST AI roadmap is an example of efforts by the United States to guide and pro- mote industry self-regulation (Barrett et al., 2022; Tabassi, 2021). The European Union’s AI Act, set to come into effect soon after years of reﬁnement, pursues a more binding regulatory framework that some argue may inﬂuence future efforts elsewhere (Siegmann & Anderljung, 2022). For now, it is not clear whether these efforts will meaningfully reduce the risks from future AI capabilities. Meanwhile, as feedback submitted on the above projects suggests, AI companies are paying close attention to the future of in- ternational regulatory environments. There is time for new regulatory initiatives to take effect before AI companies commit to a development strategy. The ﬁeld of AI governance has proposed many possible initiatives: The literature has iterated on several frameworks for auditing future AI systems: from Model Cards to System Cards, and audits that explicitly highlight the relevant effects of AI systems on their stakeholders (Brown et al., 2021; Gursoy & Kakadiaris, 2022; Mitchell et al., 2019). P. Cihon et al. (2021) explore AI certiﬁcation schemes to enforce technical and ethical safety standards. P. Cihon et al. (2020) have also explored the building of new technical standard-setting organisations. There are even discussions of novel voluntary agreements such as O’Keefe et al.’s (2020) Windfall Clause. In many cases, regulatory sandboxes have been suggested as a low commitment means to trial several of the new initiatives above. This only scratches the surface of the available menu of actions that governments and com- panies could consider (Brundage et al., 2020; P. J. Cihon et al., 2021; Naudé & Dimitri, 2020; Truby et al., 2022). Regulatory Markets present a relatively novel approach to regulation (Clark & Hadﬁeld, 2019). Governments set targets and mandate that companies employ the services of private regulators to demonstrate compliance with those targets. 3Vigilant Incentives help Regulatory Markets A P REPRINT These Regulatory Markets act as a complement and not a substitute for building government capacity to monitor the activities of AI companies and the capabilities of AI systems on the horizon (Whittlestone & Clark, 2021). Regulatory Markets have favourable qualities which seem appropriate for the un- certain and adaptable terrain of AI development. Private regulators must compete with each other to regulate AI companies. This competition may lead to inno- vations in methods to detect unsafe behaviour and better understand what safe development practises look like. Are these proposed beneﬁts likely? Will any start-ups join the proposed Regulatory Market? Our paper makes three contributions, the ﬁrst of which is to show under which conditions we can expect a Regulatory Market to be successful. We argue that well-chosen, appropriately funded regulators will participate in a Regulatory Mar- ket and can be incentivised to produce high-quality detection methods and stan- dards. These regulators are not just effective in catching unsafe behaviour. They also act as an effective deterrent to unsafe behaviour. Not all incentives will encourage high-quality regulators to join the Regulatory Market. Some incentives, for example those which encourage an adversarial re- lationship between AI companies and regulators, will actively harm the Regula- tory Market. These incentives unfortunately have an appealing efﬁciency at ﬁrst glance, since they focus on rewarding regulators for catching unsafe behaviour (we call such incentives “Bounty Incentives”). Incentives that instead appreciate the role of the Regulatory Market as a deterrent to unsafe behaviour fare much better (which we call “Vigilant Incentives”). We arrived at this ﬁrst conclusion by modelling the different incentives that would face both private regulators and the AI companies they regulate. This model builds on an existing model of the market for new AI capabilities, known as the DSAIR model (Han et al., 2020). We extend this model to capture the detection and en- forcement abilities of private regulators. Our work contributes to a growing num- ber of publications that model competitive dynamics in AI markets (Armstrong et al., 2016; Askell et al., 2019; Han et al., 2020; LaCroix & Mohseni, 2022; Naudé & Dimitri, 2020). To capture the complex dynamics that may emerge as regula- tors and companies explore the strategy space, we turn to analytical and numerical methods from Evolutionary Game Theory (Foster & Young, 1990; Fudenberg et al., 2006; Wallace & Young, 2015). Evolutionary Game Theory has been used to study other incentive mechanisms, both for issues in AI Governance, and in Cli- mate Change, another issue characterised by high uncertainty and multiple types 4Vigilant Incentives help Regulatory Markets A P REPRINT of actors (Encarnação et al., 2016; Han et al., 2020; LaCroix & Mohseni, 2022; Santos et al., 2016). As a second contribution, we discuss the trade-offs that one might consider when funding a Regulatory Market: as with other forms of regulation, deterring more unsafe behaviour often has the side effect that regulators are more likely to slow down companies in scenarios where the risks are low, an outcome we call “over- regulation” in line with prior work (Han et al., 2020; Han et al., 2021; Han et al., 2022). Here, we invoke the double-blind problem of the Collingridge Dilemma (Wor- thington, 1982). Governments will likely know little about the capabilities and risks of new AI capabilities until those technologies become entrenched. At that point, it will probably be very difﬁcult to inﬂuence who controls the market for AI. For this reason, measures such as a Regulatory Market must act under uncertainty. In particular, if the risks are low enough and the speed advantage from neglecting safety norms is high enough, then Regulatory Markets will lead to overregulation. As the Collingridge dilemma implies, these are two parameters of our model that are highly uncertain, and there exists much disagreement about where different approaches to AI sit and whether it makes sense to see AI Safety as separate from AI Capabilities in the ﬁrst place (Burden & Hernández-Orallo, 2020; Cave & Ó hÉigeartaigh, 2018; Dafoe, 2018; Vinuesa et al., 2020). We ﬁnd that we can reduce overregulation with little impact on risk through the careful design of government incentives and regulator activities. The nature of the externalities that AI systems pose can have a large inﬂuence on these designs. For our ﬁnal contribution, we compare Regulatory Markets to a government that directly regulates AI companies. Under uncertainty, we ﬁnd that Regulatory Mar- kets fare much better in balancing risk reduction and overregulation than the Gov- ernment does. We also note that Vigilant Incentives require that governments maintain a strong capacity for monitoring the market for AI. The rest of the paper proceeds as follows: Section 2 outlines our model of Regu- latory Markets, whereas Section 3 describes the evolutionary game theory method we adopt. Section 4 discusses the above three results in more detail along with de- tailed ﬁgures. Section 5 discusses how policymakers might use the model as a tool for thinking about how to evaluate a future Regulatory Market. We conclude with a brief discussion of model limitations and possible future research directions. 5Vigilant Incentives help Regulatory Markets A P REPRINT Figure 1 The Regulator’s Problem in the default scenario of interest. A regulator must choose whether to invest in high-quality evaluation tools and talent, HQ, or to accept a lower detection rate for unsafe practices, LQ. AI companies make their choice after observing the choice of the regulator. If the high-quality detection rate is high enough, then AI companies will switch from the unsafe equilibrium where they all play AUto one where they all play AS. Via backwards induction, the regulator could reason that they are choosing over the two equilibria and will act to secure whichever equilibrium outcome is best for them. 2 Model This section explains the details of the model, starting with the core set of actors that feature in the model, before outlining the decision problems that each actor faces. We ﬁrst describe the regulator’s problem and then describe the different incentives that we allow governments to award them. Finally, we discuss the AI companies’ problem, where we extend previous work from the literature. 2.1 The Regulatory Market Model Regulatory Markets involve 3 core sets of actors: • Governments who set targets for private regulators to meet and, therefore, have oversight on what regulators test for. Governments licence private reg- ulators to provide oversight of AI ﬁrms in their markets. • Private regulators compete with each other for AI companies to choose them to provide oversight. They may compete to meet government targets. Reg- ulators may have a wider array of powers to enforce their regulation than 6Vigilant Incentives help Regulatory Markets A P REPRINT existing private regulators tend to, including imposing ﬁnes, requiring au- dits, and revoking licences. • AI companies who must choose from available Private Regulators for their desired market(s). The requirements of Regulators are mandatory. We have only two populations in the baseline model, regulators and AI companies. For simplicity, we assume that one external government is responsible for setting the incentives facing private regulators. This government entity is also assumed to have sufﬁcient institutional power to enforce that AI companies work with at least one regulator should they wish to deploy their advanced AI systems. We might assume that there will be many fewer regulators than AI companies, al- though this will depend on a number of choices. Are we considering a wide range of possible AI companies, or only a select few who have dedicated their innova- tive efforts to creating General Purpose AI systems — the scope of AI companies matters? Barriers to entry may limit the number of AI companies in especially lucrative and risky domains (Askell et al., 2019; Bar (formerly Borkovsky) et al., 2009). On the other hand, the number of regulators may depend largely on the degree of success that a Regulatory Market proposal has in encouraging the creation of private regulators. Will developers at existing AI companies leave to create start- ups in the market for AI regulation? Will such start-ups be sustainable or avoid buyout from AI companies? Or will the market for private regulators mainly be carved up by existing institutions (Clark & Hadﬁeld, 2019; Hollenbeck, 2020)? 2.2 The strategic interaction between AI Companies AI companies enter into competition with each other and are matched to a relevant regulator from the Regulatory Market. Crucially, we assume that they ﬁrst observe the regulator’s choice of effort before choosing how safe to be. We assume that they can follow one of three strategies: •AS— companies always develop AI systems safely. •AU— companies never allocate effort to AI Safety. •VS— companies develop their AI systems safely, but only if they observe that regulators have invested in high-quality vetting systems. 7Vigilant Incentives help Regulatory Markets A P REPRINT Symbol Deﬁnition Range b Short term value of market 4 B Long term value of market >0 c Cost of safety measures for ﬁrms 1 W Length of time to develop transforma- tive AI safely>0 s Speed Advantage of skipping safety precautions>0 pr The risk of disaster if a ﬁrm is Unsafe. [0, 1] p 1−risk of disaster [0, 1] pl The chance of a low-quality regulator 0 revealing an unsafe ﬁrm ph The chance of a high-quality regulator revealing an unsafe ﬁrm1>ph>pl= 0 φ The regulator’s impact on the speed of unsafe ﬁrms they catch[0, 1] g Government budget allocated to regu- lators per ﬁrm regulated>0 rl Net proﬁt for regulator (low-quality) 0 rh Net proﬁt for regulator (high-quality) −1 β Learning rate 0.02 Zreg Size of Regulatory Market 50 Zai Size of AI market 50 Table 1: Parameter Table — Several of the parameters are ﬁxed because previous work on similar models has revealed that they have little inﬂuence on the results. Table 2 describes the average payoffs that AI companies receive when faced with another company playing a particular strategy, given the choice of the regulator. This model is heavily based on the DSAIR model from (Han et al., 2020). Com- panies who are always safe, AS, are at a disadvantage against companies who take 8Vigilant Incentives help Regulatory Markets A P REPRINT Strategy Always Safe (AS) Always Unsafe (AU) ASB 2W−c p h·1 φ+1B W−c AU p·(1−ph)·B W+phφ φ+1B Wp(1−p2 h)sB 2W+p2 hφ φ+1B 2W Table 2: AI Company Payoff Matrix — These payoffs capture the payoffs of dif- ferent strategies. Notice that the conditional strategy is not included since depend- ing on the Regulator’s choice it performs identically to one of the other strategies. We use the detection rate for high-quality regulators. Against a low-quality regu- lator, the payoff matrix is the same, except that we replace the detection rate with that for low-quality regulators. See the main text for explanations of each symbol. Note that the short-term beneﬁt of producing AI systems, b, has been omitted to ensure a more readable table. The omitted parameters do not inﬂuence our results. risks, AU, so it is usually the unsafe ﬁrm who is the ﬁrst to bring the new AI ca- pability to market, winning the big prize, B(note that payoffs are averaged over the length of the competition — which is Wif ﬁrms are safe, orW sif the winner is unsafe). If both companies choose the same strategy, they have an equal chance of winning the big prize. Our model builds on the DSAIR model by adding a detection rate that differs for high- and low-quality regulators, phis the detection rate for high-quality regu- lators, andplis the detection rate for low-quality regulators. We can see from Table 2 that increasing the detection rate (as occurs for high-quality regulators) reduces the payoffs to unsafe companies, which can only encourage them to be safer. Once an unsafe company is caught, the regulator (or perhaps the government) will aim to enforce that the company slows down their AI development to a speed which is a fraction φof the safe speed. This regulatory action has an uncertain impact on who is the ﬁrst to bring new AI capabilities to market. A chance remains that the previously unsafe company catches up to and overtakes the safe company, φ φ+s.4 If we restrict our attention now to this subgame played by AI companies, we can see several possible equilibrium outcomes depending on the parameters of the 4To present a simpler model of payoffs, we assume that if both companies are unsafe, and caught, that they are fully punished, φ= 0. This has no qualitative bearing on our results, but makes the equations here much easier to interpret. 9Vigilant Incentives help Regulatory Markets A P REPRINT model. If we ﬁx the choice of the regulator, we can ignore the conditional strat- egy, VS, for the time being. Figure 3a provides an illustration of the equilibria selected by social learning as we vary the risk, pr= 1−pand speed advantage, s, parameters of the model. The payoffs are symmetric, so there are only a few possibilities. If the risks are high enough, then ASis the pure strategy Nash equilibrium of the game. If the risks are low enough, AUis the only equilibrium. If the risks are somewhere in-between, then both may be equilibria. Social learning will result in players selecting the risk-dominant equilibrium in this case. For largeB Wand a detection rate,ph= 0,(Han et al., 2020) note that ASis risk dominant when p >1 3s. (Han et al., 2020) also note that society prefers companies to be safe (i.e. the sum of AI company utilities is greatest) whenever p >1 s. These equations give rise to a ’dilemma zone’, where society prefers unsafe ﬁrms to act safely (see Figure 3a).5 Now, let us allow for the choice of the regulator. As we shall discuss in more detail, the regulator can choose to be of low or high-quality: their quality deter- mines their detection rate. It is noteworthy that if the detection rate increases due to the regulator’s choice to be high-quality, that we may move from a region of the parameter space where AUis the only (or risk dominant) equilibrium for AI companies to a region where ASis the only (or risk dominant) equilibrium. There are in fact 3 relevant possibilities: ﬁrms always play AU, no matter what the regulator does, ﬁrms always play AS, and ﬁrms only play ASif facing a high- quality regulator (this is the conditional strategy, VS). In the ﬁrst scenario, high-quality regulation is not a strong deterrent. In the second scenario, a high-quality regulator is not needed. In the third scenario, the high- quality regulator acts as a strong deterrent to unsafe behaviour, which will be socially desirable if the risk of an AI disaster is high enough, i.e. if we are in the dilemma zone. 2.3 The Regulator’s Problem Regulators move ﬁrst, with their choices fully visible to AI companies before they make their own choices. Regulators must choose whether to aim to be: • high-quality ( HQ): A high-quality regulator accepts larger costs, so it has a better chance of evaluating cutting-edge AI systems. They are much more 5There are also rare choices of the values for different parameters where we can have an asymmetric equilibrium where one AI company is safe, and the other is unsafe. Our methods from Evolutionary Game Theory never select such equilibria, so we will not discuss them at greater length. 10Vigilant Incentives help Regulatory Markets A P REPRINT likely to detect unsafe behaviour on the part of AI companies and to know the appropriate procedures that AI companies should follow to ensure their work is safe. • low-quality ( LQ): They do not invest in evaluating cutting-edge AI systems, so they are unlikely to detect unsafe behaviour in more advanced systems. For the sake of simplicity, we shall assume that they have no chance of detecting unsafe behaviour. This assumption does not affect the qualitative features of our results: What matters is that the difference between the detection rates of both regulator types is sufﬁcient in the dilemma zone to move AI companies to develop AI safely. Since we assume that LQregulators essentially perform no detection services, our results also show which incentives are sufﬁcient to encourage partic- ipation in the Regulatory Market. Figure 1 illustrates how the choice to aim for high-quality may be pivotal in inﬂu- encing AI companies to develop AI more safely. Unfortunately, since high-quality regulators bear a large cost of investing in better tools and talent, this cost may out- weigh any revenue they can extract from AI companies in exchange for their ser- vices. Barring government intervention, regulators will choose to be low-quality, even if it is valuable from society’s point of view. Governments could offer a ﬂat incentive, g, to all regulators for each company they regulate. However, if they cannot tell which type a regulator is, then this incentive would have no effect on the choice that regulators make: it will still be more proﬁtable to be of low-quality. It is clear that to design more successful incentives, governments should take into account what little information may be available. 2.4 Bounty Incentives and Vigilant Incentives We consider two types of incentives. First, one could pay a bounty for any unsafe ﬁrms that regulators catch, expecting that high-quality ﬁrms will be better able to detect unsafe behaviour (“Bounty Incentives”). Second, a vigilant government could rescind a prior incentive should they discover wrongdoing on the part of the companies the regulator is responsible for (“Vigilant Incentives”).6. 6Note that this incentive design immediately implies the need for a complementary institution which engages in monitoring the behaviour of AI companies. As Clark and Hadﬁeld (2019) advise, Regulatory Markets are not intended as a perfect substitute for conventional monitoring institutions 11Vigilant Incentives help Regulatory Markets A P REPRINT Bounty Vigilant Strategy HQ LQ HQ LQ Firms play AS rh rl rh+g r l+g Firms play AUrh+gphrl+gplrh+gph2rl+gpl2 Firms play VS rhrl+gplrh+g r l+gpl2 Table 3: Regulator Payoffs under each Incentive — their payoffs also depend on the regulators’ efforts and on the choices made by AI companies. See the text for explanations of each strategy and the relevant parameters. Table 3 presents the regulator payoffs for each type of incentive, depending on what the AI companies they regulate choose to do. Higher quality regulators al- ways perform worse in the absence of any incentive, rh<rl(we always set rl= 0 for simplicity and usually set rh=−1). Bounty incentives are only on offer when AI companies play AU. Bounty incentives may also be achieved when a condi- tionally safe AI company, VS, faces a low-quality regulator, but as we set their detection rates to 0, this does not occur in the scenarios of interest. We can see that when companies play VS, that HQregulators do as poorly as they can, while LQregulators do as well as they can. The logic at play here is that Bounty Incentives would signal to private regulators that the government is willing to pay for only the tools which are effective in their job. From one point of view, there appears to be a rather appealing efﬁciency at play. If we only pay those regulators who actually detect unsafe behaviour, then we encourage competition to be the regulator who offers the best tools. These Bounty Incentives certainly have their appeal in the short run in a market with rampant unsafe behaviour. Regulators will be enticed by the lucrative oppor- tunity of catching an unsafe AI company in the act. Regulators therefore have a strong incentive to develop powerful tools for detecting a speciﬁc behaviour and may even deter the unwanted behaviour. Once the behaviour has been deterred, regulators will no longer see any proﬁt in further improving their methods, and governments will no longer have to cover the cost of those investments. 12Vigilant Incentives help Regulatory Markets A P REPRINT We now turn to Vigilant Incentives. Under Vigilant incentives, all regulators re- ceive a payment g. However, when AI companies play AUor when a conditionally safe company, VS, faces a low-quality regulator, those payments will be rescinded if they fail to catch an unsafe company. We can see that when companies play VS, thatHQregulators do as well as they can, whilst LQregulators do as poorly as they can. The line of argument here is that the government chooses to treat incentives as investments in a deterrent to unsafe behaviour. A deterrent must be funded, re- gardless of whether unsafe behaviour is currently occurring. Regulators are happy to receive the incentive but know that if they let unsafe com- panies slip away undetected, the government can claim back the incentive — re- member that in our model, we have assumed this is the only way for the govern- ment to discriminate by regulator quality. If ﬁrms are unsafe, even high-quality regulators risk losing their incentive. 3 Methods We use methods from evolutionary game theory to explore what type of behaviour the different actors within our regulatory market will learn to follow (Foster & Young, 1990; Fudenberg et al., 2006; Wallace & Young, 2015). Evolutionary Game Theory has been used to study pressing issues in AI Gover- nance, and in Climate Change (Encarnação et al., 2016; Han et al., 2020; LaCroix & Mohseni, 2022; Santos et al., 2016). The ﬁeld has also devoted much attention to the study of the efﬁciency of different incentives for resolving social dilemmas (Cimpeanu et al., 2023; Han, 2022; Sasaki et al., 2012; Sigmund et al., 2010; Sun et al., 2021). These methods have been used in the past to study games with multi- ple populations, as we do here (Encarnação et al., 2016; Rand et al., 2013; Santos et al., 2016; Zisis et al., 2015). To further motivate the use of Evolutionary Game Theory, consider that the reg- ulators and AI companies in our model will likely engage in a period of learning about the type of behaviour they wish to emulate. Although there may not be many AI companies with large enough capital to perform at the cutting edge, there are a wide number of applications of AI systems that these companies may wish to be active in. We anticipate that companies are uncertain about the net value of any particular new technology, and that regulators face uncertainty over how difﬁcult it is to evaluate new technologies. In the face of this uncertainty, we expect both groups to explore the strategy space and to imitate high-performers. It seems rea- 13Vigilant Incentives help Regulatory Markets A P REPRINT sonable to approximate this setting as a Moran process: we have a ﬁnite number of players who may over time randomly explore different strategies or instead imitate their more successful peers.7 Players are more likely to imitate the strategies of players who are comparatively more successful than they are, which we capture mathematically as the difference in expected payoffs, Π(k). Note that the expected payoffs of playing a strategy is a function of the number of players choosing other strategies. The more people play a strategy different from you, the less likely you are to interact with someone using your strategy. Your payoffs may also depend on what people are doing in other populations. In our case, the actions of regulators will inﬂuence the expected payoffs of AI companies and vice versa. We can deﬁne the success, or ﬁtness f, of one strategy AagainstBas follows: fA,B(k) = ΠA(k)−ΠB(k) (1) To keep the analysis as straightforward as possible, we also assume that the mu- tation rate is inﬁnitesimally small. In the method of Fudenberg and Imhof (2006) this assumption is made so that in the long run the evolutionary system spends all its time in one of its absorbing states. Analyses which make this assumption often ﬁnd results applicable well beyond the strict limit of very small mutation (or exploration) rates (Hauert et al., 2007; Rand et al., 2013; Sigmund et al., 2010). Recall that this is a model with multiple populations, so the absorbing states are any states where all regulators follow the same behaviour and all AI companies follow the same behaviour: HQ-AS ,HQ-AU ,HQ-VS ,LQ-AS ,LQ-AU ,LQ-VS . These states are visible in Figure 2. When one of these rare mutations does occur, we can calculate the likelihood that the single mutant will invade the relevant population, i.e. the ﬁxation probability, using the following formula ρA,B=1 1 +∑N−1 j=1eβ∑j k=1fA,B(k). (2) 7There is a discussion to be had about whether an evolutionary model is more or less appropriate for studying the market for future AI systems than, say, classical game theory. This may be especially relevant for the increasing competition to build large language models, which is typically led by companies willing to spend large amounts on the talent, computational infrastructure, and data collection needed to develop cutting-edge systems. Large companies may be more forward-looking and rational than smaller companies, and therefore may be less likely to learn through imitation of their peers. Nevertheless, we maintain that Evolutionary Game Theory is a useful ﬁrst approximation to these scenarios, especially given the close ties between these methods, the analysis of complex agent-based models, and reinforcement learning. 14Vigilant Incentives help Regulatory Markets A P REPRINT In the equation above, βrefers to the imitation rate. A larger βmeans players are more inclined to imitate a more successful player’s strategy. Different values of β may be appropriate in different contexts. For our ﬁgures, we choose a value for β which implies that players are at least 90% likely to adopt a strategy which gives payoffs one standard deviation greater than their current strategy, which seems reasonable given the high stakes involved, especially for companies. A sensitivity analysis of the ﬁgures we present in this paper suggests that our results are ro- bust to different choices of β(we chose values of βwhich instead implied a 75% and 95% adoption likelihood). The value of βcan also be informed through be- havioural experiments with human participants (Hoffman et al., 2015; Rand et al., 2013; Zisis et al., 2015). We can also see from Figure 2 that if we want to know how much time we spend in each state on average, we should care about the transitions between each of these states. Assuming that all mutations are just as likely to occur, it is straightforward to derive a transition matrix to tell us the relative likelihood with which the system is likely to move from one state to another. Due to the rare mutation limit, only one population, AI companies or regulators, will experience a mutation during a given evolutionary epoch. Therefore, we only need to consider transitions between states where one of the populations remains unchanged. We can write the elements of the transition matrix as follows, where Sis the number of states:8 Pij=\\uf8f1 \\uf8f4\\uf8f2 \\uf8f4\\uf8f3ρij S−1ifi̸=jand both states differ for one population only , 1−∑S k=1,k̸=iρik S−1ifi=j, 0if states i and j differ for more than one population.(3) By construction, this transition matrix is irreducible. Therefore, this transition ma- trix has a unique stationary distribution (see Häggström et al. (2002) for a proof). This unique stationary distribution, V, satisﬁes (I−V)P=# »0. (4) We can ﬁnd this unique stationary distribution by noting that V, is the nor- malised left eigenvector with eigenvalue 1of the transition matrix, P. We use 8Note that the ﬁxation rate used for each element is the one relevant to the population undergoing the transition. If regulators changed strategy, then the ﬁxation rate considers the success of the regulator’s new strategy against the old one, rather than the success of an AI company’s strategy which is unaffected by the transition. 15Vigilant Incentives help Regulatory Markets A P REPRINT Figure 2 Bounty Incentives allow unsafe AI companies to exploit the presence of low-quality regulators. This Markov Chain diagram shows the transitions between states and their long-term frequencies. States are coloured blue if AI companies act safely and orange if AI companies act unsafely. The parameters chosen place us in the dilemma zone, ph= 0.6,g= 1.2,φ= 0.5,pr= 0.6,s= 1.5,B/W = 100 , β= 0.02. the Grassmann-Taksar-Heyman (GTH) algorithm to compute these eigenvectors in our numerical calculation (Stewart, 2009). The stationary distribution can be interpreted as the percentage of time that the system spends in (or around) each of these states. The results to follow interpret the stationary distribution as telling us the relative frequencies with which each type of interaction occurs between a regulator and AI companies. 4 Results We now turn to our analytical and numerical results. In the ﬁrst two sections, we explain our key takeaways concerning Bounty and Vigilant incentives. After a brief discussion of why differences arise between our analytical and numerical re- sults, we then consider the optimal design of a Regulatory Market when balancing the competing concerns of risk reduction and overregulation. We then discuss how beliefs about AI risks and the size of externalities might inﬂuence the optimal de- sign. Finally, we compare a Regulatory Market with Vigilant Incentives to direct government regulation. 16Vigilant Incentives help Regulatory Markets A P REPRINT (a)AU Frequency  (b)LQ Frequency Figure 3 Bounty Incentives have negligible impact on the behaviour of regulators and companies. ( Panel a ) The parameter space (here we show the speed advan- tage,s, and level of AI risk, pr) can be split into regions where AI companies are Always Safe or Always Unsafe. AI companies choose their behaviour as they would have in the absence of any Regulatory Market. The solid lines indicate the risk dominance (top line) and socially efﬁcient thresholds (bottom line) for the always safe strategy in the absence of a Regulatory Market. The area between them is the “dilemma zone”. ( Panel b ) No regulator invests in high-quality tools. We would therefore not see any change in welfare relative to a scenario where the government incentive and high-quality detection rate are both 0. The model parameters take on values: ph= 0.6,g= 1.2,φ= 0.5,B/W = 100 ,β= 0.02. 4.1 Bounty Incentives fail to sustain a Regulatory Market When we ﬁrst introduced Bounty Incentives, we told a plausible story of why they would be appealing to introduce. Our ﬁrst result shows that this intuition was misguided. Figure 3a suggests that catching unsafe AI companies in the act is only a dream. AI companies know to play it safe when a high-quality regulator is active, so regulators cannot beneﬁt by improving their detection rate. Figure 3b conﬁrms that in the long run, regulators learn that they are better off skipping the investment, and AI companies remain unsafe in the dilemma zone. Let us consider a brief analysis of the model. The subgame-perfect Nash equilib- rium of the game can be solved by backwards induction. Assume that we have model parameters such that Figure 1 describes the relevant equilibria for AI com- panies when faced with different regulators. Therefore, each company’s strategy 17Vigilant Incentives help Regulatory Markets A P REPRINT is to play AUwhen facing a LQregulator and to play ASwhen facing a HQ regulator. This is precisely the conditional strategy, VS. The regulator will choose whichever option leads to an equilibrium with greater payoff. Let I(.)denote a function that maps the detection rates phorplto the size of the incentive they expect to receive. Thus, the regulators will choose HQ ifrh+I(ph|companies play AS)> rl+I(pl|companies play AU). We assume regulator proﬁts are lower for high-quality regulators, rh< rl, so we need to choose incentives which are increasing in either the detection rate or in the number of safe companies. The Bounty incentive achieves neither of these features, rh< rl+g∗pl, and if the low-quality detection rate is positive, discourages regulator investment. In the SPNE, regulators will be of low-quality. The Markov chain diagram, Figure 2, conveys the dynamics at play in the dilemma zone of Figure 3a. Regulators who try to be HQwill ﬁnd that AI companies will either move to the conditional strategy, VSor play AS. In either case, the regulator has no unsafe ﬁrms to catch, so it switches to LQ. If AI companies were playing AS, they would return to an unsafe strategy once the regulator gave up. Once in these states, LQ-AU orLQ-VS , regulators and AI companies are extremely unlikely to change what they do. We can explain these dynamics in relation to our story from earlier. As men- tioned above, regulators eventually become complacent once they have deterred companies from acting unsafely. However, progress in AI often leads to new ca- pabilities, capabilities which may require regulators to consistently improve and innovate on their approach to detecting unsafe practices. It is easy to imagine that new risks will go relatively unnoticed by regulators, similar to the situation credit rating agencies found themselves in prior to the 2007-8 ﬁnancial crisis (Clark & Hadﬁeld, 2019). Incumbent regulators may eventually consider improving their capabilities, but as AI companies learn to behave conditionally on observing this effort. They will quickly lose interest in further investment, as they cannot ﬁnd unsafe companies to collect a bounty for. These incumbent regulators will only give us a false impression of safety. 4.2 Vigilant Incentives are sufﬁcient to sustain a Regulatory Market that deters unsafe behaviour We now consider our “Vigilant incentive”, which satisﬁes the requirements of our preferred SPNE. HQis the SPNE whenever rh+g>rl+g·p2 l. This means that we 18Vigilant Incentives help Regulatory Markets A P REPRINT Figure 4 Vigilant Incentives encourage regulators to be high-quality innovators. This Markov Chain diagram shows the transitions between states and their long- term frequencies. States are coloured blue if AI companies act safely and orange if AI companies act unsafely. The parameters chosen place us in the dilemma zone, ph= 0.6,g= 1.2,φ= 0.5,pr= 0.6,s= 1.5,B/W = 100 ,β= 0.02. needg>rl−rh 1−p2 l. If the detection rate of low-quality ﬁrms is 0, then the government incentive only needs to be large enough to cover the proﬁt gap between low and high-quality regulators.9 Figure 5a conﬁrms that such an incentive is sufﬁcient to see the emergence of high-quality regulators as a result of social learning. We choose gto be slightly larger than the SPNE needed to encourage faster learning among regulators. Consider Figure 3 and Figure 5 for the Bounty and Vigilant incentives, respec- tively. Previously, the SPNE for Bounty incentives precisely matched the results of the evolutionary model, but the same cannot be said for Vigilant incentives: Though many regulators choose to invest in being high-quality in the dilemma zone, not all do. Moreover, even when regulators choose low-quality in the SPNE (bottom right of Figure 5b) a very small proportion of regulators remain high- quality in the evolutionary model (with so few high-quality regulators, the differ- ent shades in Figure 5b may be difﬁcult to see on ﬁrst glance). These results can be explained with reference to the evolutionary dynamics. 9Notice that if we know rl−rh, we can choose gsuch thatplhas to be sufﬁciently low for the Regulatory Market to be worthwhile. This means that our choice of gimplicitly determines which values of plare too high to justify a Regulatory Market. 19Vigilant Incentives help Regulatory Markets A P REPRINT (a)AU Frequency  (b)LQ Frequency (c)∆Welfare Figure 5 Vigilant Incentives reduce AI risk by deterring unsafe behaviour. ( a) The parameter space (here we show the speed advantage, s, and level of AI risk, pr) can be split into regions where AI companies are Always Safe or Always Unsafe. AI companies choose their behaviour as they would have in the absence of any Regulatory Market. The solid lines indicate the risk dominance (top line) and so- cially efﬁcient thresholds (bottom line) for the always safe strategy in the absence of a Regulatory Market. The area between them is the “dilemma zone”.( b) Regu- lators often choose to be high-quality in the dilemma zone”. A small percentage of regulators remain high-quality outside of it.( c) The Regulatory Market improves welfare in the dilemma zone, but slightly reduces welfare through overregulation outside of it. The model parameters take on values: ph= 0.6,g= 1.2,φ= 0.5, B W= 100 ,β= 0.02. 20Vigilant Incentives help Regulatory Markets A P REPRINT When we examine the Markov chain diagram, see Figure 4, we can see that it is now much more common to be in state HQ-VS . Most importantly, regulators in theLQ-VS state now have a reasonably strong incentive to switch to being HQ. Unlike in the previous Markov chain, there is still a somewhat probable path where companies can drift to playing AS. Regulators may become complacent, leading to lower investments after returning to the LQstrategy, which in turn can allow the revival of unsafe behaviour. This is one reason why not all regulators end up being high-quality in the dilemma zone. Another notable dynamic is the one-way transition from HQ-AU toLQ-AU . The government incentive in our case is not high enough here to encourage regulators to be high-quality when AI companies are unsafe, helping to explain the relatively slow transition to safer states. However, if gwere much higher, this transition would ﬂip, leading to a much larger fraction of regulators choosing to be high- quality even in scenarios where risks are low. Therefore, choosing a gthat allows for this dynamic is helpful in avoiding most overregulation but comes at the cost of accepting some proportion of unsafe ﬁrms. We discuss this trade-off more deeply in the next section. The key takeaway here is that the design of our incentive matters. Bounty incen- tives fail to guard regulators against AI companies who play conditional strategies, so are unhelpful for supporting a Regulatory Market. On the other hand, Vigilant incentives reward regulators for the service they offer, discriminating against reg- ulators who let unsafe AI companies enter the market undetected. This design ensures that when monitoring investments can deter unsafe behaviour, regulators are motivated to act on it. 4.3 The Optimal Design of a Regulatory Market with Vigilant Incentives Now that we have opted to use Vigilant Incentives to build our Regulatory Mar- ket, it is time to consider how we can optimally design this proposal to reduce risk whilst avoiding overregulation. The key lesson is that we can maintain high reductions in risk and reduce overregulation with careful tweaks to the following parameters: the detection rate that high-quality regulators achieve, ph, their impact on unsafe ﬁrms they catch, φ, and the government incentive, g. First, the level of detection rate, ph, has a direct inﬂuence on the extent of over- regulation. A well-chosen detection rate can ensure that the deterrent only has an effect when society prefers companies to be safe. 21Vigilant Incentives help Regulatory Markets A P REPRINT (a)  (b) Figure 6 The details of a Regulatory Market inﬂuence welfare: ( a) Risk domi- nance thresholds for different values of the detection rate for high-quality regula- tors,g= 1.2(other parameter values speciﬁed below). ( b) Expected ∆Welfare under Vigilant Incentives for different levels of incentives, g, and detection rates, ph. Only positive values are shown. Expected Welfare is computed uniformly over the space of s∈[1,5]andpr∈[0,1]. The model parameters take on values: φ= 0.5,B/W = 100 ,β= 0.02. Figure 6a summarises this ﬁnding. Higher values of phshift the threshold where social learning selects safe behaviour closer to the threshold where society prefers safety (the solid lines in the ﬁgure are the original and desired thresholds, as in previous ﬁgures; the dashed lines are the result of different detection rates). How- ever, ifphis too high, then social learning will select safe behaviour even when society prefers companies to take risks. As Figure 6a shows, for a suitable choice ofg, the optimal detection rate aligns the two thresholds. In this case, a detection rate of around 0.6seems to best align the behaviour of AI companies with the values of society. We will later return to a discussion of when it could be feasible for policymakers to inﬂuence the detection rate. Second, let us discuss the strength with which regulators penalise unsafe be- haviour. The ﬁgures we display show results for φ= 0.5, indicating that regulators ensure that companies are expected to be half as slow as companies that act safely. We have also considered φ= 1, where companies are only slowed to match the speed of safe AI companies, and φ= 0 where companies are completely barred from participating in AI development. The choice of φwhich is most appropriate depends on the detection rate. A high detection rate likely only needs lenient pun- 22Vigilant Incentives help Regulatory Markets A P REPRINT ishments to be a good deterrent to unsafe behaviour. A low detection rate requires a stronger punishment. However, there are additional considerations which lean towards our choice of φ= 0.5. Lower values of φallow a wider range of detection rates that lead to a net positive welfare effect from Regulatory Markets. These detection rates are also lower, and we should anticipate that regulators are likely to achieve lower detection rates. On the other hand, for all practical purposes, a low φsuch as φ= 0 is implausible. History suggests that it is very difﬁcult to bar companies from markets in which they have a strong foothold. Microsoft would be a notable example of a company who has faced legal repercussions for anti-competitive be- haviour, yet to this day operates in the same markets (Economides, 2001). The market for AI has companies with similar levels of power and legal capability. A good compromise is likely to slow down unsafe companies by enforcing that they follow past and present safety guidance. Additional security checks and require- ments could disadvantage such a company, but due to the uncertain nature of AI development, still leave them with a signiﬁcant chance of catching up with safer companies. This is not to say that we should rule out the strongest punishments in all cases: If companies try something truly reckless, it may be desirable to set a new precedent and shut down their activities. Third, Figure 6b suggests that the government incentive, g, should be large enough for high-quality regulators to perform better than others when faced with AI com- panies that play their conditional strategy. Otherwise, high-quality regulators would switch to being low-quality over time. In Figure 6b, g > 1is required for this purpose: note the discontinuous jump in expected welfare past g= 1. Additionally, regulators need to participate in the market, so they should do better than their outside option (which in our model we have assumed for simplicity to be0, which is the same as the net proﬁts for low-quality regulators). Increasinggfurther does encourage more regulators to invest in being high- quality, which means a stronger deterrent to unsafe behaviour. However, as we can see in Figure 6b, a gthat is too high can lead to a reduction in expected wel- fare. This reduction in expected welfare comes from overregulation, some of which is visible in the lower right corner of Figure 5c. Higher values of gensure that regula- tors continue to invest in high-quality detection methods, even when the deterrent fails to deter unsafe behaviour. If the detection rate already aligns the behaviour of AI companies with society’s preferences, then these high-quality regulators will punish companies that society would prefer not to. Overregulation can be reduced 23Vigilant Incentives help Regulatory Markets A P REPRINT (a)No externality  (b)Externality =1 5s·B ·W Figure 7 ∆Welfare under Vigilant Incentives when we consider ( a) adding col- lective risks that affect all ﬁrms, and ( b) adding large externalities that AI com- panies do not expect to bear. Both ways of capturing the systemic nature of the risks presented by upcoming AI capabilities suggest that well-designed Regula- tory Markets can greatly improve expected welfare. The model parameters take on values:ph= 0.6,g= 1.2,φ= 0.5,B/W = 100 ,β= 0.02. by keeping glow at the cost of having a higher level of unsafe behaviour in the dilemma zone. 4.4 The need for Regulatory Markets depends on beliefs about AI risk Our discussion has so far considered how the design of the Regulatory Market inﬂuences its impact on social welfare. Until now, we have not explicitly discussed how different beliefs about AI risk should affect our evaluation of a Regulatory Market. We ﬁrst ask the reader to consider their beliefs about the level of risk presented by different AI capabilities, as well as the speed advantage of skipping associated safety norms. If the level of risk is high, and the speed advantage is relatively low, then we are more likely to be in the dilemma zone where society prefers unsafe AI companies to be safe. If the level of risk is low and the speed advantage is high, then it is likely that society prefers AI companies to take risks and accelerate innovation.10 10Another popular view is that the level of risk is increasing in the speed advantage: getting AI capabilities earlier leads to higher risk. We ﬁnd that a belief that risk and speed are positively correlated places much more probability mass in the dilemma zone, giving greater justiﬁcation for a Regulatory Market. However, this view has received 24Vigilant Incentives help Regulatory Markets A P REPRINT Let us consider a purely illustrative example that focuses on Large Language Mod- els, such as GPT3 and its variants. There is an argument that the risks of an AI disaster are high if these models are widely adopted and used to generate misin- formation, although perhaps not above 50%. At the same time, it might be hard to imagine seeing the pace of development we have seen so far if AI companies could not deploy LLMs until they no longer conﬁdently generate fake information in response to a query. For illustration purposes, we might expect the mean level of risk to be normally distributed around 50% and the speed advantage to be nor- mally distributed around a factor of 4. As this example places most probability mass outside the dilemma zone, we should expect Regulatory Markets to mainly bring overregulation. So far, the risks from an AI disaster have so far been assumed to be isolated to the company who enjoys the beneﬁts of achieving breakthroughs in AI capabili- ties. This simpliﬁcation of the model appears to be inaccurate on two accounts. First, the risks of an AI disaster may be collective in the sense that a disaster af- fects all companies in the AI market — An AI disaster may cause a government backlash and an AI winter. Or if the disaster is catastrophic, the assets or the peo- ple who make up each company may be in peril (Cave & Ó hÉigeartaigh, 2018; Dafoe, 2018). Second, the systemic and possibly catastrophic nature of AI risks means that externalities are very plausible. It is unlikely that AI companies will internalise the harms that misinformation or tail risks such as disruption of critical infrastructure will cause to citizens. Figure 7 demonstrates how welfare results may change if we explicitly model the presence of externalities and the collective nature of AI disasters. If AI companies recognise the collective risk of an AI disaster, then Regulatory Markets can more directly inﬂuence behaviour. The dilemma zone is larger under collective risk because AI disasters are more likely. However, Regulatory Markets also ﬁnd it easier to deter unsafe behaviour. Regulatory Markets are therefore much more likely to have a net positive welfare effect. If the externalities are large enough, for example 20% the size of the beneﬁts to society of having new AI capabilities as soon as possible, then Regulatory Markets are signiﬁcantly better at improving welfare. Note that, as the Regulatory Market does not directly target the externality, the effect that the Regulatory Market has on behaviour does not change. criticism: An alternative view is that safety efforts are better targeted when AI capabilities are closer, suggesting that the level of risk may remain relatively unchanged with the speed advantage. 25Vigilant Incentives help Regulatory Markets A P REPRINT For the most part, the results of the model are similar in pattern to those we have discussed so far. There is a wider range of scenarios where Regulatory Markets re- duce risk without causing overregulation. The suggestions regarding g,ph, andφ remain the same. However, once the externalities are large enough, overregulation ceases to be an important concern. It becomes justiﬁable to spend larger incen- tives. Higher detection rates can also be employed as the costs of overregulation become relatively small. To summarise, Regulatory Markets become more viable when one believes the AI domain of interest has a greater risk of causing harm to society at large. If these risks are small relative to the societal beneﬁts of having AI capabilities sooner, then a Regulatory Market would not appear to be necessary. 4.5 Regulatory Markets deal with uncertainty better than a Government regulator alone How does a Regulatory Market compare to the government enforcing regulation directly? We ﬁnd results which are clearly favourable for Regulatory Markets. Though we have discussed at length the challenge of overregulation, Regulatory Markets are far better at avoiding overregulation than a government regulator. Assume the government achieves the same optimal detection rate as we assumed our Regulatory Market does. Figure 8a demonstrates that the government does better in reducing risk. In fact, since the government does not cycle between low-quality and high-quality regulators, all companies are deterred from unsafe behaviour in the dilemma zone. However, this rigidity in the detection rate of a government regulator is also the source of overregulation, see Figure 8b. Even when society prefers AI companies to take risks, the government will discover and punish these companies, slowing down innovation. Throughout this paper, we have argued that it is difﬁcult for the government to know whether a market for AI is in the dilemma zone. Given this uncertainty, government regulation risks being excessive. The difference between overregulation in Figure 8c for the government and Fig- ure 5c for a Regulatory Market is substantial. Regulatory Markets do better in this case because they can fail. Regulators do not invest in better detection methods outside the dilemma zone because if they cannot deter AI companies from acting unsafely (or catch them all), the government will not pay them. Relative to direct government intervention, this failsafe means that Regulatory Markets offer a much better deal to policymakers given the uncertainty of AI development. 26Vigilant Incentives help Regulatory Markets A P REPRINT (a)AU Frequency  (b)LQ Frequency (c)∆Welfare Figure 8 Direct government regulation — Instead of a Regulatory Market, we could allocate government spending to an institutional regulator. ( a) Assuming they would achieve the same detection rate, they would be more effective in dis- couraging unsafe behaviour. The dilemma zone is completely eliminated. ( b) However, the government always aims for high-quality. There does not exist a fallback mechanism to discourage regulation where it is not needed.( c) So, we see a massive loss in welfare due to overregulation outside the dilemma zone. Other parameter values are: g= 0,ph= 0.6,B/W = 100 ,φ= 0.5,β= 0.02. 27Vigilant Incentives help Regulatory Markets A P REPRINT We do not mean to suggest that Regulatory Markets are a replacement for gov- ernment regulation. Recall that Regulatory Markets aim to meet targets set by the government in the ﬁrst place. Moreover, the Vigilant Incentives we propose re- quire that governments are knowledgeable about cutting-edge AI deployments and can independently monitor AI companies to reveal whether regulators are living up to their targets. Government monitoring is likely essential to a thriving Reg- ulatory Market which avoids capture from the AI companies they must regulate. Ultimately, Regulatory Markets and government regulation serve as complements rather than substitutes. 5 Discussion In this paper, we have presented tentative evidence that a well-designed Regulatory Market can play a role in reducing risks from even transformative AI systems (Gruetzemacher & Whittlestone, 2022). Readers may also be curious about how practical considerations might inform our warnings against Bounty Incentives and our recommendations for Vigilant Incentives. We ﬁrst give additional reasons why Bounty Incentives are likely to result in the collapse of a Regulatory Market. We also suggest some obstacles to a Regulatory Market under Vigilant Incentives. These include collusion, difﬁculties in mea- suring the detection rate, and regulatory capture. We end with a brief discussion of how Regulatory Markets might be used internationally, as ﬁrst suggested by the original authors of the proposal (Clark & Hadﬁeld, 2019). The explicit mod- elling of these challenges and the design of tests for their presence would serve as excellent starting points for future work. 5.1 Practical considerations for Bounty Incentives We have shown that Bounty Incentives tend to fail to promote investment in higher quality regulation over time. In spite of this result, we anticipate that some readers will still believe that these incentives are worth attempting, given that the govern- ment does not have to pay anything unless unsafe behaviour is caught. We ask our readers to contemplate the following additional reasons to avoid Bounty incentives. It could be much easier for private regulators to fake or ex- aggerate unsafe claims, and they might even have incentive to do so when AI companies aim to be as safe as possible. Such corruption would destroy the rep- utability of the Regulatory Market and would only encourage unsafe behaviour. 28Vigilant Incentives help Regulatory Markets A P REPRINT Another reason is that we want to avoid pitting private regulators against AI Com- panies as adversaries. Advocates for AI Safety are often located within AI Com- panies. Fostering animosity between industry and regulators only increases the difﬁculty of achieving consensus on the risks of future AI capabilities. We may also see other forms of antisocial punishment, such as industry or industry-aligned academics denouncing regulators (Herrmann et al., 2008). This is not to say that collusion between AI Companies and Regulators is desirable. A lack of regula- tory independence could also result in ineffective regulation and may even act as a smokescreen against unacceptable behaviour (Clark & Hadﬁeld, 2019). Note that there may be schemes which act implicitly as Bounty Incentives. For example, a reputation system which gave high ratings of trust to regulators who detect and report the unsafe behaviour of companies could count as providing Bounty Incentives, especially if these ratings were key to the private regulators securing future lucrative work. We argue that reputation systems should aim to avoid implicit Bounty Incentives. Instead, reputation systems should focus on di- rectly promoting truthfulness, as well as consider other insights from the literature more speciﬁc to reputation systems (Barton, 2005; Brundage et al., 2020; P. Cihon et al., 2021). 5.2 Practical considerations for Vigilant Incentives 5.2.1 Ensuring the participation of private regulators Vigilant incentives may also be unappealing to regulators. It would be odd for pri- vate regulators to have a business model where failing to detect unsafe behaviour might risk the entire business (this does not have to be the case, but for simplic- ity we often model scenarios where high-quality regulators might not break even without government support). Stronger deterrents may lessen the risk, but care must be taken to ensure that this proposal has the ability to attract private regula- tors to participate in the Regulatory Market. In our model, we simpliﬁed away the issue of participation by assuming that low- quality regulators are usually indifferent between participating in the Regulatory Market and their outside option. However, this simpliﬁcation is unlikely to hold. Recent work uses Evolutionary Game Theory to show that incentivising participa- tion is just as important as incentivising compliance for overcoming conventional social dilemmas (Han, 2022). We should expect a similar result to hold for Regu- latory Markets. 29Vigilant Incentives help Regulatory Markets A P REPRINT Clark and Hadﬁeld (2019) argue that many of the beneﬁts of a Regulatory Market could come from its independence from industry and the competitive pressure to ﬁnd innovative ways to more cheaply evaluate cutting-edge AI systems. Both ben- eﬁts seem less likely if there are high barriers to entry or if larger regulators have a motive to buy out smaller regulators. Governments can play a role in keeping the Regulatory Market competitive by incentivising new entrants, and we welcome further research on other ways governments can promote healthy competition in Regulatory Markets.11 5.2.2 Reducing the cost to governments At ﬁrst glance, Vigilant Incentives may be unappealing to governments. These incentives ask governments to at least pay each private regulator enough so that the highest-quality regulators are better off than their lower-quality analogues. As discussed, we may also need to incentivise their participation. The literature on public goods reveals several funding mechanisms that the gov- ernment can use to raise these funds from different stakeholders (Buchholz & Sandler, 2021; Sasaki et al., 2012; Tabarrok, 1998). We leave a comparison of these mechanisms in the context of a Regulatory Market to future work. Ultimately, some groups will have to bear the cost of providing these incentives, whether they be taxpayers, AI companies, or users of AI systems. It is natural to ask if there is anything the government could do to reduce the need for these incentives in the ﬁrst place. We turn our reader’s attention to an unexplored part of our setting. We assumed that in the absence of a Regulatory Market, high-quality private regulators would make a loss relative to their lower quality peers. This assumption was motivated by the larger talent and capital costs we might expect to come from investments in better detection methods for cutting-edge AI systems. We also anticipate that if we allow markets to set the price AI companies pay to private regulators, that 11There is some measure of debate surrounding whether more concentrated markets allow for less or greater inno- vation, sometimes discussed as “dynamic efﬁciency” (Berger & Hannan, 1998; Demsetz, 1973). Companies with a high market share tend to beneﬁt from a more inelastic demand for their products. This market power reduces the need to innovate to survive. Moreover, it appears commonplace for these companies to buy out new innovative entrants. On the other hand, companies may need the large economies of scale that a higher market share provides if they are to ﬁnance more R&D. In addition, new start-ups might even be motivated to innovate in the hopes of a lucrative buy-out. Recent literature reveals that context matters when determining whether a pre-emptive buy-out motive overpowers high barriers to entry (Hollenbeck, 2020). While a stronger argument could be made in favour of market concentration for AI companies themselves, we suspect that barriers to entry and a reduced need to innovate will matter much more for private regulators, as case studies appear to suggest (Clark & Hadﬁeld, 2019). 30Vigilant Incentives help Regulatory Markets A P REPRINT AI companies are likely to pay more to regulators who they believe will evaluate them more favourably. The above assumption is not guaranteed. The cost of regulatory innovation may turn out to be somewhat low for a range of AI applications. AI companies may have motives to pay more to regulators with more prestige. It might also be difﬁ- cult for AI companies to win the trust of their user base if another AI company can demonstrate that their evaluation was both more relevant and more reliable. The logic here is also relevant to AI certiﬁcation schemes, as discussed in P. Cihon et al. (2021). If the costs of evaluating AI systems are especially high, then it becomes more likely that governments can distinguish between high-quality and low-quality reg- ulators before they perform any audits. In these cases, the government’s dilemma may look very different, as they would have much more information with which to tailor their incentives. So far, we have discussed ways the government might mitigate the cost of a Reg- ulatory Market. However, it is worth highlighting that if the risk reduction from a Regulatory Market is high, then the costs the government faces may compara- tively be very small. For this reason, we suggest that future work on Regulatory Markets consider a more thorough assessment of the costs and beneﬁts associated with Regulatory Markets. Such work seems especially timely given that the UK Government is exploring the role of government in a similar scheme (GOV .UK, 2023). One more proposal that we suggest can complement Regulatory Markets is volun- tary safety agreements (Han et al., 2022). Companies voluntarily make agreements to adhere to safety norms, expecting that those who violate the agreement will be punished, either by other companies or by an institution. Han et al. (2022) found in their model that voluntary safety agreements can increase safety compliance without risking overregulation. These agreements are useful because Regulatory Market incentives may need to be kept low to mitigate overregulation. Using these policies together can help eliminate the dilemma zone while keeping the costs of Regulatory Markets low. We could add voluntary safety agreements to a Regulatory Market as follows. Besides the targets that governments would set, private regulators could enforce voluntary safety agreements that companies agree to. The increased detection ca- pabilities of Regulatory Markets make these agreements much more credible than otherwise, since defectors from the agreement are much more likely to be caught. 31Vigilant Incentives help Regulatory Markets A P REPRINT Keep in mind that voluntary safety agreements are ignorant of externalities; gov- ernments can set targets which take externalities into account. When in the pres- ence of externalities, voluntary safety agreements cannot serve as a replacement for government targets that affect all companies. 5.3 How can we discover and manipulate the detection rate? Unfortunately, it is not clear a priori what kind of detection rate we should expect to arise in a Regulatory Market, nor is it clear whether we can shape it. This ﬁrst challenge is empirical — can we know that the Regulatory Market is likely to achieve a Goldilocks detection rate that is neither too low nor too high? This issue may not be so terrible if the government is paying close attention to data on the performance of regulators. It seems plausible that the government or another independent observer could infer the detection rate of high-quality reg- ulators. They could for example estimate the reliability of current day audits of cutting-edge AI technologies across a range of related sectors. However, a rele- vant objection remains: will we learn that a Regulatory Market is a good idea with enough time remaining to course-correct if necessary? Related to this new criticism is the issue of how to inﬂuence the detection rate. A failure to detect malpractice may be the result of a lack of time or staff to perform quality checks on any audits. It could also be the result of a failure to anticipate emerging safety concerns in the latest models. If the detection rate is way too low, it’s perhaps unlikely for further incentives to solve the problem — it may just be too difﬁcult to expect a higher detection rate. If the detection rate is too high, governments could advise that auditors perform audits with a lower probability. However, while probabilistic spot checks seem appropriate in airline security, it will not always be appropriate for AI regulators to forgo an audit. Alternatively, we could encourage regulators to be more forgiving. Note that for such a scheme to remain ethical, the discovered malpractice would still have to be amended. This would mean a result more in line with punishing unsafe ﬁrms less harshly. Rather than reducing the detection rate, we make larger detection rates more useful. One takeaway is that it is useful to keep track of the detection rate of the regulators in the Regulatory Market. Besides the reasons outlined above, it is necessary to use proxy measures to gain a better idea of whether the regulators are fulﬁlling the government’s targets. It also seems sensible to encourage as high a detection rate as possible (since it seems difﬁcult in practise to achieve high detection rates for 32Vigilant Incentives help Regulatory Markets A P REPRINT ﬂaws in novel technologies). If detection rates appear to be too high, the govern- ment could recommend lighter restrictions on the companies who defect. 5.4 Additional challenges that face a Regulatory Market 5.4.1 Large AI companies operate in multiple markets The evidence suggests that large AI companies will operate in multiple markets. Large industry-housed labs account for the vast majority of private investment into new AI capabilities (Zhang et al., 2022). This research stretches across mul- tiple sectors of the economy, whether it is in improving visual effects or towards software for better robotic assistants. It is also increasingly clear that new AI ca- pabilities allow the development of “general purpose AI systems\" which we can expect on their own to be inﬂuential in multiple markets (Gutierrez et al., 2022). The challenge presented by AI companies operating in multiple markets is that it may be difﬁcult to avoid at least one such market becoming underregulated. With so many possible applications of AI systems, it may be difﬁcult for governments to be aware of the weakest links in their response to the risks presented by different technologies. The somewhat decentralised nature of Regulatory Markets holds promise in addressing these gaps in government monitoring, but it is not as clear from our discussion so far how governments can best target their incentives in the context of many markets. In future work, we will integrate methods from network science to address this gap in our understanding of incentives for a Regulatory Market (Choi et al., 2020; Galeotti et al., 2020). Cimpeanu et al. (2022) have already studied the competitive dynamics of AI research on heterogeneous networks. Elsewhere, Cimpeanu et al. (2023) have also studied how to target incentives to foster fairness on heteroge- neous networks. However, to better represent the many markets that AI companies ﬁnd themselves in, as well as to identify weak links in terms of regulations, we may need to turn to a multilayer network representation (Boccaletti et al., 2014; Walsh, 2019). The inclusion of single and multilayer networks into the model would not only allow greater realism, it also allows for the integration of heterogeneous sources of data about the relative risks and operations of AI companies in different markets. We could use such data to inform how policymakers should allocate time and resources towards each Regulatory Market. We also open up Regulatory Markets to be tested on whether they live up to the predictions of the model — a failure to do so can inform governments of how they might change course. Of course, 33Vigilant Incentives help Regulatory Markets A P REPRINT not all sources of data will be consistent. In many cases of interest, data will be missing: not all countries will have the capacity to monitor the AI landscape and not all companies will wish to be public about their research plans. In these cases, we plan to use machine learning techniques to infer the distributions relevant to the more complex model we have alluded to above. 5.4.2 Regulatory Capture A big reason why regulation can fail is due to regulatory capture. Collusion be- tween regulators and the companies they regulate may be fairly common, as often the people qualiﬁed to work in the industry have the qualiﬁcations and network needed to take up a role in the regulator. This can lead to group-think about what the risks are, in ways which might ultimately be self-serving. It also leaves open the possibility that companies can ﬁnd ways to reward regula- tors for approving their AI systems. If the prize is large from being the ﬁrst mover in markets for future AI capabilities, then these rewards may overpower any in- centives the government might offer. Regulation in this form may be worse than no regulation at all, as it may act as a smokescreen which discourages decision makers from taking pivotal action when it might be needed most. Regulatory capture is a difﬁcult challenge, and we do not claim to present a so- lution here. Nor do we explicitly model this failure mode. Nevertheless, we can make a case that Regulatory Markets, relative to a public or hybrid regulator, are more likely to avoid this capture. A thriving Regulatory Market would have many private regulators with a diverse set of overlapping responsibilities. It seems that it would be more difﬁcult for even powerful companies to collude with most of the regulators they might work with. Additionally, the presence of an authority that complements the monitoring activities of private regulators may increase the difﬁculty of colluding undetected. In short, the careful design of incentives can encourage a Regulatory Market that operates independent of the industry, without harming relationships between AI Safety advocates and labs developing AI Capabilities. Future work could explore a similar model to ours with an explicit collusion com- ponent (Lee et al., 2019; Liu & Chen, 2021). We think it would be even more valuable for researchers to design formal tests for the presence of collusion in the market for AI in practise. Such tests should learn from models which have yielded evidence put towards antitrust cases in the past (Besanko et al., 2020). 34Vigilant Incentives help Regulatory Markets A P REPRINT 5.4.3 International Coordination Lastly, we hope to encourage our readers to consider our discussion in an interna- tional context. Clark and Hadﬁeld (2019) convey that Regulatory Markets hold the most promise if they can mimic similar standard setting organisations in achiev- ing international coordination on AI standards. There are several challenges to this endeavour. First, as the targets that governments set for regulators are outcome based, they almost assuredly imply that AI companies will need to design their AI systems with both ethical and technical standards in mind. von Ingersleben-Seip (2023) ﬁnd that so far only technical standards for AI have seen successful adoption. von Ingersleben-Seip (2023) attribute the failure to see international agreement on ethical standards to large differences in values between countries over these stan- dards.12Barring clever framings of these difﬁcult bargaining problems (Jackson et al., 2018), these conditions are unlikely to change. Nevertheless, Regulatory Markets may still be successful given that each country is free to set their own targets that their regulators must adhere to. However, if Regulatory Markets in different nations ask different requirements of AI compa- nies, then a crucial assumption of our model is broken: the model considers that all ﬁrms are equally affected by high-quality regulators. Moreover, some govern- ments may not adopt Regulatory Markets at all, especially if they have different perceptions over the risks presented by future AI capabilities. Predictably, large economies are unlikely to enforce regulatory commitments if it puts them at a disadvantage to their economic rivals, or if it lessens their lead in a strategic do- main. This narrative paints a rather bleak outlook for regulation of AI, including the Regulatory Markets proposal we discuss here. On the other hand, we have shown that Regulatory Markets could be a useful tool for deterring unsafe behaviour, one which provides more ﬂexibility to responding to changing national or international contexts. Just as with other measures, Reg- ulatory Markets could serve as a commitment device in international relations, allowing governments to commit to a safer market for AI, assuming that everyone else is willing to see through similar commitments (O’Keefe et al., 2020; Putnam, 1988). Crucially, the success of such commitments will depend on whether there is a shared perception of the risks from transformative AI (Askell et al., 2019; Jervis, 1978). 12von Ingersleben-Seip (2023) also attribute these failures to the non-excludability and non-rivalry of ethical stan- dards — the properties of a public good. However, their data appears instead to support the idea that countries are after different ethical standards, rather than choosing to free-ride on producing such standards. 35Vigilant Incentives help Regulatory Markets A P REPRINT References Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., et al. (2016). Concrete Problems in AI Safety. arXiv . https://doi.org/10.48550/arXiv. 1606.06565 Armstrong, S., Bostrom, N., & Shulman, C. (2016). Racing to the precipice: A model of artiﬁcial intelligence development. AI & SOCIETY ,31(2), 201– 206. https://doi.org/10.1007/s00146-015-0590-y Askell, A., Brundage, M., & Hadﬁeld, G. (2019). The Role of Cooperation in Responsible AI Development. arXiv . https://doi.org/10.48550/arXiv.1907. 04534 Bar (formerly Borkovsky), R. N., Doraszelski, U., & Kryukov, Y . ( (2009). A Dy- namic Quality Ladder Model with Entry and Exit: Exploring the Equilib- rium Correspondence Using the Homotopy Method (SSRN Scholarly Pa- per). Social Science Research Network. Rochester, NY. https://doi.org/10. 2139/ssrn.1502860 Barrett, A. M., Hendrycks, D., Newman, J., & Nonnecke, B. (2022). Actionable Guidance for High-Consequence AI Risk Management: Towards Standards Addressing AI Catastrophic Risks. arXiv . https://doi.org/10.48550/arXiv. 2206.08966 Barton, J. (2005). Who cares about auditor reputation?*. Contemporary Account- ing Research ,22(3), 549–586. https://doi.org/https://doi.org/10.1506/ C27U-23K8-E1VL-20R0 Berger, A. N., & Hannan, T. H. (1998). The Efﬁciency Cost of Market Power in the Banking Industry: A Test of the “Quiet Life” and Related Hypotheses. The Review of Economics and Statistics ,80(3), 454–465. https://doi.org/10. 1162/003465398557555 Besanko, D., Doraszelski, U., & Kryukov, Y . (2020). Sacriﬁce tests for predation in a dynamic pricing model: Ordover and Willig (1981) and Cabral and Riordan (1997) meet Ericson and Pakes (1995). International Journal of Industrial Organization ,70, 102522. https://doi.org/10.1016/j.ijindorg. 2019.102522 Boccaletti, S., Bianconi, G., Criado, R., del Genio, C., Gómez-Gardeñes, J., et al. (2014). The structure and dynamics of multilayer networks. Physics Re- ports ,544(1), 1–122. https://doi.org/10.1016/j.physrep.2014.07.001 Brown, S., Davidovic, J., & Hasan, A. (2021). The algorithm audit: Scoring the al- gorithms that score us. Big Data & Society ,8(1), 2053951720983865. https: //doi.org/10.1177/2053951720983865 36Vigilant Incentives help Regulatory Markets A P REPRINT Brundage, M., Avin, S., Clark, J., Toner, H., Eckersley, P., et al. (2018). The mali- cious use of artiﬁcial intelligence: Forecasting, prevention, and mitigation. https://doi.org/10.48550/ARXIV .1802.07228 Brundage, M., Avin, S., Wang, J., Belﬁeld, H., Krueger, G., et al. (2020). To- ward Trustworthy AI Development: Mechanisms for Supporting Veriﬁable Claims. arXiv . https://doi.org/10.48550/arXiv.2004.07213 Buchholz, W., & Sandler, T. (2021). Global Public Goods: A Survey. Journal of Economic Literature ,59(2), 488–545. https://doi.org/10.1257/jel.20191546 Burden, J., & Hernández-Orallo, J. (2020). Exploring ai safety in degrees: Gen- erality, capability and control. Proceedings of the Workshop on Artiﬁcial Intelligence Safety (SafeAI 2020) Co-Located with 34th AAAI Conference on Artiﬁcial Intelligence (AAAI 2020) , 36–40. Cave, S., & Ó hÉigeartaigh, S. S. (2018). An AI Race for Strategic Advantage: Rhetoric and Risks. Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society , 36–40. https://doi.org/10.1145/3278721.3278780 Choi, S., Goyal, S., & Moisan, F. (2020). Large Scale Experiments on Networks A New Platform with Applications. Cihon, P. J., Schuett, J., & Baum, S. D. (2021). Corporate governance of artiﬁcial intelligence in the public interest. Inf.,12, 275. Cihon, P., Maas, M. M., & Kemp, L. (2020). Should artiﬁcial intelligence gov- ernance be centralised? Design lessons from history. Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society , 228–234. Cihon, P., Kleinaltenkamp, M. J., Schuett, J., & Baum, S. D. (2021). AI Certiﬁ- cation: Advancing Ethical Practice by Reducing Information Asymmetries. IEEE Transactions on Technology and Society ,2(4), 200–209. https://doi. org/10.1109/TTS.2021.3077595 Cimpeanu, T., Santos, F., Pereira, L., Lenaerts, T., & Han, T. A. (2022). Artiﬁcial intelligence development races in heterogeneous settings. Scientiﬁc Reports , 12(1), 1723. https://doi.org/10.1038/s41598-022-05729-3 Cimpeanu, T., Di Stefano, A., Perret, C., & Han, T. A. (2023). Social diversity reduces the complexity and cost of fostering fairness. Chaos, Solitons & Fractals ,167, 113051. https://doi.org/https://doi.org/10.1016/j.chaos.2022. 113051 Clark, J., & Hadﬁeld, G. K. (2019). Regulatory Markets for AI Safety. arXiv . https://doi.org/10.48550/arXiv.2001.00078 Dafoe, A. (2018). AI Governance: A Research Agenda (tech. rep.). The University of Oxford. Oxford, UK. 37Vigilant Incentives help Regulatory Markets A P REPRINT Demsetz, H. (1973). Industry Structure, Market Rivalry, and Public Policy. The Journal of Law and Economics ,16(1), 1–9. https://doi.org/10.1086/466752 Economides, N. (2001). The microsoft antitrust case. Journal of Industry, Compe- tition and Trade ,1, 7–39. Encarnação, S., Santos, F. P., Santos, F. C., Blass, V ., Pacheco, J. M., et al. (2016). Paradigm shifts and the interplay between state, business and civil sectors. Royal Society Open Science ,3(12), 160753. https://doi.org/10.1098/rsos. 160753 Foster, D., & Young, P. (1990). Stochastic evolutionary game dynamics. Theoret- ical Population Biology ,38(2), 219–232. https://doi.org/10.1016/0040- 5809(90)90011-J Fudenberg, D., & Imhof, L. A. (2006). Imitation processes with small mutations. Journal of Economic Theory ,131(1), 251–262. https://doi.org/10.1016/j. jet.2005.04.006 Fudenberg, D., Nowak, M. A., Taylor, C., & Imhof, L. A. (2006). Evolutionary game dynamics in ﬁnite populations with strong selection and weak muta- tion. Theoretical Population Biology ,70(3), 352–363. https://doi.org/10. 1016/j.tpb.2006.07.006 Galeotti, A., Golub, B., & Goyal, S. (2020). Targeting Interventions in Networks. SSRN Electronic Journal . https://doi.org/10.2139/ssrn.3054353 GOV .UK. (2023). Auditing algorithms: The existing landscape, role of regulators and future outlook [Retrieved February 2023 from https://www.gov.uk/government/publications/ﬁndings-from-the-drcf- algorithmic-processing-workstream-spring-2022/auditing-algorithms-the- existing-landscape-role-of-regulators-and-future-outlook]. Gruetzemacher, R., & Whittlestone, J. (2022). The transformative potential of ar- tiﬁcial intelligence. Futures ,135, 102884. Gursoy, F., & Kakadiaris, I. A. (2022). System Cards for AI-Based Decision- Making for Public Policy. https://doi.org/10.48550/arXiv.2203.04754 Gutierrez, C., Aguirre, A., Uuk, R., Boine, C., & Franklin, M. (2022). A proposal for a deﬁnition of general purpose artiﬁcial intelligence systems. SSRN Elec- tronic Journal . https://doi.org/10.2139/ssrn.4238951 Häggström, O. et al. (2002). Finite Markov chains and algorithmic applications (V ol. 52). Cambridge University Press. Han, T. A. (2022). Institutional incentives for the evolution of committed coop- eration: Ensuring participation is as important as enhancing compliance. Journal of The Royal Society Interface ,19(188), 20220036. 38Vigilant Incentives help Regulatory Markets A P REPRINT Han, T. A., Pereira, L. M., Santos, F. C., & Lenaerts, T. (2020). To Regulate or Not: A Social Dynamics Analysis of an Idealised AI Race. Journal of Artiﬁcial Intelligence Research ,69, 881–921. https://doi.org/10.1613/jair.1.12225 Han, T. A., Pereira, L. M., Lenaerts, T., & Santos, F. C. (2021). Mediating artiﬁcial intelligence developments through negative and positive incentives. PLOS ONE ,16(1). https://doi.org/10.1371/journal.pone.0244592 Han, T. A., Lenaerts, T., Santos, F. C., & Pereira, L. M. (2022). V oluntary safety commitments provide an escape from over-regulation in AI development. Technology in Society ,68, 101843. Hauert, C., Traulsen, A., Brandt, H., Nowak, M. A., & Sigmund, K. (2007). Via freedom to coercion: The emergence of costly punishment. science , 316(5833), 1905–1907. Hernández-Orallo, J., Martínez-Plumed, F., Avin, S., & Heigeartaigh, S. O. (2019). Surveying safety-relevant AI characteristics. Aaai Workshop on Artiﬁcial Intelligence Safety (Safeai 2019) , 1–9. Herrmann, B., Thöni, C., & Gächter, S. (2008). Antisocial punishment across so- cieties. Science ,319(5868), 1362–1367. https://doi.org/10.1126/science. 1153808 Hoffman, M., Suetens, S., Gneezy, U., & Nowak, M. A. (2015). An experimen- tal investigation of evolutionary dynamics in the rock-paper-scissors game. Scientiﬁc reports ,5(1), 8817. Hollenbeck, B. (2020). Horizontal mergers and innovation in concentrated indus- tries. Quantitative Marketing and Economics ,18(1), 1–37. Jackson, M. O., Sonnenschein, H., Xing, Y ., Tombazos, C., & Al-Ubaydli, O. (2018). The Efﬁciency of Negotiations with Uncertainty and Multi- Dimensional Deals (SSRN Scholarly Paper No. ID 3153853). Social Sci- ence Research Network. Rochester, NY. https : / / doi . org / 10 . 2139 / ssrn . 3153853 Jervis, R. (1978). Cooperation under the security dilemma. World Politics ,30(2), 167–214. https://doi.org/10.2307/2009958 Krakovna, V ., Uesato, J., Mikulik, V ., Rahtz, M., Everitt, T., et al. (2020). Spec- iﬁcation gaming: The ﬂip side of AI ingenuity [Retrieved February 2023 from https://deepmind.com/blog/article/Speciﬁcation-gaming-the-ﬂip-side- of-AI-ingenuity]. LaCroix, T., & Mohseni, A. (2022). The tragedy of the AI commons. Synthese , 200(4), 289. 39Vigilant Incentives help Regulatory Markets A P REPRINT Lee, J.-H., Iwasa, Y ., Dieckmann, U., & Sigmund, K. (2019). Social evolution leads to persistent corruption. Proceedings of the National Academy of Sci- ences ,116(27), 13276–13281. https://doi.org/10.1073/pnas.1900078116 Leike, J., Martic, M., Krakovna, V ., Ortega, P. A., Everitt, T., et al. (2017). AI Safety Gridworlds. arXiv . https://doi.org/10.48550/ARXIV .1711.09883 Liu, L., & Chen, X. (2021). Evolutionary Dynamics of Cooperation in a Corrupt Society with Anti-Corruption Control. Int. J. Bifurcation Chaos ,31(03), 2150039. https://doi.org/10.1142/S0218127421500395 Mitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L., et al. (2019). Model Cards for Model Reporting. Proceedings of the Conference on Fairness, Ac- countability, and Transparency , 220–229. https://doi.org/10.1145/3287560. 3287596 Naudé, W., & Dimitri, N. (2020). The race for an artiﬁcial general intelligence: Implications for public policy. AI & SOCIETY ,35(2), 367–379. https://doi. org/10.1007/s00146-019-00887-x O’Keefe, C., Cihon, P., Garﬁnkel, B., Flynn, C., Leung, J., et al. (2020). The windfall clause: Distributing the beneﬁts of AI for the common good. Pro- ceedings of the AAAI/ACM Conference on AI, Ethics, and Society , 327–331. https://doi.org/10.1145/3375627.3375842 Putnam, R. D. (1988). Diplomacy and domestic politics: The logic of two-level games. International Organization ,42(3), 427–460. https : / / doi . org / 10 . 1017/S0020818300027697 Rand, D. G., Tarnita, C. E., Ohtsuki, H., & Nowak, M. A. (2013). Evolution of fairness in the one-shot anonymous ultimatum game. Proceedings of the National Academy of Sciences ,110(7), 2581–2586. https://doi.org/10.1073/ pnas.1214167110 Santos, F. P., Encarnação, S., Santos, F. C., Portugali, J., & Pacheco, J. M. (2016). An Evolutionary Game Theoretic Approach to Multi-Sector Coordination and Self-Organization. Entropy ,18(4), 152. https : / / doi . org / 10 . 3390 / e18040152 Sasaki, T., Brännström, Å., Dieckmann, U., & Sigmund, K. (2012). The take- it-or-leave-it option allows small penalties to overcome social dilemmas. Proceedings of the National Academy of Sciences ,109(4), 1165–1169. Shevlane, T., & Dafoe, A. (2019). The offense-defense balance of scientiﬁc knowl- edge: Does publishing AI research reduce misuse? Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society . 40Vigilant Incentives help Regulatory Markets A P REPRINT Siegmann, C., & Anderljung, M. (2022). The Brussels Effect and Artiﬁcial Intelli- gence [Retrieved February 2023 from https://www.governance.ai/research- paper/brussels-effect-ai]. https://doi.org/10.33774/apsa-2022-vxtsl Sigmund, K., De Silva, H., Traulsen, A., & Hauert, C. (2010). Social learning promotes institutions for governing the commons. Nature ,466(7308), 861– 863. Stewart, W. J. (2009). Probability, Markov Chains, Queues, and Simulation: The Mathematical Basis of Performance Modeling . Princeton University Press. https://doi.org/10.2307/j.ctvcm4gtc Sun, W., Liu, L., Chen, X., Szolnoki, A., & Vasconcelos, V . V . (2021). Combi- nation of institutional incentives for cooperative governance of risky com- mons. iScience ,24(8), 102844. https://doi.org/10.1016/j.isci.2021.102844 Tabarrok, A. (1998). The private provision of public goods via dominant assurance contracts. Public Choice ,96(3), 345–362. https : / / doi . org / 10 . 1023 / A : 1004957109535 Tabassi, E. (2021). Artiﬁcial Intelligence Risk Management Framework (AI RMF 1.0) [Last Modiﬁed: 2023-02-13T09:12-05:00]. NIST . https://doi.org/10. 6028/NIST.AI.100-1 Truby, J., Brown, R. D., Ibrahim, I. A., & Parellada, O. C. (2022). A Sandbox Approach to Regulating High-Risk Artiﬁcial Intelligence Applications. Eu- ropean Journal of Risk Regulation ,13(2), 270–294. https://doi.org/10.1017/ err.2021.52 Vinuesa, R., Azizpour, H., Leite, I., Balaam, M., Dignum, V ., et al. (2020). The role of artiﬁcial intelligence in achieving the Sustainable Development Goals. Nature Communications ,11(1), 233. https : / / doi . org / 10 . 1038 / s41467-019-14108-y von Ingersleben-Seip, N. (2023). Competition and cooperation in artiﬁcial intelli- gence standard setting: Explaining emergent patterns. Review of Policy Re- search . https://doi.org/10.1111/ropr.12538 Wallace, C., & Young, H. P. (2015). Stochastic evolutionary game dynamics. Handbook of game theory with economic applications (pp. 327–380). El- sevier. Walsh, A. M. (2019). Games on Multi-Layer Networks (tech. rep. No. 1954). Fac- ulty of Economics, University of Cambridge. Whittlestone, J., & Clark, J. (2021). Why and How Governments Should Monitor AI Development. arXiv . https://doi.org/10.48550/arXiv.2108.12427 Worthington, R. (1982). The Social Control of Technology. By David Collingridge. (New York: St. Martin’s Press, 1980. Pp. i + 200. $22.50.) 41Vigilant Incentives help Regulatory Markets A P REPRINT American Political Science Review ,76(1), 134–135. https : / / doi . org / 10 . 2307/1960465 Zhang, D., Maslej, N., Brynjolfsson, E., Etchemendy, J., Lyons, T., et al. (2022). The ai index 2022 annual report. Zisis, I., Di Guida, S., Han, T. A., Kirchsteiger, G., & Lenaerts, T. (2015). Generos- ity motivated by acceptance-evolutionary analysis of an anticipation game. Scientiﬁc reports ,5(1), 1–11. Zwetsloot, R., & Dafoe, A. (2019). Thinking About Risks From AI: Accidents, Misuse and Structure [Retrieved February 2023 from https://www.lawfareblog.com/thinking-about-risks-ai-accidents-misuse- and-structure]. 42arXiv:2308.02033v1  [cs.CY]  7 Jul 2023AI and the EU Digital Markets Act: Addressing the Risks of Bigness in Generative AI Ayse Gizem Yasar1Andrew Chong* 2Evan Dong* 3Thomas Krendl Gilbert* 4Sarah Hladikova* 5 Roland Maio* 6Carlos Mougan* 7Xudong Shen* 8Shubham Singh* 9Ana-Andreea Stoica* 10 Savannah Thais* 6Miri Zilka* 11 Abstract As AI technology advances rapidly, concerns over the risks of bigness in digital markets are also growing. The EU’s Digital Markets Act (DMA) aims to address these risks. Still, the current framework may not adequately cover generative AI systems that could become gateways for AI-based services. This paper argues for integrating certain AI software as “core platform services” and classifying certain developers as gatekeepers under the DMA. We also propose an assessment of gatekeeper obliga- tions to ensure they cover generative AI services. As the EU considers generative AI-speciﬁc rules and possible DMA amendments, this paper provides insights towards diversity and openness in generative AI services. 1. Introduction The European Union’s (EU) response to “bigness” ( Bran- deis,1934 ;Wu,2018 ) in digital markets goes beyond tra- ditional antitrust and competition law enforcement. Under EU competition law, large companies have not been viewed as inherently problematic. However, there is an increasing concern that “a few large platforms increasingly act as gate - ways or gatekeepers between business users and end users and enjoy an entrenched and durable position, often as a *Authors ordered alphabetically, except the ﬁrst author.1Lon- don School of Economics, London, United Kingdom2University of California, Berkeley, USA3Cornell University,USA4Cornell Tech, USA5Tufts University, USA6Columbia University, USA 7University of Southampton, United Kingdom8National Univer- sity of Singapore, Singapore9University of Illinois Chicago, USA 10Max Planck Institute for Intelligent Systems, T¨ ubingen, G er- many11University of Cambridge, UK. Correspondence to: Ayse Gizem Yasar <a.g.yasar@lse.ac.uk >. Proceedings of the 40thInternational Conference on Machine Learning , Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s).result of the creation of conglomerate ecosystems around their core platform services, which reinforces existing en - try barriers.” ( European Commission ,2020 ). According to the European Commission, these entrenched positions lead to unfair behaviour vis-` a-vis business users of these plat - forms, as well as reduced innovation and contestability in core platform services. These concerns culminated in the creation of a new Euro- pean regulation that goes beyond traditional competition law rules: the Digital Markets Act (DMA). The DMA is set up to counteract platform size and gatekeeping rather than abuse of dominance or monopoly power that competi- tion/antitrust laws target. It aims to address the shortcom - ings of competition law in keeping the entry barriers low and ensuring fair game between “gatekeepers” and their smaller rivals that depend on gatekeepers’ services. The regulation covers some well-established platform service s like operating systems, messaging platforms, and online ad - vertising. The DMA has been applauded by many who are concerned about the rise of large digital platforms. It has also been cr it- icised because it may no longer be possible to inject diver- sity into digital markets once certain players have become squarely entrenched. This is a concern that we share. The DMA is silent on AI, but we observe that gatekeepers are starting to emerge in generative AI applications. The DMA provides an opportunity to maintain a fair and diverse space for AI applications before a few large players become en- trenched and durable. There is now political momentum in both the EU and the US to address the emergence of gen- erative AI gatekeepers ( Vestager ,2023 ;Subcommittee on Privacy, Technology, and the Law ,2023 ) This paper argues that the generative AI industry should be directly addressed in the DMA. In particular, we argue that generative AI services should be integrated into the DMA’s list of core platform services. We show that certain gener- ative AI services embody gatekeeper characteristics in the sense of the DMA. While certain use cases of generative AI might indirectly fall under the DMA’s current list of core platform services, there are complex ways in whichAddressing the Risks of Bigness in Generative AI generative AI services may act as gatekeepers in their own right. Among those, we highlight the gatekeeping potential of generative AI service providers due to: (i)computing power, (ii)early mover advantages, and (iii)data resources and integrated systems. We conclude with a discussion on how the DMA could be amended to ensure contestability and fairness in the market for generative AI services. 2. Gatekeepers and the DMA The DMA came into force around the same time as the Dig- ital Services Act, which regulates platform accountabilit y and content moderation. Both regulations impose speciﬁc obligations on companies above certain size thresholds, al - beit framed and described differently. The EU is also about to ﬁnalise its AI Act, which adopts a risk-based approach. It outright prohibits certain AI applications that bear un- acceptable risks to people’s safety, and introduces trans- parency and accountability rules for high-risk applicatio ns. The DMA targets “gatekeeper” companies, which is deﬁned under Article 3(1) as an undertaking that: (i)“has a signiﬁcant impact on the [EU’s] internal market”, (ii)“provides a core platform service which is an important gateway for business users to reach end-users”, and (iii) “enjoys an entrenched and durable position in its operations, or it is foreseeable that it will enjoy such a position in the near future.” The current list of “core platform services” provided in Ar- ticle 2(2) covers ten established digital services, such as op- erating systems, web browsers, and social networking. It does not explicitly cover generative AI services. In the res t of this paper, we bring forward ways in which generative AI can be provided as a platform service and argue for its explicit integration into the DMA. 3. Platformization of AI The DMA applies to core platform services. Although some companies are likely to offer AI only as a product, an- other potentially effective route to proﬁt is to provide gen - erative AI as a platform. For example, OpenAI has created several large foundation models (e.g., GPT-4 and DALL-E) that can serve as the basis for a wide range of applications. The company began to monetize these foundational models in different ways, including: (i)by releasing some models directly to the public (e.g., ChatGPT) using a “freemium” business model, and (ii)by offering API access to its mod- els and enabling the development of applications built on top of them. The latter allows organizations to integrate OpenAI’s models into their own products, which they then provide to the public. To the extent generative AI applica- tions are provided as a platform, they can be brought within the remit of the DMA.4. Emergence of Gatekeepers in Generative AI The generative AI industry is already driven by a small number of companies, notably OpenAI, Google, Microsoft, and Meta, who hold a signiﬁcant competitive advantage due to their extensive data resources, specialized hardwar e architectures, vertical integration, network effects, ﬁn an- cial clout, know-how, and early-mover advantage. While some of these advantages are speciﬁc to AI applications, such as specialized hardware architectures, most—such as data resources, ﬁnancial clout, network effects, integrat ion and the importance of early movers—have been present in digital markets in general and given rise to the gatekeeping positions that the DMA now seeks to address ( European Commission ,2020 ). Without regulatory intervention, these signiﬁcant advantages will likely turn into entrenched pos i- tions in generative AI services, as they have in other digita l markets. Computing Power in the Hands of a Few: While the cost of ﬁne-tuning large generative AI models is decreasing , achieving state-of-the-art performance still requires a h igh budget, thus creating an entry barrier for potential player s and inhibiting diversity and market growth. This entry bar- rier disproportionately affects smaller companies, publi c in- stitutions, and universities, who often lack the ﬁnancial r e- sources to establish independent large-scale generative A I systems. Consequently, the concentration of cutting-edge computing power and expertise in the hands of a few play- ers risks limiting player diversity and stiﬂing innovation within the generative AI industry. Early Mover Advantage: Early movers’ head start, like OpenAI and DeepMind, in developing generative AI sys- tems may also lead to entrenched positions. Early movers currently face the challenge of determining whether they should make their AI systems available as open source ( Vin- cent,2023 ). However, even if they do, such open-source versions often come with strings attached to enable mone- tization, which was the core dispute in the European Com- mission’s Google Android case in the context of mobile operating systems ( European Commission ,2022 ). In any case, monetization is now becoming increasingly common for developers ( Dastin et al. ,2023 ). Data Resources and Integrated Systems: A new trend in generative AI systems is the recent inﬂux of integrated services: search engines integrate LLMs, personal as- sistants, note-taking, editing, creative tasks automatio n, video-editing applications, or generative AI-augmented search ( Reid ,2023 ). As integrated systems that use gen- erative AI become ubiquitous, their convenience and poten- tial for creative endeavours trade off with user autonomy. Meanwhile, platform tendency for integration and the asso- ciated loss of user autonomy is not new: Google, MicrosoftAddressing the Risks of Bigness in Generative AI and Apple have all pushed for integrated systems in their now-established services. Furthermore, large players may beneﬁt from both existing troves of data from legacy appli- cations (e.g., Google from G-suite users), as well as data generated as users interact with AI applications (through prompts and other inputs). The more popular an applica- tion is, the more it will beneﬁt from human feedback, which creates feedback loops associated with network effects— another factor motivating the DMA. As a result of service and data integration, it becomes increasingly difﬁcult for a user to switch between platforms and transport their data and projects. The DMA speciﬁcally prohibits data and ser- vice integration across different offerings of core platfo rm services (e.g., Article 5(2)(b) and 5(8)). However, as men- tioned above, this list does not include any generative AI applications. 5. Contestability and Fairness in the Generative AI Industry The potential harms of AI have been discussed for years, but the current race toward generative AI enhancement might lead to the emergence of a few large players at the expense of contestability and fairness in the generative AI industry. The draft AI Act and the DMA do not directly address this problem. Certain cases of generative AI applications might indirect ly fall within the remit of the DMA when gatekeepers inte- grate proprietary or third-party generative AI into their c ore platform services. However, such indirect application fal ls short of addressing our concerns. The DMA’s gatekeeper obligations that are designed to counteract contestabilit y and fairness issues will not apply to their generative AI of- fering when it is provided as a standalone platform service. There is evidence that unfair practices are already emerg- ing in generative AI. For example, Microsoft has reportedly threatened to cut off the access of at least two of its search index business customers that were building their own gen- erative AI tools using data from the search index ( Nylen & Bass ,2023 ). Such behavior appears to be intended to weaken the contestability of Microsoft’s own generative AI offering by denying potential competitors the key input fac - tor of data. Such practices are exactly what the DMA aims to eliminate, but they are currently not caught by the reg- ulation. Generative AI providers remain free to leverage their superior bargaining power to engage in such unfair practices, and their already considerable economic power to prevent contestability. The DMA provides an opportunity to address contestability and fairness issues in the generative AI industry by desig- nating certain AI software as core platform services and certain developers as gatekeepers. An initial assessment o fgatekeeper obligations under the DMA (Articles 5-6-7) re- veals that some of them would already apply to generative AI systems. For example, under Article 6(2), gatekeepers are prevented from using, in competition with their busi- ness users, the data generated by business users of their core platform services and by these businesses’ customers. This provision would prevent generative AI gatekeepers from free-riding on data produced by businesses relying on their API to provide downstream applications, either in generative AI verticals or other industries. Similarly, a generative AI gatekeeper would be prevented from forcing business users to rely on its own identiﬁcation services, web browser engine, payment service, or other technical services, for services provided using the gatekeeper’s gen - erative AI under Article 5(7). A similar provision under Article 5(8) would prevent generative AI gatekeepers from forcing businesses and end users to subscribe to or register with any of its other core platform services as a condition to use its generative AI service. The DMA can thus support contestability and innovation in AI systems, promoting the development of a more diverse and accessible generative AI market. In conclusion, this paper presents a set of proposals to amend the DMA to prevent the emergence of entrenched positions and to address potential harms related to gate- keeping in the generative AI industry.Addressing the Risks of Bigness in Generative AI References Brandeis, L. D. The Curse of Bigness: Miscellaneous Papers of Louis D. Brand eis. Viking Press, 1934. Dastin, J., Hu, K., and Dave, P. Exclusive: Chatgpt owner ope nai projects $1 billion in revenue by 2024. Reuters, 2023. URLhttps://www.reuters.com/business/chatgpt-owner-open ai-projects-1-billion-revenue-by-2024-sources-2022- 12-15/ . European Commission. Proposal for a regulation of the europ ean parliament and of the council on con- testable and fair markets in the digital sector (digital mar kets act). COM/2020/842 ﬁnal, 2020. URL https://eur-lex.europa.eu/legal-content/en/TXT/?uri =COM%3A2020%3A842%3AFIN . European Commission. Google and Alphabet v Commission (Goo gle Android), 2022. URL https://ec.europa.eu/competition/antitrust/cases/de c_docs/40099/40099_9993_3.pdf . Nylen, L. and Bass, D. Microsoft threatens data restriction s in rival ai search. Bloomberg, 2023. URL https://finance.yahoo.com/news/microsoft-threatens- restrict-data-rival-002746878.html . Reid, E. Supercharging search with generative ai. Google, 2 023. URL https://blog.google/products/search/generative-ai-s earch/ . Subcommittee on Privacy, Technology, and the Law. Oversigh t of a.i.: Rules for artiﬁcial intelligence. Subcommittee H ear- ing, 2023. URL https://www.judiciary.senate.gov/committee-activity /hearings/oversight-of-ai-rules-for-artificial-inte lligence . Dirksen Senate Ofﬁce Building Room 226, Washington, D.C. Vestager, M. Eu eyes new rules for generative ai this year. Ni kkei Asie Interview, 2023. URL https://asia.nikkei.com/Editor-s-Picks/Interview/EU -eyes-new-rules-for-generative-AI-this-year-Vestage r. Vincent, J. Openai co-founder on company’s past approach to openly sharing research: “we were wrong”, Mar 2023. URL https://www.theverge.com/2023/3/15/23640180/openai- gpt-4-launch-closed-research-ilya-sutskever-intervi ew. Wu, T. The curse of bigness. Columbia Global Reports , 75, 2018.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_embeddings(full_text: str) -> None:\n",
    "    # \"\"\"Splits the text into chunks and creates a Qdrant vector store.\n",
    "\n",
    "    # Args:\n",
    "    #     full_text (str): The full text content of the papers.\n",
    "    # \"\"\"\n",
    "    try:\n",
    "        # Split the text into chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "        paper_chunks = text_splitter.create_documents([full_text])\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error splitting text into chunks: {e}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Create Qdrant vector store\n",
    "        qdrant = Qdrant.from_documents(\n",
    "            documents=paper_chunks,\n",
    "            embedding=CohereEmbeddings(model=\"embed-english-light-v3.0\"),\n",
    "            path=\"./db\",\n",
    "            collection_name=\"arxiv_papers\",\n",
    "        )\n",
    "        return qdrant.as_retriever()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating Qdrant vector store: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-06 21:46:50,175 - INFO - HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "2024-07-06 21:46:57,854 - INFO - HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "2024-07-06 21:47:04,306 - INFO - HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n",
      "2024-07-06 21:47:07,908 - INFO - HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "retriver = get_embeddings(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags=['Qdrant', 'CohereEmbeddings'] vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x0000021CBE7657E0>\n"
     ]
    }
   ],
   "source": [
    "print(retriver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is defence futute with AI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-06 21:47:29,484 - INFO - HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'_id': 'c4a8534cdb544e868b99d81271324de2', '_collection_name': 'arxiv_papers'}, page_content='to expect it to be sustained indefinitely. UK  Defence must be able to quickly generate it when needed .  Additionally the system needs to be affordable , a by no  means insignificant factor in a time of strong fiscal head - winds for UK Defence.  Therefore, we need the ability to  evolve our AI system in an effective, timely and cost - effective manner.  These four elements are illustrated in  Figure 2.   Figure 2 : Four elements of the broad context of AI   2 AI immaturity: Defence Challenge to AI  Technical Capabilities   One challenge with the adoption of AI within Defence is  that many Defence tasks require AI capabilities which are  currently immature.  Examples inclu de [7, 8] : \\uf0b7 Military decision -making within combat operations can  be characte rised as having a “high regret” if the “wrong”  decision is made; so requiring a high degree of trust in  any dec ision-making conducted by AI and the associated  need for interpretabi lity. The co mplexity of this challenge  is'),\n",
       " Document(metadata={'_id': '82327a2f096a4c16a7fa124c42ef03e5', '_collection_name': 'arxiv_papers'}, page_content='to expect it to be sustained indefinitely. UK  Defence must be able to quickly generate it when needed .  Additionally the system needs to be affordable , a by no  means insignificant factor in a time of strong fiscal head - winds for UK Defence.  Therefore, we need the ability to  evolve our AI system in an effective, timely and cost - effective manner.  These four elements are illustrated in  Figure 2.   Figure 2 : Four elements of the broad context of AI   2 AI immaturity: Defence Challenge to AI  Technical Capabilities   One challenge with the adoption of AI within Defence is  that many Defence tasks require AI capabilities which are  currently immature.  Examples inclu de [7, 8] : \\uf0b7 Military decision -making within combat operations can  be characte rised as having a “high regret” if the “wrong”  decision is made; so requiring a high degree of trust in  any dec ision-making conducted by AI and the associated  need for interpretabi lity. The co mplexity of this challenge  is'),\n",
       " Document(metadata={'_id': '580ffbf6a96a4d958cde1aaa75a9c6b8', '_collection_name': 'arxiv_papers'}, page_content='to expect it to be sustained indefinitely. UK  Defence must be able to quickly generate it when needed .  Additionally the system needs to be affordable , a by no  means insignificant factor in a time of strong fiscal head - winds for UK Defence.  Therefore, we need the ability to  evolve our AI system in an effective, timely and cost - effective manner.  These four elements are illustrated in  Figure 2.   Figure 2 : Four elements of the broad context of AI   2 AI immaturity: Defence Challenge to AI  Technical Capabilities   One challenge with the adoption of AI within Defence is  that many Defence tasks require AI capabilities which are  currently immature.  Examples inclu de [7, 8] : \\uf0b7 Military decision -making within combat operations can  be characte rised as having a “high regret” if the “wrong”  decision is made; so requiring a high degree of trust in  any dec ision-making conducted by AI and the associated  need for interpretabi lity. The co mplexity of this challenge  is'),\n",
       " Document(metadata={'_id': '53b701fa858342e488cfce0654974cb9', '_collection_name': 'arxiv_papers'}, page_content='process is within the  portfolio for Defence Legal  (Commonwealth of Australia  2018) . AI will also b e needed to manage  Australia’s  grey zone threat  (Townshend, Lonergan,  and Warden 2021) .    The ADO  acknowledges that they need to  effectively use their data holdings  to harness the  opportunities of AI technologies  and t he Defence Artificial Intelligence Centre  (DAIC)  has  been established to accelerate Defence’s AI capability  (Department of Defence 2021a, 35;  2020b) . The ADO  has launched  The AI for Decision Making Initiative  (Defence Science  Institute 2020a, 2021; Defe nce Science & Technology Group 2021a)  and a  Defence Artificial    2 See https://www1.defence.gov.au/about/people -group      10 Intelligence Resea rch Network (DAIRNET)  to develop  AI “to process noisy and dynamic data  in order to produce outcomes to provide decision superiority”  (Defence Science & Technology  Group 2021b) .     These efforts include  some human -centred  projects , such as')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "retriver.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_retriever() -> Optional[Qdrant]:\n",
    "#     \"\"\"Gets the retriever from the existing Qdrant collection.\n",
    "\n",
    "#     Returns:\n",
    "#         Optional[Qdrant]: The retriever if successful, otherwise None.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         qdrant = Qdrant.from_existing_collection(\n",
    "#             embedding=CohereEmbeddings(model=\"embed-english-light-v3.0\"),\n",
    "#             collection_name=\"arxiv_papers\"\n",
    "#         )\n",
    "#         return qdrant.as_retriever(k=5)\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Error getting retriever: {e}\")\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver = get_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Qdrant', 'CohereEmbeddings'], vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x0000021CBE712F80>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VectorStoreRetriever' object has no attribute 'similarity_search'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is defence futute with AI\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m found_docs \u001b[38;5;241m=\u001b[39m \u001b[43mretriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search\u001b[49m(query)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'VectorStoreRetriever' object has no attribute 'similarity_search'"
     ]
    }
   ],
   "source": [
    "query = \"What is defence futute with AI\"\n",
    "found_docs = retriver.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
